



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.4.3">
    
    
      
        <title>Tfagents - easyagents</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/application.30686662.css">
      
        <link rel="stylesheet" href="../../../../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#4caf50">
      
    
    
      <script src="../../../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../../../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="green" data-md-color-accent="lightgreen">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#module-easyagentsbackendstfagents" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../../.." title="easyagents" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              easyagents
            </span>
            <span class="md-header-nav__topic">
              
                Tfagents
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/christianhidber/easyagents/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    easyagents
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../../.." title="easyagents" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    easyagents
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/christianhidber/easyagents/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    easyagents
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../../../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../../CODE_OF_CONDUCT/" title="Code Of Conduct" class="md-nav__link">
      Code Of Conduct
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../../CONTRIBUTING/" title="Contributing" class="md-nav__link">
      Contributing
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../../../documentation/markdown/Release Notes/" title="Release Notes" class="md-nav__link">
      Release Notes
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5" checked>
    
    <label class="md-nav__link" for="nav-5">
      Reference
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Reference
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1" type="checkbox" id="nav-5-1" checked>
    
    <label class="md-nav__link" for="nav-5-1">
      Easyagents
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="2">
      <label class="md-nav__title" for="nav-5-1">
        Easyagents
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../agents/" title="Agents" class="md-nav__link">
      Agents
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../core/" title="Core" class="md-nav__link">
      Core
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1-3" type="checkbox" id="nav-5-1-3" checked>
    
    <label class="md-nav__link" for="nav-5-1-3">
      Backends
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-5-1-3">
        Backends
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../core/" title="Core" class="md-nav__link">
      Core
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../kerasrl/" title="Kerasrl" class="md-nav__link">
      Kerasrl
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Tfagents
      </label>
    
    <a href="./" title="Tfagents" class="md-nav__link md-nav__link--active">
      Tfagents
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tfagent" class="md-nav__link">
    TfAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfagentagentfactory" class="md-nav__link">
    TfAgentAgentFactory
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create_agent" class="md-nav__link">
    create_agent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_algorithms" class="md-nav__link">
    get_algorithms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfdqnagent" class="md-nav__link">
    TfDqnAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#collect_step" class="md-nav__link">
    collect_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_1" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_1" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_1" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_1" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_1" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_1" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_1" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_1" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_1" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_1" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfppoagent" class="md-nav__link">
    TfPpoAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log_2" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_2" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_2" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_2" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_2" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_2" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_2" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_2" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_2" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_2" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfrandomagent" class="md-nav__link">
    TfRandomAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log_3" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_3" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_3" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_3" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_3" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_3" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_3" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_3" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_3" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_3" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfreinforceagent" class="md-nav__link">
    TfReinforceAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_5" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_5" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log_4" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_4" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_4" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_4" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_4" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_4" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_4" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_4" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_4" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_4" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfsacagent" class="md-nav__link">
    TfSacAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_6" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_6" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log_5" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_5" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_5" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_5" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_5" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_5" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_5" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_5" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_5" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_5" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../tforce/" title="Tforce" class="md-nav__link">
      Tforce
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-1-4" type="checkbox" id="nav-5-1-4">
    
    <label class="md-nav__link" for="nav-5-1-4">
      Callbacks
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="3">
      <label class="md-nav__title" for="nav-5-1-4">
        Callbacks
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../callbacks/duration/" title="Duration" class="md-nav__link">
      Duration
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../callbacks/log/" title="Log" class="md-nav__link">
      Log
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../callbacks/plot/" title="Plot" class="md-nav__link">
      Plot
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tfagent" class="md-nav__link">
    TfAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfagentagentfactory" class="md-nav__link">
    TfAgentAgentFactory
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create_agent" class="md-nav__link">
    create_agent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_algorithms" class="md-nav__link">
    get_algorithms
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfdqnagent" class="md-nav__link">
    TfDqnAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#collect_step" class="md-nav__link">
    collect_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_1" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_1" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_1" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_1" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_1" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_1" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_1" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_1" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_1" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_1" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfppoagent" class="md-nav__link">
    TfPpoAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log_2" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_2" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_2" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_2" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_2" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_2" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_2" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_2" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_2" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_2" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfrandomagent" class="md-nav__link">
    TfRandomAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log_3" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_3" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_3" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_3" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_3" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_3" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_3" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_3" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_3" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_3" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfreinforceagent" class="md-nav__link">
    TfReinforceAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_5" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_5" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log_4" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_4" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_4" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_4" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_4" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_4" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_4" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_4" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_4" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_4" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tfsacagent" class="md-nav__link">
    TfSacAgent
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_6" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_6" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log_5" class="md-nav__link">
    log
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log_api_5" class="md-nav__link">
    log_api
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_begin_5" class="md-nav__link">
    on_play_episode_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_play_episode_end_5" class="md-nav__link">
    on_play_episode_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_begin_5" class="md-nav__link">
    on_train_iteration_begin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#on_train_iteration_end_5" class="md-nav__link">
    on_train_iteration_end
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_5" class="md-nav__link">
    play
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#play_implementation_5" class="md-nav__link">
    play_implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_5" class="md-nav__link">
    train
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#train_implementation_5" class="md-nav__link">
    train_implementation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/christianhidber/easyagents/edit/master/reference/easyagents/backends/tfagents.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="module-easyagentsbackendstfagents">Module easyagents.backends.tfagents</h1>
<p>This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents)</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;This module contains the backend implementation for tf Agents (see https://github.com/tensorflow/agents)&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># noinspection PyUnresolvedReferences</span>

<span class="kn">import</span> <span class="nn">easyagents.agents</span>

<span class="kn">from</span> <span class="nn">easyagents</span> <span class="kn">import</span> <span class="n">core</span>

<span class="kn">from</span> <span class="nn">easyagents.backends</span> <span class="kn">import</span> <span class="n">core</span> <span class="k">as</span> <span class="n">bcore</span>

<span class="kn">from</span> <span class="nn">easyagents.backends</span> <span class="kn">import</span> <span class="n">monitor</span>

<span class="c1"># noinspection PyPackageRequirements</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">tf_agents.agents.ddpg</span> <span class="kn">import</span> <span class="n">critic_network</span>

<span class="kn">from</span> <span class="nn">tf_agents.agents.dqn</span> <span class="kn">import</span> <span class="n">dqn_agent</span>

<span class="kn">from</span> <span class="nn">tf_agents.agents.ppo</span> <span class="kn">import</span> <span class="n">ppo_agent</span>

<span class="kn">from</span> <span class="nn">tf_agents.agents.reinforce</span> <span class="kn">import</span> <span class="n">reinforce_agent</span>

<span class="kn">from</span> <span class="nn">tf_agents.agents.sac</span> <span class="kn">import</span> <span class="n">sac_agent</span>

<span class="kn">from</span> <span class="nn">tf_agents.drivers</span> <span class="kn">import</span> <span class="n">dynamic_step_driver</span>

<span class="kn">from</span> <span class="nn">tf_agents.drivers.dynamic_episode_driver</span> <span class="kn">import</span> <span class="n">DynamicEpisodeDriver</span>

<span class="kn">from</span> <span class="nn">tf_agents.environments</span> <span class="kn">import</span> <span class="n">gym_wrapper</span><span class="p">,</span> <span class="n">py_environment</span><span class="p">,</span> <span class="n">tf_py_environment</span>

<span class="kn">from</span> <span class="nn">tf_agents.networks</span> <span class="kn">import</span> <span class="n">actor_distribution_network</span><span class="p">,</span> <span class="n">normal_projection_network</span><span class="p">,</span> <span class="n">q_network</span><span class="p">,</span> <span class="n">value_network</span>

<span class="kn">from</span> <span class="nn">tf_agents.policies</span> <span class="kn">import</span> <span class="n">greedy_policy</span><span class="p">,</span> <span class="n">tf_policy</span><span class="p">,</span> <span class="n">random_tf_policy</span>

<span class="kn">from</span> <span class="nn">tf_agents.replay_buffers</span> <span class="kn">import</span> <span class="n">tf_uniform_replay_buffer</span>

<span class="kn">from</span> <span class="nn">tf_agents.replay_buffers.tf_uniform_replay_buffer</span> <span class="kn">import</span> <span class="n">TFUniformReplayBuffer</span>

<span class="kn">from</span> <span class="nn">tf_agents.trajectories</span> <span class="kn">import</span> <span class="n">trajectory</span>

<span class="kn">from</span> <span class="nn">tf_agents.utils</span> <span class="kn">import</span> <span class="n">common</span>

<span class="kn">import</span> <span class="nn">gym</span>

<span class="c1"># noinspection PyUnresolvedReferences,PyAbstractClass</span>

<span class="k">class</span> <span class="nc">TfAgent</span><span class="p">(</span><span class="n">bcore</span><span class="o">.</span><span class="n">BackendAgent</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Reinforcement learning agents based on googles tf_agent implementations</span>

<span class="sd">        https://github.com/tensorflow/agents</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span> <span class="n">backend_name</span><span class="o">=</span><span class="n">TfAgentAgentFactory</span><span class="o">.</span><span class="n">backend_name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_policy</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_play_env</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">_create_gym_with_wrapper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">discount</span><span class="p">):</span>

        <span class="n">gym_spec</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">gym_env_name</span><span class="p">)</span>

        <span class="n">gym_env</span> <span class="o">=</span> <span class="n">gym_spec</span><span class="o">.</span><span class="n">make</span><span class="p">()</span>

        <span class="c1"># simplify_box_bounds: Whether to replace bounds of Box space that are arrays</span>

        <span class="c1">#  with identical values with one number and rely on broadcasting.</span>

        <span class="c1"># important, simplify_box_bounds True crashes environments with boundaries with identical values</span>

        <span class="n">env</span> <span class="o">=</span> <span class="n">gym_wrapper</span><span class="o">.</span><span class="n">GymWrapper</span><span class="p">(</span>

            <span class="n">gym_env</span><span class="p">,</span>

            <span class="n">discount</span><span class="o">=</span><span class="n">discount</span><span class="p">,</span>

            <span class="n">simplify_box_bounds</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">env</span>

    <span class="k">def</span> <span class="nf">_create_env</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">discount</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf_py_environment</span><span class="o">.</span><span class="n">TFPyEnvironment</span><span class="p">:</span>

        <span class="sd">&quot;&quot;&quot; creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment</span>

<span class="sd">            Args:</span>

<span class="sd">                discount: the reward discount factor</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">discount</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;discount not admissible&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;TFPyEnvironment&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;( suite_gym.load( ... ) )&#39;</span><span class="p">)</span>

        <span class="c1"># suit_gym.load crashes our environment</span>

        <span class="c1"># py_env = suite_gym.load(self.model_config.gym_env_name, discount=discount)</span>

        <span class="n">py_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_gym_with_wrapper</span><span class="p">(</span><span class="n">discount</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">tf_py_environment</span><span class="o">.</span><span class="n">TFPyEnvironment</span><span class="p">(</span><span class="n">py_env</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_get_gym_env</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tf_py_env</span><span class="p">:</span> <span class="n">tf_py_environment</span><span class="o">.</span><span class="n">TFPyEnvironment</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">monitor</span><span class="o">.</span><span class="n">_MonitorEnv</span><span class="p">:</span>

        <span class="sd">&quot;&quot;&quot; extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env&quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tf_py_env</span><span class="p">,</span> <span class="n">tf_py_environment</span><span class="o">.</span><span class="n">TFPyEnvironment</span><span class="p">),</span> \

            <span class="s2">&quot;passed tf_py_env is not an instance of TFPyEnvironment&quot;</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tf_py_env</span><span class="o">.</span><span class="n">pyenv</span><span class="p">,</span> <span class="n">py_environment</span><span class="o">.</span><span class="n">PyEnvironment</span><span class="p">),</span> \

            <span class="s2">&quot;passed TFPyEnvironment.pyenv does not contain a PyEnvironment&quot;</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf_py_env</span><span class="o">.</span><span class="n">pyenv</span><span class="o">.</span><span class="n">envs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;passed TFPyEnvironment.pyenv does not contain a unique environment&quot;</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">tf_py_env</span><span class="o">.</span><span class="n">pyenv</span><span class="o">.</span><span class="n">envs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">gym</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">monitor</span><span class="o">.</span><span class="n">_MonitorEnv</span><span class="p">),</span> <span class="s2">&quot;passed TFPyEnvironment does not contain a _MonitorEnv&quot;</span>

        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">play_implementation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Agent specific implementation of playing a single episodes with the current policy.</span>

<span class="sd">            Args:</span>

<span class="sd">                play_context: play configuration to be used</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">play_context</span><span class="p">,</span> <span class="s2">&quot;play_context not set.&quot;</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_policy</span><span class="p">,</span> <span class="s2">&quot;trained_policy not set. call train() first.&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_play_env</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_play_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_env</span><span class="p">()</span>

        <span class="n">gym_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_gym_env</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_play_env</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_play_episode_begin</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">gym_env</span><span class="p">)</span>

            <span class="n">time_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_play_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

            <span class="k">while</span> <span class="ow">not</span> <span class="n">time_step</span><span class="o">.</span><span class="n">is_last</span><span class="p">():</span>

                <span class="n">action_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_policy</span><span class="o">.</span><span class="n">action</span><span class="p">(</span><span class="n">time_step</span><span class="p">)</span>

                <span class="n">time_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_play_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_step</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_play_episode_end</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">play_context</span><span class="o">.</span><span class="n">play_done</span><span class="p">:</span>

                <span class="k">break</span>

<span class="c1"># noinspection PyUnresolvedReferences</span>

<span class="k">class</span> <span class="nc">TfDqnAgent</span><span class="p">(</span><span class="n">TfAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot; creates a new agent based on the DQN algorithm using the tfagents implementation.</span>

<span class="sd">        Args:</span>

<span class="sd">            model_config: the model configuration including the name of the target gym environment</span>

<span class="sd">                as well as the neural network architecture.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">collect_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">tf_py_environment</span><span class="o">.</span><span class="n">TFPyEnvironment</span><span class="p">,</span> <span class="n">policy</span><span class="p">:</span> <span class="n">tf_policy</span><span class="o">.</span><span class="n">Base</span><span class="p">,</span>

                     <span class="n">replay_buffer</span><span class="p">:</span> <span class="n">TFUniformReplayBuffer</span><span class="p">):</span>

        <span class="n">time_step</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">current_time_step</span><span class="p">()</span>

        <span class="n">action_step</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">action</span><span class="p">(</span><span class="n">time_step</span><span class="p">)</span>

        <span class="n">next_time_step</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_step</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>

        <span class="n">traj</span> <span class="o">=</span> <span class="n">trajectory</span><span class="o">.</span><span class="n">from_transition</span><span class="p">(</span><span class="n">time_step</span><span class="p">,</span> <span class="n">action_step</span><span class="p">,</span> <span class="n">next_time_step</span><span class="p">)</span>

        <span class="n">replay_buffer</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">traj</span><span class="p">)</span>

    <span class="c1"># noinspection DuplicatedCode</span>

    <span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Tf-Agents Ppo Implementation of the train loop.</span>

<span class="sd">        The implementation follows</span>

<span class="sd">        https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_context</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span><span class="p">)</span>

        <span class="n">dc</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="n">train_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_env</span><span class="p">(</span><span class="n">discount</span><span class="o">=</span><span class="n">dc</span><span class="o">.</span><span class="n">reward_discount_gamma</span><span class="p">)</span>

        <span class="n">observation_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">()</span>

        <span class="n">action_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">()</span>

        <span class="n">timestep_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">time_step_spec</span><span class="p">()</span>

        <span class="c1"># SetUp Optimizer, Networks and DqnAgent</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;AdamOptimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">dc</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;QNetwork&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">q_net</span> <span class="o">=</span> <span class="n">q_network</span><span class="o">.</span><span class="n">QNetwork</span><span class="p">(</span><span class="n">observation_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">,</span> <span class="n">fc_layer_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">fc_layers</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;DqnAgent&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">tf_agent</span> <span class="o">=</span> <span class="n">dqn_agent</span><span class="o">.</span><span class="n">DqnAgent</span><span class="p">(</span><span class="n">timestep_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">,</span>

                                      <span class="n">q_network</span><span class="o">=</span><span class="n">q_net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>

                                      <span class="n">td_errors_loss_fn</span><span class="o">=</span><span class="n">common</span><span class="o">.</span><span class="n">element_wise_squared_loss</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;tf_agent.initialize&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">tf_agent</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_policy</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">policy</span>

        <span class="c1"># SetUp Data collection &amp; Buffering</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;TFUniformReplayBuffer&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">TFUniformReplayBuffer</span><span class="p">(</span><span class="n">data_spec</span><span class="o">=</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_data_spec</span><span class="p">,</span>

                                              <span class="n">batch_size</span><span class="o">=</span><span class="n">train_env</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>

                                              <span class="n">max_length</span><span class="o">=</span><span class="n">dc</span><span class="o">.</span><span class="n">max_steps_in_buffer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;RandomTFPolicy&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">random_policy</span> <span class="o">=</span> <span class="n">random_tf_policy</span><span class="o">.</span><span class="n">RandomTFPolicy</span><span class="p">(</span><span class="n">timestep_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;replay_buffer.add_batch&#39;</span><span class="p">,</span> <span class="s1">&#39;(trajectory)&#39;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dc</span><span class="o">.</span><span class="n">num_steps_buffer_preload</span><span class="p">):</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">collect_step</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">train_env</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">random_policy</span><span class="p">,</span> <span class="n">replay_buffer</span><span class="o">=</span><span class="n">replay_buffer</span><span class="p">)</span>

        <span class="c1"># Train</span>

        <span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span><span class="p">,</span> <span class="n">autograph</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;replay_buffer.as_dataset&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;(num_parallel_calls=3, &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3)&#39;</span><span class="p">)</span>

        <span class="n">dataset</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">as_dataset</span><span class="p">(</span><span class="n">num_parallel_calls</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sample_batch_size</span><span class="o">=</span><span class="n">dc</span><span class="o">.</span><span class="n">num_steps_sampled_from_buffer</span><span class="p">,</span>

                                           <span class="n">num_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="n">iter_dataset</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;for each iteration&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;  replay_buffer.add_batch&#39;</span><span class="p">,</span> <span class="s1">&#39;(trajectory)&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;  tf_agent.train&#39;</span><span class="p">,</span> <span class="s1">&#39;(experience=trajectory)&#39;</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_begin</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dc</span><span class="o">.</span><span class="n">num_steps_per_iteration</span><span class="p">):</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">collect_step</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">train_env</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_policy</span><span class="p">,</span> <span class="n">replay_buffer</span><span class="o">=</span><span class="n">replay_buffer</span><span class="p">)</span>

            <span class="n">trajectories</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">iter_dataset</span><span class="p">)</span>

            <span class="n">tf_loss_info</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">experience</span><span class="o">=</span><span class="n">trajectories</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_end</span><span class="p">(</span><span class="n">tf_loss_info</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">train_context</span><span class="o">.</span><span class="n">training_done</span><span class="p">:</span>

                <span class="k">break</span>

        <span class="k">return</span>

<span class="c1"># noinspection PyUnresolvedReferences</span>

<span class="k">class</span> <span class="nc">TfPpoAgent</span><span class="p">(</span><span class="n">TfAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot; creates a new agent based on the PPO algorithm using the tfagents implementation.</span>

<span class="sd">        PPO is an actor-critic algorithm using 2 neural networks. The actor network</span>

<span class="sd">        to predict the next action to be taken and the critic network to estimate</span>

<span class="sd">        the value of the game state we are currently in (the expected, discounted</span>

<span class="sd">        sum of future rewards when following the current actor network).</span>

<span class="sd">        Args:</span>

<span class="sd">            model_config: the model configuration including the name of the target gym environment</span>

<span class="sd">                as well as the neural network architecture.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span>

    <span class="c1"># noinspection DuplicatedCode</span>

    <span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Tf-Agents Ppo Implementation of the train loop.&quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_context</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">PpoTrainContext</span><span class="p">)</span>

        <span class="n">tc</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">PpoTrainContext</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="n">train_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_env</span><span class="p">(</span><span class="n">discount</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">reward_discount_gamma</span><span class="p">)</span>

        <span class="n">observation_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">()</span>

        <span class="n">action_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">()</span>

        <span class="n">timestep_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">time_step_spec</span><span class="p">()</span>

        <span class="c1"># SetUp Optimizer, Networks and PpoAgent</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;AdamOptimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;ActorDistributionNetwork&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">actor_net</span> <span class="o">=</span> <span class="n">actor_distribution_network</span><span class="o">.</span><span class="n">ActorDistributionNetwork</span><span class="p">(</span><span class="n">observation_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">,</span>

                                                                        <span class="n">fc_layer_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">fc_layers</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;ValueNetwork&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">value_net</span> <span class="o">=</span> <span class="n">value_network</span><span class="o">.</span><span class="n">ValueNetwork</span><span class="p">(</span><span class="n">observation_spec</span><span class="p">,</span> <span class="n">fc_layer_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">fc_layers</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;PpoAgent&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">tf_agent</span> <span class="o">=</span> <span class="n">ppo_agent</span><span class="o">.</span><span class="n">PPOAgent</span><span class="p">(</span><span class="n">timestep_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span>

                                      <span class="n">actor_net</span><span class="o">=</span><span class="n">actor_net</span><span class="p">,</span> <span class="n">value_net</span><span class="o">=</span><span class="n">value_net</span><span class="p">,</span>

                                      <span class="n">num_epochs</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">num_epochs_per_iteration</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;tf_agent.initialize&#39;</span><span class="p">,</span><span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">tf_agent</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_policy</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">policy</span>

        <span class="c1"># SetUp Data collection &amp; Buffering</span>

        <span class="n">collect_data_spec</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_data_spec</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;TFUniformReplayBuffer&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">TFUniformReplayBuffer</span><span class="p">(</span><span class="n">collect_data_spec</span><span class="p">,</span>

                                              <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">max_steps_in_buffer</span><span class="p">)</span>

        <span class="n">collect_policy</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_policy</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;DynamicEpisodeDriver&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">collect_driver</span> <span class="o">=</span> <span class="n">DynamicEpisodeDriver</span><span class="p">(</span><span class="n">train_env</span><span class="p">,</span> <span class="n">collect_policy</span><span class="p">,</span> <span class="n">observers</span><span class="o">=</span><span class="p">[</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">add_batch</span><span class="p">],</span>

                                              <span class="n">num_episodes</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">num_episodes_per_iteration</span><span class="p">)</span>

        <span class="c1"># Train</span>

        <span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">,</span> <span class="n">autograph</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span><span class="p">,</span> <span class="n">autograph</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_begin</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;-----&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:&lt;4}      -----&#39;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;collect_driver.run&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

            <span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;replay_buffer.gather_all&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

            <span class="n">trajectories</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">gather_all</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;tf_agent.train&#39;</span><span class="p">,</span> <span class="s1">&#39;(experience=...)&#39;</span><span class="p">)</span>

            <span class="n">loss_info</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">experience</span><span class="o">=</span><span class="n">trajectories</span><span class="p">)</span>

            <span class="n">total_loss</span> <span class="o">=</span> <span class="n">loss_info</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

            <span class="n">actor_loss</span> <span class="o">=</span> <span class="n">loss_info</span><span class="o">.</span><span class="n">extra</span><span class="o">.</span><span class="n">policy_gradient_loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

            <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">loss_info</span><span class="o">.</span><span class="n">extra</span><span class="o">.</span><span class="n">value_estimation_loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;loss={total_loss:&lt;7.1f} [actor={actor_loss:&lt;7.1f} critic={critic_loss:&lt;7.1f}]&#39;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;replay_buffer.clear&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

            <span class="n">replay_buffer</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_end</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">total_loss</span><span class="p">,</span> <span class="n">actor_loss</span><span class="o">=</span><span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span><span class="o">=</span><span class="n">critic_loss</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">tc</span><span class="o">.</span><span class="n">training_done</span><span class="p">:</span>

                <span class="k">break</span>

        <span class="k">return</span>

<span class="c1"># noinspection PyUnresolvedReferences</span>

<span class="k">class</span> <span class="nc">TfRandomAgent</span><span class="p">(</span><span class="n">TfAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot; creates a new random agent based on uniform random actions.</span>

<span class="sd">        Args:</span>

<span class="sd">            model_config: the model configuration including the name of the target gym environment</span>

<span class="sd">                as well as the neural network architecture.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_set_trained_policy</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_set_trained_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Tf-Agents Random Implementation of the train loop.&quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;Creating environment...&#39;</span><span class="p">)</span>

        <span class="n">train_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_env</span><span class="p">()</span>

        <span class="n">action_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">()</span>

        <span class="n">timestep_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">time_step_spec</span><span class="p">()</span>

        <span class="c1"># SetUp Data collection &amp; Buffering</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;RandomTFPolicy&#39;</span><span class="p">,</span> <span class="s1">&#39;create&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_policy</span> <span class="o">=</span> <span class="n">random_tf_policy</span><span class="o">.</span><span class="n">RandomTFPolicy</span><span class="p">(</span><span class="n">timestep_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">)</span>

    <span class="c1"># noinspection DuplicatedCode</span>

    <span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Training...&quot;</span><span class="p">)</span>

        <span class="n">train_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_env</span><span class="p">()</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_begin</span><span class="p">()</span>

            <span class="c1"># ensure that 1 episode is played during the iteration</span>

            <span class="n">time_step</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

            <span class="k">while</span> <span class="ow">not</span> <span class="n">time_step</span><span class="o">.</span><span class="n">is_last</span><span class="p">():</span>

                <span class="n">action_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trained_policy</span><span class="o">.</span><span class="n">action</span><span class="p">(</span><span class="n">time_step</span><span class="p">)</span>

                <span class="n">time_step</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_step</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_end</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">train_context</span><span class="o">.</span><span class="n">training_done</span><span class="p">:</span>

                <span class="k">break</span>

        <span class="k">return</span>

<span class="c1"># noinspection PyUnresolvedReferences</span>

<span class="k">class</span> <span class="nc">TfReinforceAgent</span><span class="p">(</span><span class="n">TfAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot; creates a new agent based on the Reinforce algorithm using the tfagents implementation.</span>

<span class="sd">        Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict</span>

<span class="sd">        the actions.</span>

<span class="sd">        Args:</span>

<span class="sd">            model_config: the model configuration including the name of the target gym environment</span>

<span class="sd">                as well as the neural network architecture.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span>

    <span class="c1"># noinspection DuplicatedCode</span>

    <span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Tf-Agents Reinforce Implementation of the train loop.&quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_context</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">EpisodesTrainContext</span><span class="p">)</span>

        <span class="n">tc</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">EpisodesTrainContext</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;Creating environment...&#39;</span><span class="p">)</span>

        <span class="n">train_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_env</span><span class="p">(</span><span class="n">discount</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">reward_discount_gamma</span><span class="p">)</span>

        <span class="n">observation_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">()</span>

        <span class="n">action_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">()</span>

        <span class="n">timestep_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">time_step_spec</span><span class="p">()</span>

        <span class="c1"># SetUp Optimizer, Networks and PpoAgent</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;AdamOptimizer&#39;</span><span class="p">,</span> <span class="s1">&#39;create&#39;</span><span class="p">)</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;ActorDistributionNetwork&#39;</span><span class="p">,</span> <span class="s1">&#39;create&#39;</span><span class="p">)</span>

        <span class="n">actor_net</span> <span class="o">=</span> <span class="n">actor_distribution_network</span><span class="o">.</span><span class="n">ActorDistributionNetwork</span><span class="p">(</span><span class="n">observation_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">,</span>

                                                                        <span class="n">fc_layer_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">fc_layers</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;ReinforceAgent&#39;</span><span class="p">,</span> <span class="s1">&#39;create&#39;</span><span class="p">)</span>

        <span class="n">tf_agent</span> <span class="o">=</span> <span class="n">reinforce_agent</span><span class="o">.</span><span class="n">ReinforceAgent</span><span class="p">(</span><span class="n">timestep_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">,</span> <span class="n">actor_network</span><span class="o">=</span><span class="n">actor_net</span><span class="p">,</span>

                                                  <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;tf_agent.initialize()&#39;</span><span class="p">)</span>

        <span class="n">tf_agent</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_policy</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">policy</span>

        <span class="c1"># SetUp Data collection &amp; Buffering</span>

        <span class="n">collect_data_spec</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_data_spec</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;TFUniformReplayBuffer&#39;</span><span class="p">,</span> <span class="s1">&#39;create&#39;</span><span class="p">)</span>

        <span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">TFUniformReplayBuffer</span><span class="p">(</span><span class="n">collect_data_spec</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">max_steps_in_buffer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;DynamicEpisodeDriver&#39;</span><span class="p">,</span> <span class="s1">&#39;create&#39;</span><span class="p">)</span>

        <span class="n">collect_driver</span> <span class="o">=</span> <span class="n">DynamicEpisodeDriver</span><span class="p">(</span><span class="n">train_env</span><span class="p">,</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_policy</span><span class="p">,</span>

                                              <span class="n">observers</span><span class="o">=</span><span class="p">[</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">add_batch</span><span class="p">],</span>

                                              <span class="n">num_episodes</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">num_episodes_per_iteration</span><span class="p">)</span>

        <span class="c1"># Train</span>

        <span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">,</span> <span class="n">autograph</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span><span class="p">,</span> <span class="n">autograph</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;Starting training...&#39;</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_begin</span><span class="p">()</span>

            <span class="n">msg</span> <span class="o">=</span> <span class="n">f</span><span class="s1">&#39;iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:&lt;4}&#39;</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;collect_driver.run&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

            <span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;replay_buffer.gather_all&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

            <span class="n">trajectories</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">gather_all</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;tf_agent.train&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

            <span class="n">loss_info</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">experience</span><span class="o">=</span><span class="n">trajectories</span><span class="p">)</span>

            <span class="n">total_loss</span> <span class="o">=</span> <span class="n">loss_info</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;loss={total_loss:&lt;7.1f}&#39;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;replay_buffer.clear&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

            <span class="n">replay_buffer</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_end</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">total_loss</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">tc</span><span class="o">.</span><span class="n">training_done</span><span class="p">:</span>

                <span class="k">break</span>

        <span class="k">return</span>

<span class="c1"># noinspection PyUnresolvedReferences</span>

<span class="k">class</span> <span class="nc">TfSacAgent</span><span class="p">(</span><span class="n">TfAgent</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot; creates a new agent based on the SAC algorithm using the tfagents implementation.</span>

<span class="sd">        adapted from</span>

<span class="sd">            https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb</span>

<span class="sd">        Args:</span>

<span class="sd">            model_config: the model configuration including the name of the target gym environment</span>

<span class="sd">                as well as the neural network architecture.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_config</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span>

    <span class="c1"># noinspection DuplicatedCode</span>

    <span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Tf-Agents Ppo Implementation of the train loop.&quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">train_context</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span><span class="p">)</span>

        <span class="n">tc</span><span class="p">:</span> <span class="n">core</span><span class="o">.</span><span class="n">StepsTrainContext</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="n">train_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_env</span><span class="p">(</span><span class="n">discount</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">reward_discount_gamma</span><span class="p">)</span>

        <span class="n">observation_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">()</span>

        <span class="n">action_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">()</span>

        <span class="n">timestep_spec</span> <span class="o">=</span> <span class="n">train_env</span><span class="o">.</span><span class="n">time_step_spec</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;CriticNetwork&#39;</span><span class="p">,</span>

                     <span class="n">f</span><span class="s1">&#39;(observation_spec, action_spec), observation_fc_layer_params=None, &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers})&#39;</span><span class="p">)</span>

        <span class="n">critic_net</span> <span class="o">=</span> <span class="n">critic_network</span><span class="o">.</span><span class="n">CriticNetwork</span><span class="p">((</span><span class="n">observation_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">),</span>

                                                  <span class="n">observation_fc_layer_params</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">action_fc_layer_params</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>

                                                  <span class="n">joint_fc_layer_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">fc_layers</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">normal_projection_net</span><span class="p">(</span><span class="n">action_spec_arg</span><span class="p">,</span> <span class="n">init_means_output_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>

            <span class="k">return</span> <span class="n">normal_projection_network</span><span class="o">.</span><span class="n">NormalProjectionNetwork</span><span class="p">(</span><span class="n">action_spec_arg</span><span class="p">,</span>

                                                                     <span class="n">mean_transform</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>

                                                                     <span class="n">state_dependent_std</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>

                                                                     <span class="n">init_means_output_factor</span><span class="o">=</span><span class="n">init_means_output_factor</span><span class="p">,</span>

                                                                     <span class="n">std_transform</span><span class="o">=</span><span class="n">sac_agent</span><span class="o">.</span><span class="n">std_clip_transform</span><span class="p">,</span>

                                                                     <span class="n">scale_distribution</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;ActorDistributionNetwork&#39;</span><span class="p">,</span>

                     <span class="n">f</span><span class="s1">&#39;observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;continuous_projection_net=...)&#39;</span><span class="p">)</span>

        <span class="n">actor_net</span> <span class="o">=</span> <span class="n">actor_distribution_network</span><span class="o">.</span><span class="n">ActorDistributionNetwork</span><span class="p">(</span><span class="n">observation_spec</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">,</span>

                                                                        <span class="n">fc_layer_params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">fc_layers</span><span class="p">,</span>

                                                                        <span class="n">continuous_projection_net</span><span class="o">=</span><span class="n">normal_projection_net</span><span class="p">)</span>

        <span class="c1"># self.log_api(&#39;tf.compat.v1.train.get_or_create_global_step&#39;,&#39;()&#39;)</span>

        <span class="c1"># global_step = tf.compat.v1.train.get_or_create_global_step()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;SacAgent&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;(timestep_spec, action_spec, actor_network=..., critic_network=..., &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;gamma={tc.reward_discount_gamma})&#39;</span><span class="p">)</span>

        <span class="n">tf_agent</span> <span class="o">=</span> <span class="n">sac_agent</span><span class="o">.</span><span class="n">SacAgent</span><span class="p">(</span>

            <span class="n">timestep_spec</span><span class="p">,</span>

            <span class="n">action_spec</span><span class="p">,</span>

            <span class="n">actor_network</span><span class="o">=</span><span class="n">actor_net</span><span class="p">,</span>

            <span class="n">critic_network</span><span class="o">=</span><span class="n">critic_net</span><span class="p">,</span>

            <span class="n">actor_optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">),</span>

            <span class="n">critic_optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">),</span>

            <span class="n">alpha_optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">),</span>

            <span class="c1"># target_update_tau=0.005,</span>

            <span class="c1"># target_update_period=1,</span>

            <span class="c1"># td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,</span>

            <span class="n">gamma</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">reward_discount_gamma</span><span class="p">)</span>

        <span class="c1"># reward_scale_factor=1.0,</span>

        <span class="c1"># gradient_clipping=None,</span>

        <span class="c1"># train_step_counter=global_step)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;tf_agent.initialize&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="n">tf_agent</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_trained_policy</span> <span class="o">=</span> <span class="n">greedy_policy</span><span class="o">.</span><span class="n">GreedyPolicy</span><span class="p">(</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">policy</span><span class="p">)</span>

        <span class="n">collect_policy</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_policy</span>

        <span class="c1"># setup and preload replay buffer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;TFUniformReplayBuffer&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;(data_spec=tf_agent.collect_data_spec, &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer})&#39;</span><span class="p">)</span>

        <span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">tf_uniform_replay_buffer</span><span class="o">.</span><span class="n">TFUniformReplayBuffer</span><span class="p">(</span><span class="n">data_spec</span><span class="o">=</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_data_spec</span><span class="p">,</span>

                                                                       <span class="n">batch_size</span><span class="o">=</span><span class="n">train_env</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>

                                                                       <span class="n">max_length</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">max_steps_in_buffer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;DynamicStepDriver&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;(env, collect_policy, observers=[replay_buffer.add_batch], &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;num_steps={tc.num_steps_buffer_preload})&#39;</span><span class="p">)</span>

        <span class="n">initial_collect_driver</span> <span class="o">=</span> <span class="n">dynamic_step_driver</span><span class="o">.</span><span class="n">DynamicStepDriver</span><span class="p">(</span><span class="n">train_env</span><span class="p">,</span>

                                                                       <span class="n">collect_policy</span><span class="p">,</span>

                                                                       <span class="n">observers</span><span class="o">=</span><span class="p">[</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">add_batch</span><span class="p">],</span>

                                                                       <span class="n">num_steps</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">num_steps_buffer_preload</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;initial_collect_driver.run()&#39;</span><span class="p">)</span>

        <span class="n">initial_collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

        <span class="c1"># Dataset generates trajectories with shape [Bx2x...]</span>

        <span class="n">dataset</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">as_dataset</span><span class="p">(</span><span class="n">num_parallel_calls</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sample_batch_size</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">num_steps_sampled_from_buffer</span><span class="p">,</span>

                                           <span class="n">num_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;DynamicStepDriver&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;(env, collect_policy, observers=[replay_buffer.add_batch], &#39;</span> <span class="o">+</span>

                     <span class="n">f</span><span class="s1">&#39;num_steps={tc.num_steps_per_iteration})&#39;</span><span class="p">)</span>

        <span class="n">collect_driver</span> <span class="o">=</span> <span class="n">dynamic_step_driver</span><span class="o">.</span><span class="n">DynamicStepDriver</span><span class="p">(</span><span class="n">train_env</span><span class="p">,</span>

                                                               <span class="n">collect_policy</span><span class="p">,</span>

                                                               <span class="n">observers</span><span class="o">=</span><span class="p">[</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">add_batch</span><span class="p">],</span>

                                                               <span class="n">num_steps</span><span class="o">=</span><span class="n">tc</span><span class="o">.</span><span class="n">num_steps_per_iteration</span><span class="p">)</span>

        <span class="c1"># (Optional) Optimize by wrapping some of the code in a graph using TF function.</span>

        <span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span><span class="p">)</span>

        <span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;for each iteration&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;  collect_driver.run&#39;</span><span class="p">,</span> <span class="s1">&#39;()&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_api</span><span class="p">(</span><span class="s1">&#39;  tf_agent.train&#39;</span><span class="p">,</span> <span class="s1">&#39;(experience=...)&#39;</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_begin</span><span class="p">()</span>

            <span class="c1"># Collect a few steps using collect_policy and save to the replay buffer.</span>

            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tc</span><span class="o">.</span><span class="n">num_steps_per_iteration</span><span class="p">):</span>

                <span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

            <span class="c1"># Sample a batch of data from the buffer and update the agent&#39;s network.</span>

            <span class="n">experience</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>

            <span class="n">loss_info</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

            <span class="n">total_loss</span> <span class="o">=</span> <span class="n">loss_info</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

            <span class="n">actor_loss</span> <span class="o">=</span> <span class="n">loss_info</span><span class="o">.</span><span class="n">extra</span><span class="o">.</span><span class="n">actor_loss</span>

            <span class="n">alpha_loss</span> <span class="o">=</span> <span class="n">loss_info</span><span class="o">.</span><span class="n">extra</span><span class="o">.</span><span class="n">alpha_loss</span>

            <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">loss_info</span><span class="o">.</span><span class="n">extra</span><span class="o">.</span><span class="n">critic_loss</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">on_train_iteration_end</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">total_loss</span><span class="p">,</span> <span class="n">actor_loss</span><span class="o">=</span><span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span><span class="o">=</span><span class="n">critic_loss</span><span class="p">,</span>

                                        <span class="n">alpha_loss</span><span class="o">=</span><span class="n">alpha_loss</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">tc</span><span class="o">.</span><span class="n">training_done</span><span class="p">:</span>

                <span class="k">break</span>

        <span class="k">return</span>

<span class="k">class</span> <span class="nc">TfAgentAgentFactory</span><span class="p">(</span><span class="n">bcore</span><span class="o">.</span><span class="n">BackendAgentFactory</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Backend for TfAgents.</span>

<span class="sd">        Serves as a factory to create algorithm specific wrappers for the TfAgents implementations.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">backend_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;tfagents&#39;</span>

    <span class="k">def</span> <span class="nf">get_algorithms</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Type</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">BackendAgent</span><span class="p">]]:</span>

        <span class="sd">&quot;&quot;&quot;Yields a mapping of EasyAgent types to the implementations provided by this backend.&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="p">{</span><span class="n">easyagents</span><span class="o">.</span><span class="n">agents</span><span class="o">.</span><span class="n">DqnAgent</span><span class="p">:</span> <span class="n">TfDqnAgent</span><span class="p">,</span>

                <span class="n">easyagents</span><span class="o">.</span><span class="n">agents</span><span class="o">.</span><span class="n">PpoAgent</span><span class="p">:</span> <span class="n">TfPpoAgent</span><span class="p">,</span>

                <span class="n">easyagents</span><span class="o">.</span><span class="n">agents</span><span class="o">.</span><span class="n">RandomAgent</span><span class="p">:</span> <span class="n">TfRandomAgent</span><span class="p">,</span>

                <span class="n">easyagents</span><span class="o">.</span><span class="n">agents</span><span class="o">.</span><span class="n">ReinforceAgent</span><span class="p">:</span> <span class="n">TfReinforceAgent</span><span class="p">,</span>

                <span class="n">easyagents</span><span class="o">.</span><span class="n">agents</span><span class="o">.</span><span class="n">SacAgent</span><span class="p">:</span> <span class="n">TfSacAgent</span><span class="p">}</span>
</pre></div>


</details>
<h2 id="classes">Classes</h2>
<h3 id="tfagent">TfAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">TfAgent</span><span class="p">(</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span>
<span class="p">)</span>
</pre></div>


<p>Reinforcement learning agents based on googles tf_agent implementations</p>
<p>https://github.com/tensorflow/agents</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="nv">class</span> <span class="nv">TfAgent</span><span class="ss">(</span><span class="nv">bcore</span>.<span class="nv">BackendAgent</span>, <span class="nv">metaclass</span><span class="o">=</span><span class="nv">ABCMeta</span><span class="ss">)</span>:

    <span class="s2">&quot;&quot;&quot;</span><span class="s">Reinforcement learning agents based on googles tf_agent implementations</span>

        <span class="nv">https</span>:<span class="o">//</span><span class="nv">github</span>.<span class="nv">com</span><span class="o">/</span><span class="nv">tensorflow</span><span class="o">/</span><span class="nv">agents</span>

    <span class="s2">&quot;&quot;&quot;</span>

    <span class="nv">def</span> <span class="nv">__init__</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">model_config</span>: <span class="nv">core</span>.<span class="nv">ModelConfig</span><span class="ss">)</span>:

        <span class="nv">super</span><span class="ss">()</span>.<span class="nv">__init__</span><span class="ss">(</span><span class="nv">model_config</span><span class="o">=</span><span class="nv">model_config</span>, <span class="nv">backend_name</span><span class="o">=</span><span class="nv">TfAgentAgentFactory</span>.<span class="nv">backend_name</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="nv">self</span>.<span class="nv">_play_env</span>: <span class="nv">Optional</span>[<span class="nv">gym</span>.<span class="nv">Env</span>] <span class="o">=</span> <span class="nv">None</span>

    <span class="nv">def</span> <span class="nv">_create_gym_with_wrapper</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">discount</span><span class="ss">)</span>:

        <span class="nv">gym_spec</span> <span class="o">=</span> <span class="nv">gym</span>.<span class="nv">spec</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">gym_env_name</span><span class="ss">)</span>

        <span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">gym_spec</span>.<span class="nv">make</span><span class="ss">()</span>

        # <span class="nv">simplify_box_bounds</span>: <span class="nv">Whether</span> <span class="nv">to</span> <span class="nv">replace</span> <span class="nv">bounds</span> <span class="nv">of</span> <span class="nv">Box</span> <span class="nv">space</span> <span class="nv">that</span> <span class="nv">are</span> <span class="nv">arrays</span>

        #  <span class="nv">with</span> <span class="nv">identical</span> <span class="nv">values</span> <span class="nv">with</span> <span class="nv">one</span> <span class="nv">number</span> <span class="nv">and</span> <span class="nv">rely</span> <span class="nv">on</span> <span class="nv">broadcasting</span>.

        # <span class="nv">important</span>, <span class="nv">simplify_box_bounds</span> <span class="nv">True</span> <span class="nv">crashes</span> <span class="nv">environments</span> <span class="nv">with</span> <span class="nv">boundaries</span> <span class="nv">with</span> <span class="nv">identical</span> <span class="nv">values</span>

        <span class="nv">env</span> <span class="o">=</span> <span class="nv">gym_wrapper</span>.<span class="nv">GymWrapper</span><span class="ss">(</span>

            <span class="nv">gym_env</span>,

            <span class="nv">discount</span><span class="o">=</span><span class="nv">discount</span>,

            <span class="nv">simplify_box_bounds</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="k">return</span> <span class="nv">env</span>

    <span class="nv">def</span> <span class="nv">_create_env</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">discount</span>: <span class="nv">float</span> <span class="o">=</span> <span class="mi">1</span><span class="ss">)</span> <span class="o">-&gt;</span> <span class="nv">tf_py_environment</span>.<span class="nv">TFPyEnvironment</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s"> creates a new instance of the gym environment and wraps it in a tfagent TFPyEnvironment</span>

            <span class="nv">Args</span>:

                <span class="nv">discount</span>: <span class="nv">the</span> <span class="nv">reward</span> <span class="nv">discount</span> <span class="nv">factor</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="nv">discount</span> <span class="o">&lt;=</span> <span class="mi">1</span>, <span class="s2">&quot;</span><span class="s">discount not admissible</span><span class="s2">&quot;</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="nv">f</span><span class="s1">&#39;</span><span class="s">TFPyEnvironment</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">( suite_gym.load( ... ) )</span><span class="s1">&#39;</span><span class="ss">)</span>

        # <span class="nv">suit_gym</span>.<span class="nv">load</span> <span class="nv">crashes</span> <span class="nv">our</span> <span class="nv">environment</span>

        # <span class="nv">py_env</span> <span class="o">=</span> <span class="nv">suite_gym</span>.<span class="nv">load</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">gym_env_name</span>, <span class="nv">discount</span><span class="o">=</span><span class="nv">discount</span><span class="ss">)</span>

        <span class="nv">py_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_gym_with_wrapper</span><span class="ss">(</span><span class="nv">discount</span><span class="ss">)</span>

        <span class="nb">result</span> <span class="o">=</span> <span class="nv">tf_py_environment</span>.<span class="nv">TFPyEnvironment</span><span class="ss">(</span><span class="nv">py_env</span><span class="ss">)</span>

        <span class="k">return</span> <span class="nb">result</span>

    <span class="nv">def</span> <span class="nv">_get_gym_env</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">tf_py_env</span>: <span class="nv">tf_py_environment</span>.<span class="nv">TFPyEnvironment</span><span class="ss">)</span> <span class="o">-&gt;</span> <span class="nv">monitor</span>.<span class="nv">_MonitorEnv</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s"> extracts the underlying _MonitorEnv from tf_py_env created by _create_tfagent_env</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">tf_py_env</span>, <span class="nv">tf_py_environment</span>.<span class="nv">TFPyEnvironment</span><span class="ss">)</span>, \

            <span class="s2">&quot;</span><span class="s">passed tf_py_env is not an instance of TFPyEnvironment</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">tf_py_env</span>.<span class="nv">pyenv</span>, <span class="nv">py_environment</span>.<span class="nv">PyEnvironment</span><span class="ss">)</span>, \

            <span class="s2">&quot;</span><span class="s">passed TFPyEnvironment.pyenv does not contain a PyEnvironment</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">len</span><span class="ss">(</span><span class="nv">tf_py_env</span>.<span class="nv">pyenv</span>.<span class="nv">envs</span><span class="ss">)</span> <span class="o">==</span> <span class="mi">1</span>, <span class="s2">&quot;</span><span class="s">passed TFPyEnvironment.pyenv does not contain a unique environment</span><span class="s2">&quot;</span>

        <span class="nb">result</span> <span class="o">=</span> <span class="nv">tf_py_env</span>.<span class="nv">pyenv</span>.<span class="nv">envs</span>[<span class="mi">0</span>].<span class="nv">gym</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nb">result</span>, <span class="nv">monitor</span>.<span class="nv">_MonitorEnv</span><span class="ss">)</span>, <span class="s2">&quot;</span><span class="s">passed TFPyEnvironment does not contain a _MonitorEnv</span><span class="s2">&quot;</span>

        <span class="k">return</span> <span class="nb">result</span>

    <span class="nv">def</span> <span class="nv">play_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">play_context</span>: <span class="nv">core</span>.<span class="nv">PlayContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Agent specific implementation of playing a single episodes with the current policy.</span>

            <span class="nv">Args</span>:

                <span class="nv">play_context</span>: <span class="nv">play</span> <span class="nv">configuration</span> <span class="nv">to</span> <span class="nv">be</span> <span class="nv">used</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">play_context</span>, <span class="s2">&quot;</span><span class="s">play_context not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>, <span class="s2">&quot;</span><span class="s">trained_policy not set. call train() first.</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_get_gym_env</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_play_env</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">gym_env</span><span class="ss">)</span>

            <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">reset</span><span class="ss">()</span>

            <span class="k">while</span> <span class="nv">not</span> <span class="nv">time_step</span>.<span class="nv">is_last</span><span class="ss">()</span>:

                <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

                <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_play_episode_end</span><span class="ss">()</span>

            <span class="k">if</span> <span class="nv">play_context</span>.<span class="nv">play_done</span>:

                <span class="k">break</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.backends.core.BackendAgent</li>
<li>easyagents.backends.core._BackendAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="descendants">Descendants</h4>
<ul>
<li>easyagents.backends.tfagents.TfDqnAgent</li>
<li>easyagents.backends.tfagents.TfPpoAgent</li>
<li>easyagents.backends.tfagents.TfRandomAgent</li>
<li>easyagents.backends.tfagents.TfReinforceAgent</li>
<li>easyagents.backends.tfagents.TfSacAgent</li>
</ul>
<h4 id="methods">Methods</h4>
<h5 id="log">log</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span>
</pre></div>


<p>Logs msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">log_msg</span>: <span class="nv">str</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="log_api">log_api</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log_api</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">api_target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Logs a call to api_target with additional log_msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log_api</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">api_target</span>: <span class="nv">str</span>, <span class="nv">log_msg</span>: <span class="nv">Optional</span>[<span class="nv">str</span>] <span class="o">=</span> <span class="nv">None</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs a call to api_target with additional log_msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">api_target</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">api_target</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_api_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">api_target</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_begin">on_play_episode_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_begin</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Env</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the beginning of a new episode</p>
<p>Args:
    env: the gym environment used to play the episode.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">env</span>: <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the beginning of a new episode</span>

        <span class="nv">Args</span>:

            <span class="nv">env</span>: <span class="nv">the</span> <span class="nv">gym</span> <span class="nv">environment</span> <span class="nv">used</span> <span class="nv">to</span> <span class="nv">play</span> <span class="nv">the</span> <span class="nv">episode</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">env</span>, <span class="s2">&quot;</span><span class="s">env not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">env</span>, <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>, <span class="s2">&quot;</span><span class="s">env not an an instance of gym.Env.</span><span class="s2">&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">env</span>

        <span class="nv">pc</span>.<span class="nv">steps_done_in_episode</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">pc</span>.<span class="nv">actions</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">sum_of_rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_end">on_play_episode_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the end of an episode</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the end of an episode</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span> <span class="nv">and</span> <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">&gt;=</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span>:

            <span class="nv">pc</span>.<span class="nv">play_done</span> <span class="o">=</span> <span class="nv">True</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_begin">on_train_iteration_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_begin</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the begining of a new iteration</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the begining of a new iteration</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">tc</span>.<span class="nv">steps_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">==</span> <span class="mi">0</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>.<span class="nv">episodes_done</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_end">on_train_iteration_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the end of an iteration</p>
<p>Evaluates the current policy. Use kwargs to set additional dict values in train context.
Eg for an ActorCriticTrainContext the losses may be set like this:
    on_train_iteration(loss=123,actor_loss=456,critic_loss=789)</p>
<p>Args:
    loss: loss after the training of the model in this iteration or math.nan if the loss is not available
    **kwargs: if a keyword matches a dict property of the TrainContext instance, then
                the dict[episodes_done_in_training] is set to the arg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">loss</span>: <span class="nv">float</span>, <span class="o">**</span><span class="nv">kwargs</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the end of an iteration</span>

        <span class="nv">Evaluates</span> <span class="nv">the</span> <span class="nv">current</span> <span class="nv">policy</span>. <span class="nv">Use</span> <span class="nv">kwargs</span> <span class="nv">to</span> <span class="nv">set</span> <span class="nv">additional</span> <span class="nv">dict</span> <span class="nv">values</span> <span class="nv">in</span> <span class="nv">train</span> <span class="nv">context</span>.

        <span class="nv">Eg</span> <span class="k">for</span> <span class="nv">an</span> <span class="nv">ActorCriticTrainContext</span> <span class="nv">the</span> <span class="nv">losses</span> <span class="nv">may</span> <span class="nv">be</span> <span class="nv">set</span> <span class="nv">like</span> <span class="nv">this</span>:

            <span class="nv">on_train_iteration</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="mi">123</span>,<span class="nv">actor_loss</span><span class="o">=</span><span class="mi">456</span>,<span class="nv">critic_loss</span><span class="o">=</span><span class="mi">789</span><span class="ss">)</span>

        <span class="nv">Args</span>:

            <span class="nv">loss</span>: <span class="nv">loss</span> <span class="nv">after</span> <span class="nv">the</span> <span class="nv">training</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">model</span> <span class="nv">in</span> <span class="nv">this</span> <span class="nv">iteration</span> <span class="nv">or</span> <span class="nv">math</span>.<span class="nv">nan</span> <span class="k">if</span> <span class="nv">the</span> <span class="nv">loss</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">available</span>

            <span class="o">**</span><span class="nv">kwargs</span>: <span class="k">if</span> <span class="nv">a</span> <span class="nv">keyword</span> <span class="nv">matches</span> <span class="nv">a</span> <span class="nv">dict</span> <span class="nv">property</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">TrainContext</span> <span class="nv">instance</span>, <span class="k">then</span>

                        <span class="nv">the</span> <span class="nv">dict</span>[<span class="nv">episodes_done_in_training</span>] <span class="nv">is</span> <span class="nv">set</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">arg</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">totals</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="ss">(</span><span class="nv">totals</span>.<span class="nv">episodes_done</span> <span class="o">-</span> <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span><span class="ss">)</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span> <span class="o">+=</span> <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span>

        <span class="nv">tc</span>.<span class="nv">loss</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">loss</span>

        # <span class="nv">set</span> <span class="nv">traincontext</span> <span class="nv">dict</span> <span class="nv">from</span> <span class="nv">kwargs</span>:

        <span class="k">for</span> <span class="nv">prop_name</span> <span class="nv">in</span> <span class="nv">kwargs</span>:

            <span class="nv">prop_instance</span> <span class="o">=</span> <span class="nv">getattr</span><span class="ss">(</span><span class="nv">tc</span>, <span class="nv">prop_name</span>, <span class="nv">None</span><span class="ss">)</span>

            <span class="nv">prop_value</span> <span class="o">=</span> <span class="nv">kwargs</span>[<span class="nv">prop_name</span>]

            <span class="k">if</span> <span class="nv">prop_instance</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span> <span class="nv">and</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">prop_instance</span>, <span class="nv">dict</span><span class="ss">)</span>:

                <span class="nv">prop_instance</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">prop_value</span>

        <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span>:

            <span class="nv">tc</span>.<span class="nv">training_done</span> <span class="o">=</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">&gt;=</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="nv">and</span> <span class="ss">(</span><span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">%</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="o">==</span> <span class="mi">0</span><span class="ss">)</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="play">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to play_implementation overriden by the subclass.</p>
<p>Args:
    play_context: play configuration to be used
    callbacks: list of callbacks called during play.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to play_implementation overriden by the subclass.</span>

<span class="ss">            Args:</span>

<span class="ss">                play_context: play configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during play.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">play_context</span><span class="p">,</span> <span class="ss">&quot;play_context not set&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="k">is</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;play_context already set in agent_context&quot;</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="n">play_context</span>

        <span class="n">old_callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">play_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">old_callbacks</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="play_implementation">play_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span>
<span class="p">)</span>
</pre></div>


<p>Agent specific implementation of playing a single episodes with the current policy.</p>
<p>Args:
    play_context: play configuration to be used</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">play_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">play_context</span>: <span class="nv">core</span>.<span class="nv">PlayContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Agent specific implementation of playing a single episodes with the current policy.</span>

            <span class="nv">Args</span>:

                <span class="nv">play_context</span>: <span class="nv">play</span> <span class="nv">configuration</span> <span class="nv">to</span> <span class="nv">be</span> <span class="nv">used</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">play_context</span>, <span class="s2">&quot;</span><span class="s">play_context not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>, <span class="s2">&quot;</span><span class="s">trained_policy not set. call train() first.</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_get_gym_env</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_play_env</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">gym_env</span><span class="ss">)</span>

            <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">reset</span><span class="ss">()</span>

            <span class="k">while</span> <span class="nv">not</span> <span class="nv">time_step</span>.<span class="nv">is_last</span><span class="ss">()</span>:

                <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

                <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_play_episode_end</span><span class="ss">()</span>

            <span class="k">if</span> <span class="nv">play_context</span>.<span class="nv">play_done</span>:

                <span class="k">break</span>
</pre></div>


</details>
<h5 id="train">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to train_implementation overriden by the subclass</p>
<p>Args:
    train_context: training configuration to be used
    callbacks: list of callbacks called during the training and evaluation.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to train_implementation overriden by the subclass</span>

<span class="ss">            Args:</span>

<span class="ss">                train_context: training configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during the training and evaluation.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">train_context</span><span class="p">,</span> <span class="ss">&quot;train_context not set&quot;</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="k">self</span><span class="p">.</span><span class="n">log_api</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;backend_name&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;{self._backend_name}&#39;</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_set_seed</span><span class="p">()</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">train_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="train_implementation">train_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span>
<span class="p">)</span>
</pre></div>


<p>Agent specific implementation of the train loop.</p>
<p>The implementation should have the form:</p>
<p>while True:
    on_iteration_begin
    for e in num_episodes_per_iterations
        play episode and record steps (while steps_in_episode &lt; max_steps_per_episode and)
    train policy for num_epochs_per_iteration epochs
    on_iteration_end( loss )
    if training_done
        break</p>
<p>Args:
    train_context: context configuring the train loop</p>
<p>Hints:
o the subclasses training loss is passed through to BackendAgent by on_iteration_end.
  Thus the subclass must not add the experienced loss to the TrainContext.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    @<span class="nv">abstractmethod</span>

    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Agent specific implementation of the train loop.</span>

            <span class="nv">The</span> <span class="nv">implementation</span> <span class="nv">should</span> <span class="nv">have</span> <span class="nv">the</span> <span class="nv">form</span>:

            <span class="k">while</span> <span class="nv">True</span>:

                <span class="nv">on_iteration_begin</span>

                <span class="k">for</span> <span class="nv">e</span> <span class="nv">in</span> <span class="nv">num_episodes_per_iterations</span>

                    <span class="nv">play</span> <span class="nv">episode</span> <span class="nv">and</span> <span class="nv">record</span> <span class="nv">steps</span> <span class="ss">(</span><span class="k">while</span> <span class="nv">steps_in_episode</span> <span class="o">&lt;</span> <span class="nv">max_steps_per_episode</span> <span class="nv">and</span><span class="ss">)</span>

                <span class="nv">train</span> <span class="nv">policy</span> <span class="k">for</span> <span class="nv">num_epochs_per_iteration</span> <span class="nv">epochs</span>

                <span class="nv">on_iteration_end</span><span class="ss">(</span> <span class="nv">loss</span> <span class="ss">)</span>

                <span class="k">if</span> <span class="nv">training_done</span>

                    <span class="k">break</span>

            <span class="nv">Args</span>:

                <span class="nv">train_context</span>: <span class="nv">context</span> <span class="nv">configuring</span> <span class="nv">the</span> <span class="nv">train</span> <span class="k">loop</span>

            <span class="nv">Hints</span>:

            <span class="nv">o</span> <span class="nv">the</span> <span class="nv">subclasses</span> <span class="nv">training</span> <span class="nv">loss</span> <span class="nv">is</span> <span class="nv">passed</span> <span class="nv">through</span> <span class="nv">to</span> <span class="nv">BackendAgent</span> <span class="nv">by</span> <span class="nv">on_iteration_end</span>.

              <span class="nv">Thus</span> <span class="nv">the</span> <span class="nv">subclass</span> <span class="nv">must</span> <span class="nv">not</span> <span class="nv">add</span> <span class="nv">the</span> <span class="nv">experienced</span> <span class="nv">loss</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">TrainContext</span>.

        <span class="s2">&quot;&quot;&quot;</span>
</pre></div>


</details>
<h3 id="tfagentagentfactory">TfAgentAgentFactory</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">TfAgentAgentFactory</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<p>Backend for TfAgents.</p>
<p>Serves as a factory to create algorithm specific wrappers for the TfAgents implementations.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="nv">class</span> <span class="nv">TfAgentAgentFactory</span><span class="ss">(</span><span class="nv">bcore</span>.<span class="nv">BackendAgentFactory</span><span class="ss">)</span>:

    <span class="s2">&quot;&quot;&quot;</span><span class="s">Backend for TfAgents.</span>

        <span class="nv">Serves</span> <span class="nv">as</span> <span class="nv">a</span> <span class="nv">factory</span> <span class="nv">to</span> <span class="nv">create</span> <span class="nv">algorithm</span> <span class="nv">specific</span> <span class="nv">wrappers</span> <span class="k">for</span> <span class="nv">the</span> <span class="nv">TfAgents</span> <span class="nv">implementations</span>.

    <span class="s2">&quot;&quot;&quot;</span>

    <span class="nv">backend_name</span>: <span class="nv">str</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="s">tfagents</span><span class="s1">&#39;</span>

    <span class="nv">def</span> <span class="nv">get_algorithms</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span> <span class="o">-&gt;</span> <span class="nv">Dict</span>[<span class="nv">Type</span>, <span class="nv">Type</span>[<span class="nv">easyagents</span>.<span class="nv">backends</span>.<span class="nv">core</span>.<span class="nv">BackendAgent</span>]]:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Yields a mapping of EasyAgent types to the implementations provided by this backend.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="k">return</span> {<span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">DqnAgent</span>: <span class="nv">TfDqnAgent</span>,

                <span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">PpoAgent</span>: <span class="nv">TfPpoAgent</span>,

                <span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">RandomAgent</span>: <span class="nv">TfRandomAgent</span>,

                <span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">ReinforceAgent</span>: <span class="nv">TfReinforceAgent</span>,

                <span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">SacAgent</span>: <span class="nv">TfSacAgent</span>}
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.backends.core.BackendAgentFactory</li>
<li>abc.ABC</li>
</ul>
<h4 id="class-variables">Class variables</h4>
<div class="codehilite"><pre><span></span><span class="n">backend_name</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="n">tensorflow_v2_eager_compatible</span>
</pre></div>


<h4 id="methods_1">Methods</h4>
<h5 id="create_agent">create_agent</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">create_agent</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">easyagent_type</span><span class="p">:</span> <span class="n">Type</span><span class="p">,</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">_BackendAgent</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span>
</pre></div>


<p>Creates a backend agent instance implementing the algorithm given by agent_type.</p>
<p>Args:
    easyagent_type: the EasyAgent derived type for which an implementing backend instance will be created
    model_config: the model_config passed to the constructor of the backend instance.</p>
<p>Returns:
    instance of the agent or None if not implemented by this backend.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">create_agent</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">easyagent_type</span>: <span class="nv">Type</span>, <span class="nv">model_config</span>: <span class="nv">core</span>.<span class="nv">ModelConfig</span><span class="ss">)</span> \

            <span class="o">-&gt;</span> <span class="nv">Optional</span>[<span class="nv">_BackendAgent</span>]:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Creates a backend agent instance implementing the algorithm given by agent_type.</span>

        <span class="nv">Args</span>:

            <span class="nv">easyagent_type</span>: <span class="nv">the</span> <span class="nv">EasyAgent</span> <span class="nv">derived</span> <span class="nv">type</span> <span class="k">for</span> <span class="nv">which</span> <span class="nv">an</span> <span class="nv">implementing</span> <span class="nv">backend</span> <span class="nv">instance</span> <span class="nv">will</span> <span class="nv">be</span> <span class="nv">created</span>

            <span class="nv">model_config</span>: <span class="nv">the</span> <span class="nv">model_config</span> <span class="nv">passed</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">constructor</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">backend</span> <span class="nv">instance</span>.

        <span class="nv">Returns</span>:

            <span class="nv">instance</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">agent</span> <span class="nv">or</span> <span class="nv">None</span> <span class="k">if</span> <span class="nv">not</span> <span class="nv">implemented</span> <span class="nv">by</span> <span class="nv">this</span> <span class="nv">backend</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nb">result</span>: <span class="nv">Optional</span>[<span class="nv">_BackendAgent</span>] <span class="o">=</span> <span class="nv">None</span>

        <span class="nv">algorithms</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">get_algorithms</span><span class="ss">()</span>

        <span class="k">if</span> <span class="nv">easyagent_type</span> <span class="nv">in</span> <span class="nv">algorithms</span>:

            <span class="nb">result</span> <span class="o">=</span> <span class="nv">algorithms</span>[<span class="nv">easyagent_type</span>]<span class="ss">(</span><span class="nv">model_config</span><span class="o">=</span><span class="nv">model_config</span><span class="ss">)</span>

        <span class="k">return</span> <span class="nb">result</span>
</pre></div>


</details>
<h5 id="get_algorithms">get_algorithms</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_algorithms</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Type</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">BackendAgent</span><span class="p">]]</span>
</pre></div>


<p>Yields a mapping of EasyAgent types to the implementations provided by this backend.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">get_algorithms</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span> <span class="o">-&gt;</span> <span class="nv">Dict</span>[<span class="nv">Type</span>, <span class="nv">Type</span>[<span class="nv">easyagents</span>.<span class="nv">backends</span>.<span class="nv">core</span>.<span class="nv">BackendAgent</span>]]:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Yields a mapping of EasyAgent types to the implementations provided by this backend.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="k">return</span> {<span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">DqnAgent</span>: <span class="nv">TfDqnAgent</span>,

                <span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">PpoAgent</span>: <span class="nv">TfPpoAgent</span>,

                <span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">RandomAgent</span>: <span class="nv">TfRandomAgent</span>,

                <span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">ReinforceAgent</span>: <span class="nv">TfReinforceAgent</span>,

                <span class="nv">easyagents</span>.<span class="nv">agents</span>.<span class="nv">SacAgent</span>: <span class="nv">TfSacAgent</span>}
</pre></div>


</details>
<h3 id="tfdqnagent">TfDqnAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">TfDqnAgent</span><span class="p">(</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span>
<span class="p">)</span>
</pre></div>


<p>creates a new agent based on the DQN algorithm using the tfagents implementation.</p>
<p>Args:
    model_config: the model configuration including the name of the target gym environment
        as well as the neural network architecture.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="nv">class</span> <span class="nv">TfDqnAgent</span><span class="ss">(</span><span class="nv">TfAgent</span><span class="ss">)</span>:

    <span class="s2">&quot;&quot;&quot;</span><span class="s"> creates a new agent based on the DQN algorithm using the tfagents implementation.</span>

        <span class="nv">Args</span>:

            <span class="nv">model_config</span>: <span class="nv">the</span> <span class="nv">model</span> <span class="nv">configuration</span> <span class="nv">including</span> <span class="nv">the</span> <span class="nv">name</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">target</span> <span class="nv">gym</span> <span class="nv">environment</span>

                <span class="nv">as</span> <span class="nv">well</span> <span class="nv">as</span> <span class="nv">the</span> <span class="nv">neural</span> <span class="nv">network</span> <span class="nv">architecture</span>.

    <span class="s2">&quot;&quot;&quot;</span>

    <span class="nv">def</span> <span class="nv">__init__</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">model_config</span>: <span class="nv">core</span>.<span class="nv">ModelConfig</span><span class="ss">)</span>:

        <span class="nv">super</span><span class="ss">()</span>.<span class="nv">__init__</span><span class="ss">(</span><span class="nv">model_config</span><span class="o">=</span><span class="nv">model_config</span><span class="ss">)</span>

    <span class="nv">def</span> <span class="nv">collect_step</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">env</span>: <span class="nv">tf_py_environment</span>.<span class="nv">TFPyEnvironment</span>, <span class="nv">policy</span>: <span class="nv">tf_policy</span>.<span class="nv">Base</span>,

                     <span class="nv">replay_buffer</span>: <span class="nv">TFUniformReplayBuffer</span><span class="ss">)</span>:

        <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">env</span>.<span class="nv">current_time_step</span><span class="ss">()</span>

        <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

        <span class="nv">next_time_step</span> <span class="o">=</span> <span class="nv">env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

        <span class="nv">traj</span> <span class="o">=</span> <span class="nv">trajectory</span>.<span class="nv">from_transition</span><span class="ss">(</span><span class="nv">time_step</span>, <span class="nv">action_step</span>, <span class="nv">next_time_step</span><span class="ss">)</span>

        <span class="nv">replay_buffer</span>.<span class="nv">add_batch</span><span class="ss">(</span><span class="nv">traj</span><span class="ss">)</span>

    # <span class="nv">noinspection</span> <span class="nv">DuplicatedCode</span>

    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Tf-Agents Ppo Implementation of the train loop.</span>

        <span class="nv">The</span> <span class="nv">implementation</span> <span class="nv">follows</span>

        <span class="nv">https</span>:<span class="o">//</span><span class="nv">colab</span>.<span class="nv">research</span>.<span class="nv">google</span>.<span class="nv">com</span><span class="o">/</span><span class="nv">github</span><span class="o">/</span><span class="nv">tensorflow</span><span class="o">/</span><span class="nv">agents</span><span class="o">/</span><span class="nv">blob</span><span class="o">/</span><span class="nv">master</span><span class="o">/</span><span class="nv">tf_agents</span><span class="o">/</span><span class="nv">colabs</span><span class="o">/</span><span class="mi">1</span><span class="nv">_dqn_tutorial</span>.<span class="nv">ipynb</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">train_context</span>, <span class="nv">core</span>.<span class="nv">StepsTrainContext</span><span class="ss">)</span>

        <span class="nv">dc</span>: <span class="nv">core</span>.<span class="nv">StepsTrainContext</span> <span class="o">=</span> <span class="nv">train_context</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">(</span><span class="nv">discount</span><span class="o">=</span><span class="nv">dc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        <span class="nv">observation_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">observation_spec</span><span class="ss">()</span>

        <span class="nv">action_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">action_spec</span><span class="ss">()</span>

        <span class="nv">timestep_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">time_step_spec</span><span class="ss">()</span>

        # <span class="nv">SetUp</span> <span class="nv">Optimizer</span>, <span class="nv">Networks</span> <span class="nv">and</span> <span class="nv">DqnAgent</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">AdamOptimizer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">optimizer</span> <span class="o">=</span> <span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">dc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">QNetwork</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">q_net</span> <span class="o">=</span> <span class="nv">q_network</span>.<span class="nv">QNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span>, <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DqnAgent</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span> <span class="o">=</span> <span class="nv">dqn_agent</span>.<span class="nv">DqnAgent</span><span class="ss">(</span><span class="nv">timestep_spec</span>, <span class="nv">action_spec</span>,

                                      <span class="nv">q_network</span><span class="o">=</span><span class="nv">q_net</span>, <span class="nv">optimizer</span><span class="o">=</span><span class="nv">optimizer</span>,

                                      <span class="nv">td_errors_loss_fn</span><span class="o">=</span><span class="nv">common</span>.<span class="nv">element_wise_squared_loss</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.initialize</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">initialize</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">policy</span>

        # <span class="nv">SetUp</span> <span class="nv">Data</span> <span class="nv">collection</span> <span class="o">&amp;</span> <span class="nv">Buffering</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">TFUniformReplayBuffer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">replay_buffer</span> <span class="o">=</span> <span class="nv">TFUniformReplayBuffer</span><span class="ss">(</span><span class="nv">data_spec</span><span class="o">=</span><span class="nv">tf_agent</span>.<span class="nv">collect_data_spec</span>,

                                              <span class="nv">batch_size</span><span class="o">=</span><span class="nv">train_env</span>.<span class="nv">batch_size</span>,

                                              <span class="nv">max_length</span><span class="o">=</span><span class="nv">dc</span>.<span class="nv">max_steps_in_buffer</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">RandomTFPolicy</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">random_policy</span> <span class="o">=</span> <span class="nv">random_tf_policy</span>.<span class="nv">RandomTFPolicy</span><span class="ss">(</span><span class="nv">timestep_spec</span>, <span class="nv">action_spec</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.add_batch</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(trajectory)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="k">for</span> <span class="nv">_</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="nv">dc</span>.<span class="nv">num_steps_buffer_preload</span><span class="ss">)</span>:

            <span class="nv">self</span>.<span class="nv">collect_step</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">train_env</span>, <span class="nv">policy</span><span class="o">=</span><span class="nv">random_policy</span>, <span class="nv">replay_buffer</span><span class="o">=</span><span class="nv">replay_buffer</span><span class="ss">)</span>

        # <span class="nv">Train</span>

        <span class="nv">tf_agent</span>.<span class="nv">train</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">train</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.as_dataset</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(num_parallel_calls=3, </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">dataset</span> <span class="o">=</span> <span class="nv">replay_buffer</span>.<span class="nv">as_dataset</span><span class="ss">(</span><span class="nv">num_parallel_calls</span><span class="o">=</span><span class="mi">3</span>, <span class="nv">sample_batch_size</span><span class="o">=</span><span class="nv">dc</span>.<span class="nv">num_steps_sampled_from_buffer</span>,

                                           <span class="nv">num_steps</span><span class="o">=</span><span class="mi">2</span><span class="ss">)</span>.<span class="nv">prefetch</span><span class="ss">(</span><span class="mi">3</span><span class="ss">)</span>

        <span class="nv">iter_dataset</span> <span class="o">=</span> <span class="nv">iter</span><span class="ss">(</span><span class="nv">dataset</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">for each iteration</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">  replay_buffer.add_batch</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(trajectory)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">  tf_agent.train</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(experience=trajectory)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            <span class="k">for</span> <span class="nv">_</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="nv">dc</span>.<span class="nv">num_steps_per_iteration</span><span class="ss">)</span>:

                <span class="nv">self</span>.<span class="nv">collect_step</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">train_env</span>, <span class="nv">policy</span><span class="o">=</span><span class="nv">tf_agent</span>.<span class="nv">collect_policy</span>, <span class="nv">replay_buffer</span><span class="o">=</span><span class="nv">replay_buffer</span><span class="ss">)</span>

            <span class="nv">trajectories</span>, <span class="nv">_</span> <span class="o">=</span> <span class="k">next</span><span class="ss">(</span><span class="nv">iter_dataset</span><span class="ss">)</span>

            <span class="nv">tf_loss_info</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">(</span><span class="nv">experience</span><span class="o">=</span><span class="nv">trajectories</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">tf_loss_info</span>.<span class="nv">loss</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">train_context</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_2">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.backends.tfagents.TfAgent</li>
<li>easyagents.backends.core.BackendAgent</li>
<li>easyagents.backends.core._BackendAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_2">Methods</h4>
<h5 id="collect_step">collect_step</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">collect_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">tf_agents</span><span class="o">.</span><span class="n">environments</span><span class="o">.</span><span class="n">tf_py_environment</span><span class="o">.</span><span class="n">TFPyEnvironment</span><span class="p">,</span>
    <span class="n">policy</span><span class="p">:</span> <span class="n">tf_agents</span><span class="o">.</span><span class="n">policies</span><span class="o">.</span><span class="n">tf_policy</span><span class="o">.</span><span class="n">Base</span><span class="p">,</span>
    <span class="n">replay_buffer</span><span class="p">:</span> <span class="n">tf_agents</span><span class="o">.</span><span class="n">replay_buffers</span><span class="o">.</span><span class="n">tf_uniform_replay_buffer</span><span class="o">.</span><span class="n">TFUniformReplayBuffer</span>
<span class="p">)</span>
</pre></div>


<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">collect_step</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">env</span><span class="p">:</span> <span class="n">tf_py_environment</span><span class="p">.</span><span class="n">TFPyEnvironment</span><span class="p">,</span> <span class="n">policy</span><span class="p">:</span> <span class="n">tf_policy</span><span class="p">.</span><span class="n">Base</span><span class="p">,</span>

                     <span class="n">replay_buffer</span><span class="p">:</span> <span class="n">TFUniformReplayBuffer</span><span class="p">):</span>

        <span class="n">time_step</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">current_time_step</span><span class="p">()</span>

        <span class="n">action_step</span> <span class="o">=</span> <span class="n">policy</span><span class="p">.</span><span class="n">action</span><span class="p">(</span><span class="n">time_step</span><span class="p">)</span>

        <span class="n">next_time_step</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_step</span><span class="p">.</span><span class="n">action</span><span class="p">)</span>

        <span class="n">traj</span> <span class="o">=</span> <span class="n">trajectory</span><span class="p">.</span><span class="n">from_transition</span><span class="p">(</span><span class="n">time_step</span><span class="p">,</span> <span class="n">action_step</span><span class="p">,</span> <span class="n">next_time_step</span><span class="p">)</span>

        <span class="n">replay_buffer</span><span class="p">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">traj</span><span class="p">)</span>
</pre></div>


</details>
<h5 id="log_1">log</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span>
</pre></div>


<p>Logs msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">log_msg</span>: <span class="nv">str</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="log_api_1">log_api</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log_api</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">api_target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Logs a call to api_target with additional log_msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log_api</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">api_target</span>: <span class="nv">str</span>, <span class="nv">log_msg</span>: <span class="nv">Optional</span>[<span class="nv">str</span>] <span class="o">=</span> <span class="nv">None</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs a call to api_target with additional log_msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">api_target</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">api_target</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_api_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">api_target</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_begin_1">on_play_episode_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_begin</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Env</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the beginning of a new episode</p>
<p>Args:
    env: the gym environment used to play the episode.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">env</span>: <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the beginning of a new episode</span>

        <span class="nv">Args</span>:

            <span class="nv">env</span>: <span class="nv">the</span> <span class="nv">gym</span> <span class="nv">environment</span> <span class="nv">used</span> <span class="nv">to</span> <span class="nv">play</span> <span class="nv">the</span> <span class="nv">episode</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">env</span>, <span class="s2">&quot;</span><span class="s">env not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">env</span>, <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>, <span class="s2">&quot;</span><span class="s">env not an an instance of gym.Env.</span><span class="s2">&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">env</span>

        <span class="nv">pc</span>.<span class="nv">steps_done_in_episode</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">pc</span>.<span class="nv">actions</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">sum_of_rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_end_1">on_play_episode_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the end of an episode</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the end of an episode</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span> <span class="nv">and</span> <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">&gt;=</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span>:

            <span class="nv">pc</span>.<span class="nv">play_done</span> <span class="o">=</span> <span class="nv">True</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_begin_1">on_train_iteration_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_begin</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the begining of a new iteration</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the begining of a new iteration</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">tc</span>.<span class="nv">steps_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">==</span> <span class="mi">0</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>.<span class="nv">episodes_done</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_end_1">on_train_iteration_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the end of an iteration</p>
<p>Evaluates the current policy. Use kwargs to set additional dict values in train context.
Eg for an ActorCriticTrainContext the losses may be set like this:
    on_train_iteration(loss=123,actor_loss=456,critic_loss=789)</p>
<p>Args:
    loss: loss after the training of the model in this iteration or math.nan if the loss is not available
    **kwargs: if a keyword matches a dict property of the TrainContext instance, then
                the dict[episodes_done_in_training] is set to the arg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">loss</span>: <span class="nv">float</span>, <span class="o">**</span><span class="nv">kwargs</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the end of an iteration</span>

        <span class="nv">Evaluates</span> <span class="nv">the</span> <span class="nv">current</span> <span class="nv">policy</span>. <span class="nv">Use</span> <span class="nv">kwargs</span> <span class="nv">to</span> <span class="nv">set</span> <span class="nv">additional</span> <span class="nv">dict</span> <span class="nv">values</span> <span class="nv">in</span> <span class="nv">train</span> <span class="nv">context</span>.

        <span class="nv">Eg</span> <span class="k">for</span> <span class="nv">an</span> <span class="nv">ActorCriticTrainContext</span> <span class="nv">the</span> <span class="nv">losses</span> <span class="nv">may</span> <span class="nv">be</span> <span class="nv">set</span> <span class="nv">like</span> <span class="nv">this</span>:

            <span class="nv">on_train_iteration</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="mi">123</span>,<span class="nv">actor_loss</span><span class="o">=</span><span class="mi">456</span>,<span class="nv">critic_loss</span><span class="o">=</span><span class="mi">789</span><span class="ss">)</span>

        <span class="nv">Args</span>:

            <span class="nv">loss</span>: <span class="nv">loss</span> <span class="nv">after</span> <span class="nv">the</span> <span class="nv">training</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">model</span> <span class="nv">in</span> <span class="nv">this</span> <span class="nv">iteration</span> <span class="nv">or</span> <span class="nv">math</span>.<span class="nv">nan</span> <span class="k">if</span> <span class="nv">the</span> <span class="nv">loss</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">available</span>

            <span class="o">**</span><span class="nv">kwargs</span>: <span class="k">if</span> <span class="nv">a</span> <span class="nv">keyword</span> <span class="nv">matches</span> <span class="nv">a</span> <span class="nv">dict</span> <span class="nv">property</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">TrainContext</span> <span class="nv">instance</span>, <span class="k">then</span>

                        <span class="nv">the</span> <span class="nv">dict</span>[<span class="nv">episodes_done_in_training</span>] <span class="nv">is</span> <span class="nv">set</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">arg</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">totals</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="ss">(</span><span class="nv">totals</span>.<span class="nv">episodes_done</span> <span class="o">-</span> <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span><span class="ss">)</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span> <span class="o">+=</span> <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span>

        <span class="nv">tc</span>.<span class="nv">loss</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">loss</span>

        # <span class="nv">set</span> <span class="nv">traincontext</span> <span class="nv">dict</span> <span class="nv">from</span> <span class="nv">kwargs</span>:

        <span class="k">for</span> <span class="nv">prop_name</span> <span class="nv">in</span> <span class="nv">kwargs</span>:

            <span class="nv">prop_instance</span> <span class="o">=</span> <span class="nv">getattr</span><span class="ss">(</span><span class="nv">tc</span>, <span class="nv">prop_name</span>, <span class="nv">None</span><span class="ss">)</span>

            <span class="nv">prop_value</span> <span class="o">=</span> <span class="nv">kwargs</span>[<span class="nv">prop_name</span>]

            <span class="k">if</span> <span class="nv">prop_instance</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span> <span class="nv">and</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">prop_instance</span>, <span class="nv">dict</span><span class="ss">)</span>:

                <span class="nv">prop_instance</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">prop_value</span>

        <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span>:

            <span class="nv">tc</span>.<span class="nv">training_done</span> <span class="o">=</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">&gt;=</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="nv">and</span> <span class="ss">(</span><span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">%</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="o">==</span> <span class="mi">0</span><span class="ss">)</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="play_1">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to play_implementation overriden by the subclass.</p>
<p>Args:
    play_context: play configuration to be used
    callbacks: list of callbacks called during play.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to play_implementation overriden by the subclass.</span>

<span class="ss">            Args:</span>

<span class="ss">                play_context: play configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during play.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">play_context</span><span class="p">,</span> <span class="ss">&quot;play_context not set&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="k">is</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;play_context already set in agent_context&quot;</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="n">play_context</span>

        <span class="n">old_callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">play_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">old_callbacks</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="play_implementation_1">play_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span>
<span class="p">)</span>
</pre></div>


<p>Agent specific implementation of playing a single episodes with the current policy.</p>
<p>Args:
    play_context: play configuration to be used</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">play_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">play_context</span>: <span class="nv">core</span>.<span class="nv">PlayContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Agent specific implementation of playing a single episodes with the current policy.</span>

            <span class="nv">Args</span>:

                <span class="nv">play_context</span>: <span class="nv">play</span> <span class="nv">configuration</span> <span class="nv">to</span> <span class="nv">be</span> <span class="nv">used</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">play_context</span>, <span class="s2">&quot;</span><span class="s">play_context not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>, <span class="s2">&quot;</span><span class="s">trained_policy not set. call train() first.</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_get_gym_env</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_play_env</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">gym_env</span><span class="ss">)</span>

            <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">reset</span><span class="ss">()</span>

            <span class="k">while</span> <span class="nv">not</span> <span class="nv">time_step</span>.<span class="nv">is_last</span><span class="ss">()</span>:

                <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

                <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_play_episode_end</span><span class="ss">()</span>

            <span class="k">if</span> <span class="nv">play_context</span>.<span class="nv">play_done</span>:

                <span class="k">break</span>
</pre></div>


</details>
<h5 id="train_1">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to train_implementation overriden by the subclass</p>
<p>Args:
    train_context: training configuration to be used
    callbacks: list of callbacks called during the training and evaluation.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to train_implementation overriden by the subclass</span>

<span class="ss">            Args:</span>

<span class="ss">                train_context: training configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during the training and evaluation.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">train_context</span><span class="p">,</span> <span class="ss">&quot;train_context not set&quot;</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="k">self</span><span class="p">.</span><span class="n">log_api</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;backend_name&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;{self._backend_name}&#39;</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_set_seed</span><span class="p">()</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">train_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="train_implementation_1">train_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span>
<span class="p">)</span>
</pre></div>


<p>Tf-Agents Ppo Implementation of the train loop.</p>
<p>The implementation follows
https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/1_dqn_tutorial.ipynb</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Tf-Agents Ppo Implementation of the train loop.</span>

        <span class="nv">The</span> <span class="nv">implementation</span> <span class="nv">follows</span>

        <span class="nv">https</span>:<span class="o">//</span><span class="nv">colab</span>.<span class="nv">research</span>.<span class="nv">google</span>.<span class="nv">com</span><span class="o">/</span><span class="nv">github</span><span class="o">/</span><span class="nv">tensorflow</span><span class="o">/</span><span class="nv">agents</span><span class="o">/</span><span class="nv">blob</span><span class="o">/</span><span class="nv">master</span><span class="o">/</span><span class="nv">tf_agents</span><span class="o">/</span><span class="nv">colabs</span><span class="o">/</span><span class="mi">1</span><span class="nv">_dqn_tutorial</span>.<span class="nv">ipynb</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">train_context</span>, <span class="nv">core</span>.<span class="nv">StepsTrainContext</span><span class="ss">)</span>

        <span class="nv">dc</span>: <span class="nv">core</span>.<span class="nv">StepsTrainContext</span> <span class="o">=</span> <span class="nv">train_context</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">(</span><span class="nv">discount</span><span class="o">=</span><span class="nv">dc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        <span class="nv">observation_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">observation_spec</span><span class="ss">()</span>

        <span class="nv">action_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">action_spec</span><span class="ss">()</span>

        <span class="nv">timestep_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">time_step_spec</span><span class="ss">()</span>

        # <span class="nv">SetUp</span> <span class="nv">Optimizer</span>, <span class="nv">Networks</span> <span class="nv">and</span> <span class="nv">DqnAgent</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">AdamOptimizer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">optimizer</span> <span class="o">=</span> <span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">dc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">QNetwork</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">q_net</span> <span class="o">=</span> <span class="nv">q_network</span>.<span class="nv">QNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span>, <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DqnAgent</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span> <span class="o">=</span> <span class="nv">dqn_agent</span>.<span class="nv">DqnAgent</span><span class="ss">(</span><span class="nv">timestep_spec</span>, <span class="nv">action_spec</span>,

                                      <span class="nv">q_network</span><span class="o">=</span><span class="nv">q_net</span>, <span class="nv">optimizer</span><span class="o">=</span><span class="nv">optimizer</span>,

                                      <span class="nv">td_errors_loss_fn</span><span class="o">=</span><span class="nv">common</span>.<span class="nv">element_wise_squared_loss</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.initialize</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">initialize</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">policy</span>

        # <span class="nv">SetUp</span> <span class="nv">Data</span> <span class="nv">collection</span> <span class="o">&amp;</span> <span class="nv">Buffering</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">TFUniformReplayBuffer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">replay_buffer</span> <span class="o">=</span> <span class="nv">TFUniformReplayBuffer</span><span class="ss">(</span><span class="nv">data_spec</span><span class="o">=</span><span class="nv">tf_agent</span>.<span class="nv">collect_data_spec</span>,

                                              <span class="nv">batch_size</span><span class="o">=</span><span class="nv">train_env</span>.<span class="nv">batch_size</span>,

                                              <span class="nv">max_length</span><span class="o">=</span><span class="nv">dc</span>.<span class="nv">max_steps_in_buffer</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">RandomTFPolicy</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">random_policy</span> <span class="o">=</span> <span class="nv">random_tf_policy</span>.<span class="nv">RandomTFPolicy</span><span class="ss">(</span><span class="nv">timestep_spec</span>, <span class="nv">action_spec</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.add_batch</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(trajectory)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="k">for</span> <span class="nv">_</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="nv">dc</span>.<span class="nv">num_steps_buffer_preload</span><span class="ss">)</span>:

            <span class="nv">self</span>.<span class="nv">collect_step</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">train_env</span>, <span class="nv">policy</span><span class="o">=</span><span class="nv">random_policy</span>, <span class="nv">replay_buffer</span><span class="o">=</span><span class="nv">replay_buffer</span><span class="ss">)</span>

        # <span class="nv">Train</span>

        <span class="nv">tf_agent</span>.<span class="nv">train</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">train</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.as_dataset</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(num_parallel_calls=3, </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">sample_batch_size={dc.num_steps_sampled_from_buffer}, num_steps=2).prefetch(3)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">dataset</span> <span class="o">=</span> <span class="nv">replay_buffer</span>.<span class="nv">as_dataset</span><span class="ss">(</span><span class="nv">num_parallel_calls</span><span class="o">=</span><span class="mi">3</span>, <span class="nv">sample_batch_size</span><span class="o">=</span><span class="nv">dc</span>.<span class="nv">num_steps_sampled_from_buffer</span>,

                                           <span class="nv">num_steps</span><span class="o">=</span><span class="mi">2</span><span class="ss">)</span>.<span class="nv">prefetch</span><span class="ss">(</span><span class="mi">3</span><span class="ss">)</span>

        <span class="nv">iter_dataset</span> <span class="o">=</span> <span class="nv">iter</span><span class="ss">(</span><span class="nv">dataset</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">for each iteration</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">  replay_buffer.add_batch</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(trajectory)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">  tf_agent.train</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(experience=trajectory)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            <span class="k">for</span> <span class="nv">_</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="nv">dc</span>.<span class="nv">num_steps_per_iteration</span><span class="ss">)</span>:

                <span class="nv">self</span>.<span class="nv">collect_step</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">train_env</span>, <span class="nv">policy</span><span class="o">=</span><span class="nv">tf_agent</span>.<span class="nv">collect_policy</span>, <span class="nv">replay_buffer</span><span class="o">=</span><span class="nv">replay_buffer</span><span class="ss">)</span>

            <span class="nv">trajectories</span>, <span class="nv">_</span> <span class="o">=</span> <span class="k">next</span><span class="ss">(</span><span class="nv">iter_dataset</span><span class="ss">)</span>

            <span class="nv">tf_loss_info</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">(</span><span class="nv">experience</span><span class="o">=</span><span class="nv">trajectories</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">tf_loss_info</span>.<span class="nv">loss</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">train_context</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
<h3 id="tfppoagent">TfPpoAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">TfPpoAgent</span><span class="p">(</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span>
<span class="p">)</span>
</pre></div>


<p>creates a new agent based on the PPO algorithm using the tfagents implementation.
PPO is an actor-critic algorithm using 2 neural networks. The actor network
to predict the next action to be taken and the critic network to estimate
the value of the game state we are currently in (the expected, discounted
sum of future rewards when following the current actor network).</p>
<p>Args:
    model_config: the model configuration including the name of the target gym environment
        as well as the neural network architecture.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="nv">class</span> <span class="nv">TfPpoAgent</span><span class="ss">(</span><span class="nv">TfAgent</span><span class="ss">)</span>:

    <span class="s2">&quot;&quot;&quot;</span><span class="s"> creates a new agent based on the PPO algorithm using the tfagents implementation.</span>

        <span class="nv">PPO</span> <span class="nv">is</span> <span class="nv">an</span> <span class="nv">actor</span><span class="o">-</span><span class="nv">critic</span> <span class="nv">algorithm</span> <span class="nv">using</span> <span class="mi">2</span> <span class="nv">neural</span> <span class="nv">networks</span>. <span class="nv">The</span> <span class="nv">actor</span> <span class="nv">network</span>

        <span class="nv">to</span> <span class="nv">predict</span> <span class="nv">the</span> <span class="k">next</span> <span class="nv">action</span> <span class="nv">to</span> <span class="nv">be</span> <span class="nv">taken</span> <span class="nv">and</span> <span class="nv">the</span> <span class="nv">critic</span> <span class="nv">network</span> <span class="nv">to</span> <span class="nv">estimate</span>

        <span class="nv">the</span> <span class="nv">value</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">game</span> <span class="nv">state</span> <span class="nv">we</span> <span class="nv">are</span> <span class="nv">currently</span> <span class="nv">in</span> <span class="ss">(</span><span class="nv">the</span> <span class="nv">expected</span>, <span class="nv">discounted</span>

        <span class="nv">sum</span> <span class="nv">of</span> <span class="nv">future</span> <span class="nv">rewards</span> <span class="nv">when</span> <span class="nv">following</span> <span class="nv">the</span> <span class="nv">current</span> <span class="nv">actor</span> <span class="nv">network</span><span class="ss">)</span>.

        <span class="nv">Args</span>:

            <span class="nv">model_config</span>: <span class="nv">the</span> <span class="nv">model</span> <span class="nv">configuration</span> <span class="nv">including</span> <span class="nv">the</span> <span class="nv">name</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">target</span> <span class="nv">gym</span> <span class="nv">environment</span>

                <span class="nv">as</span> <span class="nv">well</span> <span class="nv">as</span> <span class="nv">the</span> <span class="nv">neural</span> <span class="nv">network</span> <span class="nv">architecture</span>.

    <span class="s2">&quot;&quot;&quot;</span>

    <span class="nv">def</span> <span class="nv">__init__</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">model_config</span>: <span class="nv">core</span>.<span class="nv">ModelConfig</span><span class="ss">)</span>:

        <span class="nv">super</span><span class="ss">()</span>.<span class="nv">__init__</span><span class="ss">(</span><span class="nv">model_config</span><span class="o">=</span><span class="nv">model_config</span><span class="ss">)</span>

    # <span class="nv">noinspection</span> <span class="nv">DuplicatedCode</span>

    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Tf-Agents Ppo Implementation of the train loop.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">train_context</span>, <span class="nv">core</span>.<span class="nv">PpoTrainContext</span><span class="ss">)</span>

        <span class="nv">tc</span>: <span class="nv">core</span>.<span class="nv">PpoTrainContext</span> <span class="o">=</span> <span class="nv">train_context</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">(</span><span class="nv">discount</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        <span class="nv">observation_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">observation_spec</span><span class="ss">()</span>

        <span class="nv">action_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">action_spec</span><span class="ss">()</span>

        <span class="nv">timestep_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">time_step_spec</span><span class="ss">()</span>

        # <span class="nv">SetUp</span> <span class="nv">Optimizer</span>, <span class="nv">Networks</span> <span class="nv">and</span> <span class="nv">PpoAgent</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">AdamOptimizer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">optimizer</span> <span class="o">=</span> <span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ActorDistributionNetwork</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">actor_net</span> <span class="o">=</span> <span class="nv">actor_distribution_network</span>.<span class="nv">ActorDistributionNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span>,

                                                                        <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ValueNetwork</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">value_net</span> <span class="o">=</span> <span class="nv">value_network</span>.<span class="nv">ValueNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">PpoAgent</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span> <span class="o">=</span> <span class="nv">ppo_agent</span>.<span class="nv">PPOAgent</span><span class="ss">(</span><span class="nv">timestep_spec</span>, <span class="nv">action_spec</span>, <span class="nv">optimizer</span>,

                                      <span class="nv">actor_net</span><span class="o">=</span><span class="nv">actor_net</span>, <span class="nv">value_net</span><span class="o">=</span><span class="nv">value_net</span>,

                                      <span class="nv">num_epochs</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_epochs_per_iteration</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.initialize</span><span class="s1">&#39;</span>,<span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">initialize</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">policy</span>

        # <span class="nv">SetUp</span> <span class="nv">Data</span> <span class="nv">collection</span> <span class="o">&amp;</span> <span class="nv">Buffering</span>

        <span class="nv">collect_data_spec</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">collect_data_spec</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">TFUniformReplayBuffer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">replay_buffer</span> <span class="o">=</span> <span class="nv">TFUniformReplayBuffer</span><span class="ss">(</span><span class="nv">collect_data_spec</span>,

                                              <span class="nv">batch_size</span><span class="o">=</span><span class="mi">1</span>, <span class="nv">max_length</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">max_steps_in_buffer</span><span class="ss">)</span>

        <span class="nv">collect_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">collect_policy</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DynamicEpisodeDriver</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">collect_driver</span> <span class="o">=</span> <span class="nv">DynamicEpisodeDriver</span><span class="ss">(</span><span class="nv">train_env</span>, <span class="nv">collect_policy</span>, <span class="nv">observers</span><span class="o">=</span>[<span class="nv">replay_buffer</span>.<span class="nv">add_batch</span>],

                                              <span class="nv">num_episodes</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_episodes_per_iteration</span><span class="ss">)</span>

        # <span class="nv">Train</span>

        <span class="nv">collect_driver</span>.<span class="nv">run</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">collect_driver</span>.<span class="nv">run</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">train</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">train</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">-----</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:&lt;4}      -----</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">collect_driver.run</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">collect_driver</span>.<span class="nv">run</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.gather_all</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">trajectories</span> <span class="o">=</span> <span class="nv">replay_buffer</span>.<span class="nv">gather_all</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.train</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(experience=...)</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">loss_info</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">(</span><span class="nv">experience</span><span class="o">=</span><span class="nv">trajectories</span><span class="ss">)</span>

            <span class="nv">total_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">actor_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">policy_gradient_loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">critic_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">value_estimation_loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">loss={total_loss:&lt;7.1f} [actor={actor_loss:&lt;7.1f} critic={critic_loss:&lt;7.1f}]</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.clear</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">replay_buffer</span>.<span class="nv">clear</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="nv">total_loss</span>, <span class="nv">actor_loss</span><span class="o">=</span><span class="nv">actor_loss</span>, <span class="nv">critic_loss</span><span class="o">=</span><span class="nv">critic_loss</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_3">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.backends.tfagents.TfAgent</li>
<li>easyagents.backends.core.BackendAgent</li>
<li>easyagents.backends.core._BackendAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_3">Methods</h4>
<h5 id="log_2">log</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span>
</pre></div>


<p>Logs msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">log_msg</span>: <span class="nv">str</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="log_api_2">log_api</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log_api</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">api_target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Logs a call to api_target with additional log_msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log_api</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">api_target</span>: <span class="nv">str</span>, <span class="nv">log_msg</span>: <span class="nv">Optional</span>[<span class="nv">str</span>] <span class="o">=</span> <span class="nv">None</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs a call to api_target with additional log_msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">api_target</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">api_target</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_api_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">api_target</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_begin_2">on_play_episode_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_begin</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Env</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the beginning of a new episode</p>
<p>Args:
    env: the gym environment used to play the episode.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">env</span>: <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the beginning of a new episode</span>

        <span class="nv">Args</span>:

            <span class="nv">env</span>: <span class="nv">the</span> <span class="nv">gym</span> <span class="nv">environment</span> <span class="nv">used</span> <span class="nv">to</span> <span class="nv">play</span> <span class="nv">the</span> <span class="nv">episode</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">env</span>, <span class="s2">&quot;</span><span class="s">env not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">env</span>, <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>, <span class="s2">&quot;</span><span class="s">env not an an instance of gym.Env.</span><span class="s2">&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">env</span>

        <span class="nv">pc</span>.<span class="nv">steps_done_in_episode</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">pc</span>.<span class="nv">actions</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">sum_of_rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_end_2">on_play_episode_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the end of an episode</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the end of an episode</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span> <span class="nv">and</span> <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">&gt;=</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span>:

            <span class="nv">pc</span>.<span class="nv">play_done</span> <span class="o">=</span> <span class="nv">True</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_begin_2">on_train_iteration_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_begin</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the begining of a new iteration</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the begining of a new iteration</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">tc</span>.<span class="nv">steps_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">==</span> <span class="mi">0</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>.<span class="nv">episodes_done</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_end_2">on_train_iteration_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the end of an iteration</p>
<p>Evaluates the current policy. Use kwargs to set additional dict values in train context.
Eg for an ActorCriticTrainContext the losses may be set like this:
    on_train_iteration(loss=123,actor_loss=456,critic_loss=789)</p>
<p>Args:
    loss: loss after the training of the model in this iteration or math.nan if the loss is not available
    **kwargs: if a keyword matches a dict property of the TrainContext instance, then
                the dict[episodes_done_in_training] is set to the arg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">loss</span>: <span class="nv">float</span>, <span class="o">**</span><span class="nv">kwargs</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the end of an iteration</span>

        <span class="nv">Evaluates</span> <span class="nv">the</span> <span class="nv">current</span> <span class="nv">policy</span>. <span class="nv">Use</span> <span class="nv">kwargs</span> <span class="nv">to</span> <span class="nv">set</span> <span class="nv">additional</span> <span class="nv">dict</span> <span class="nv">values</span> <span class="nv">in</span> <span class="nv">train</span> <span class="nv">context</span>.

        <span class="nv">Eg</span> <span class="k">for</span> <span class="nv">an</span> <span class="nv">ActorCriticTrainContext</span> <span class="nv">the</span> <span class="nv">losses</span> <span class="nv">may</span> <span class="nv">be</span> <span class="nv">set</span> <span class="nv">like</span> <span class="nv">this</span>:

            <span class="nv">on_train_iteration</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="mi">123</span>,<span class="nv">actor_loss</span><span class="o">=</span><span class="mi">456</span>,<span class="nv">critic_loss</span><span class="o">=</span><span class="mi">789</span><span class="ss">)</span>

        <span class="nv">Args</span>:

            <span class="nv">loss</span>: <span class="nv">loss</span> <span class="nv">after</span> <span class="nv">the</span> <span class="nv">training</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">model</span> <span class="nv">in</span> <span class="nv">this</span> <span class="nv">iteration</span> <span class="nv">or</span> <span class="nv">math</span>.<span class="nv">nan</span> <span class="k">if</span> <span class="nv">the</span> <span class="nv">loss</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">available</span>

            <span class="o">**</span><span class="nv">kwargs</span>: <span class="k">if</span> <span class="nv">a</span> <span class="nv">keyword</span> <span class="nv">matches</span> <span class="nv">a</span> <span class="nv">dict</span> <span class="nv">property</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">TrainContext</span> <span class="nv">instance</span>, <span class="k">then</span>

                        <span class="nv">the</span> <span class="nv">dict</span>[<span class="nv">episodes_done_in_training</span>] <span class="nv">is</span> <span class="nv">set</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">arg</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">totals</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="ss">(</span><span class="nv">totals</span>.<span class="nv">episodes_done</span> <span class="o">-</span> <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span><span class="ss">)</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span> <span class="o">+=</span> <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span>

        <span class="nv">tc</span>.<span class="nv">loss</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">loss</span>

        # <span class="nv">set</span> <span class="nv">traincontext</span> <span class="nv">dict</span> <span class="nv">from</span> <span class="nv">kwargs</span>:

        <span class="k">for</span> <span class="nv">prop_name</span> <span class="nv">in</span> <span class="nv">kwargs</span>:

            <span class="nv">prop_instance</span> <span class="o">=</span> <span class="nv">getattr</span><span class="ss">(</span><span class="nv">tc</span>, <span class="nv">prop_name</span>, <span class="nv">None</span><span class="ss">)</span>

            <span class="nv">prop_value</span> <span class="o">=</span> <span class="nv">kwargs</span>[<span class="nv">prop_name</span>]

            <span class="k">if</span> <span class="nv">prop_instance</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span> <span class="nv">and</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">prop_instance</span>, <span class="nv">dict</span><span class="ss">)</span>:

                <span class="nv">prop_instance</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">prop_value</span>

        <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span>:

            <span class="nv">tc</span>.<span class="nv">training_done</span> <span class="o">=</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">&gt;=</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="nv">and</span> <span class="ss">(</span><span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">%</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="o">==</span> <span class="mi">0</span><span class="ss">)</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="play_2">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to play_implementation overriden by the subclass.</p>
<p>Args:
    play_context: play configuration to be used
    callbacks: list of callbacks called during play.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to play_implementation overriden by the subclass.</span>

<span class="ss">            Args:</span>

<span class="ss">                play_context: play configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during play.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">play_context</span><span class="p">,</span> <span class="ss">&quot;play_context not set&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="k">is</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;play_context already set in agent_context&quot;</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="n">play_context</span>

        <span class="n">old_callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">play_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">old_callbacks</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="play_implementation_2">play_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span>
<span class="p">)</span>
</pre></div>


<p>Agent specific implementation of playing a single episodes with the current policy.</p>
<p>Args:
    play_context: play configuration to be used</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">play_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">play_context</span>: <span class="nv">core</span>.<span class="nv">PlayContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Agent specific implementation of playing a single episodes with the current policy.</span>

            <span class="nv">Args</span>:

                <span class="nv">play_context</span>: <span class="nv">play</span> <span class="nv">configuration</span> <span class="nv">to</span> <span class="nv">be</span> <span class="nv">used</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">play_context</span>, <span class="s2">&quot;</span><span class="s">play_context not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>, <span class="s2">&quot;</span><span class="s">trained_policy not set. call train() first.</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_get_gym_env</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_play_env</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">gym_env</span><span class="ss">)</span>

            <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">reset</span><span class="ss">()</span>

            <span class="k">while</span> <span class="nv">not</span> <span class="nv">time_step</span>.<span class="nv">is_last</span><span class="ss">()</span>:

                <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

                <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_play_episode_end</span><span class="ss">()</span>

            <span class="k">if</span> <span class="nv">play_context</span>.<span class="nv">play_done</span>:

                <span class="k">break</span>
</pre></div>


</details>
<h5 id="train_2">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to train_implementation overriden by the subclass</p>
<p>Args:
    train_context: training configuration to be used
    callbacks: list of callbacks called during the training and evaluation.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to train_implementation overriden by the subclass</span>

<span class="ss">            Args:</span>

<span class="ss">                train_context: training configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during the training and evaluation.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">train_context</span><span class="p">,</span> <span class="ss">&quot;train_context not set&quot;</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="k">self</span><span class="p">.</span><span class="n">log_api</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;backend_name&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;{self._backend_name}&#39;</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_set_seed</span><span class="p">()</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">train_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="train_implementation_2">train_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span>
<span class="p">)</span>
</pre></div>


<p>Tf-Agents Ppo Implementation of the train loop.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Tf-Agents Ppo Implementation of the train loop.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">train_context</span>, <span class="nv">core</span>.<span class="nv">PpoTrainContext</span><span class="ss">)</span>

        <span class="nv">tc</span>: <span class="nv">core</span>.<span class="nv">PpoTrainContext</span> <span class="o">=</span> <span class="nv">train_context</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">(</span><span class="nv">discount</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        <span class="nv">observation_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">observation_spec</span><span class="ss">()</span>

        <span class="nv">action_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">action_spec</span><span class="ss">()</span>

        <span class="nv">timestep_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">time_step_spec</span><span class="ss">()</span>

        # <span class="nv">SetUp</span> <span class="nv">Optimizer</span>, <span class="nv">Networks</span> <span class="nv">and</span> <span class="nv">PpoAgent</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">AdamOptimizer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">optimizer</span> <span class="o">=</span> <span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ActorDistributionNetwork</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">actor_net</span> <span class="o">=</span> <span class="nv">actor_distribution_network</span>.<span class="nv">ActorDistributionNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span>,

                                                                        <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ValueNetwork</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">value_net</span> <span class="o">=</span> <span class="nv">value_network</span>.<span class="nv">ValueNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">PpoAgent</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span> <span class="o">=</span> <span class="nv">ppo_agent</span>.<span class="nv">PPOAgent</span><span class="ss">(</span><span class="nv">timestep_spec</span>, <span class="nv">action_spec</span>, <span class="nv">optimizer</span>,

                                      <span class="nv">actor_net</span><span class="o">=</span><span class="nv">actor_net</span>, <span class="nv">value_net</span><span class="o">=</span><span class="nv">value_net</span>,

                                      <span class="nv">num_epochs</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_epochs_per_iteration</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.initialize</span><span class="s1">&#39;</span>,<span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">initialize</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">policy</span>

        # <span class="nv">SetUp</span> <span class="nv">Data</span> <span class="nv">collection</span> <span class="o">&amp;</span> <span class="nv">Buffering</span>

        <span class="nv">collect_data_spec</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">collect_data_spec</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">TFUniformReplayBuffer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">replay_buffer</span> <span class="o">=</span> <span class="nv">TFUniformReplayBuffer</span><span class="ss">(</span><span class="nv">collect_data_spec</span>,

                                              <span class="nv">batch_size</span><span class="o">=</span><span class="mi">1</span>, <span class="nv">max_length</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">max_steps_in_buffer</span><span class="ss">)</span>

        <span class="nv">collect_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">collect_policy</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DynamicEpisodeDriver</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">collect_driver</span> <span class="o">=</span> <span class="nv">DynamicEpisodeDriver</span><span class="ss">(</span><span class="nv">train_env</span>, <span class="nv">collect_policy</span>, <span class="nv">observers</span><span class="o">=</span>[<span class="nv">replay_buffer</span>.<span class="nv">add_batch</span>],

                                              <span class="nv">num_episodes</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_episodes_per_iteration</span><span class="ss">)</span>

        # <span class="nv">Train</span>

        <span class="nv">collect_driver</span>.<span class="nv">run</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">collect_driver</span>.<span class="nv">run</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">train</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">train</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">-----</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:&lt;4}      -----</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">collect_driver.run</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">collect_driver</span>.<span class="nv">run</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.gather_all</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">trajectories</span> <span class="o">=</span> <span class="nv">replay_buffer</span>.<span class="nv">gather_all</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.train</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(experience=...)</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">loss_info</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">(</span><span class="nv">experience</span><span class="o">=</span><span class="nv">trajectories</span><span class="ss">)</span>

            <span class="nv">total_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">actor_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">policy_gradient_loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">critic_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">value_estimation_loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">loss={total_loss:&lt;7.1f} [actor={actor_loss:&lt;7.1f} critic={critic_loss:&lt;7.1f}]</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.clear</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">replay_buffer</span>.<span class="nv">clear</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="nv">total_loss</span>, <span class="nv">actor_loss</span><span class="o">=</span><span class="nv">actor_loss</span>, <span class="nv">critic_loss</span><span class="o">=</span><span class="nv">critic_loss</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
<h3 id="tfrandomagent">TfRandomAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">TfRandomAgent</span><span class="p">(</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span>
<span class="p">)</span>
</pre></div>


<p>creates a new random agent based on uniform random actions.</p>
<p>Args:
    model_config: the model configuration including the name of the target gym environment
        as well as the neural network architecture.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="nv">class</span> <span class="nv">TfRandomAgent</span><span class="ss">(</span><span class="nv">TfAgent</span><span class="ss">)</span>:

    <span class="s2">&quot;&quot;&quot;</span><span class="s"> creates a new random agent based on uniform random actions.</span>

        <span class="nv">Args</span>:

            <span class="nv">model_config</span>: <span class="nv">the</span> <span class="nv">model</span> <span class="nv">configuration</span> <span class="nv">including</span> <span class="nv">the</span> <span class="nv">name</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">target</span> <span class="nv">gym</span> <span class="nv">environment</span>

                <span class="nv">as</span> <span class="nv">well</span> <span class="nv">as</span> <span class="nv">the</span> <span class="nv">neural</span> <span class="nv">network</span> <span class="nv">architecture</span>.

    <span class="s2">&quot;&quot;&quot;</span>

    <span class="nv">def</span> <span class="nv">__init__</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">model_config</span>: <span class="nv">core</span>.<span class="nv">ModelConfig</span><span class="ss">)</span>:

        <span class="nv">super</span><span class="ss">()</span>.<span class="nv">__init__</span><span class="ss">(</span><span class="nv">model_config</span><span class="o">=</span><span class="nv">model_config</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">_set_trained_policy</span><span class="ss">()</span>

    <span class="nv">def</span> <span class="nv">_set_trained_policy</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Tf-Agents Random Implementation of the train loop.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">log</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">Creating environment...</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="nv">action_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">action_spec</span><span class="ss">()</span>

        <span class="nv">timestep_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">time_step_spec</span><span class="ss">()</span>

        # <span class="nv">SetUp</span> <span class="nv">Data</span> <span class="nv">collection</span> <span class="o">&amp;</span> <span class="nv">Buffering</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">RandomTFPolicy</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">random_tf_policy</span>.<span class="nv">RandomTFPolicy</span><span class="ss">(</span><span class="nv">timestep_spec</span>, <span class="nv">action_spec</span><span class="ss">)</span>

    # <span class="nv">noinspection</span> <span class="nv">DuplicatedCode</span>

    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="nv">self</span>.<span class="nv">log</span><span class="ss">(</span><span class="s2">&quot;</span><span class="s">Training...</span><span class="s2">&quot;</span><span class="ss">)</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            # <span class="nv">ensure</span> <span class="nv">that</span> <span class="mi">1</span> <span class="nv">episode</span> <span class="nv">is</span> <span class="nv">played</span> <span class="nv">during</span> <span class="nv">the</span> <span class="nv">iteration</span>

            <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">reset</span><span class="ss">()</span>

            <span class="k">while</span> <span class="nv">not</span> <span class="nv">time_step</span>.<span class="nv">is_last</span><span class="ss">()</span>:

                <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

                <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">math</span>.<span class="nv">nan</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">train_context</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_4">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.backends.tfagents.TfAgent</li>
<li>easyagents.backends.core.BackendAgent</li>
<li>easyagents.backends.core._BackendAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_4">Methods</h4>
<h5 id="log_3">log</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span>
</pre></div>


<p>Logs msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">log_msg</span>: <span class="nv">str</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="log_api_3">log_api</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log_api</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">api_target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Logs a call to api_target with additional log_msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log_api</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">api_target</span>: <span class="nv">str</span>, <span class="nv">log_msg</span>: <span class="nv">Optional</span>[<span class="nv">str</span>] <span class="o">=</span> <span class="nv">None</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs a call to api_target with additional log_msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">api_target</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">api_target</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_api_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">api_target</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_begin_3">on_play_episode_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_begin</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Env</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the beginning of a new episode</p>
<p>Args:
    env: the gym environment used to play the episode.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">env</span>: <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the beginning of a new episode</span>

        <span class="nv">Args</span>:

            <span class="nv">env</span>: <span class="nv">the</span> <span class="nv">gym</span> <span class="nv">environment</span> <span class="nv">used</span> <span class="nv">to</span> <span class="nv">play</span> <span class="nv">the</span> <span class="nv">episode</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">env</span>, <span class="s2">&quot;</span><span class="s">env not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">env</span>, <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>, <span class="s2">&quot;</span><span class="s">env not an an instance of gym.Env.</span><span class="s2">&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">env</span>

        <span class="nv">pc</span>.<span class="nv">steps_done_in_episode</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">pc</span>.<span class="nv">actions</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">sum_of_rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_end_3">on_play_episode_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the end of an episode</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the end of an episode</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span> <span class="nv">and</span> <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">&gt;=</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span>:

            <span class="nv">pc</span>.<span class="nv">play_done</span> <span class="o">=</span> <span class="nv">True</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_begin_3">on_train_iteration_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_begin</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the begining of a new iteration</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the begining of a new iteration</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">tc</span>.<span class="nv">steps_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">==</span> <span class="mi">0</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>.<span class="nv">episodes_done</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_end_3">on_train_iteration_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the end of an iteration</p>
<p>Evaluates the current policy. Use kwargs to set additional dict values in train context.
Eg for an ActorCriticTrainContext the losses may be set like this:
    on_train_iteration(loss=123,actor_loss=456,critic_loss=789)</p>
<p>Args:
    loss: loss after the training of the model in this iteration or math.nan if the loss is not available
    **kwargs: if a keyword matches a dict property of the TrainContext instance, then
                the dict[episodes_done_in_training] is set to the arg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">loss</span>: <span class="nv">float</span>, <span class="o">**</span><span class="nv">kwargs</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the end of an iteration</span>

        <span class="nv">Evaluates</span> <span class="nv">the</span> <span class="nv">current</span> <span class="nv">policy</span>. <span class="nv">Use</span> <span class="nv">kwargs</span> <span class="nv">to</span> <span class="nv">set</span> <span class="nv">additional</span> <span class="nv">dict</span> <span class="nv">values</span> <span class="nv">in</span> <span class="nv">train</span> <span class="nv">context</span>.

        <span class="nv">Eg</span> <span class="k">for</span> <span class="nv">an</span> <span class="nv">ActorCriticTrainContext</span> <span class="nv">the</span> <span class="nv">losses</span> <span class="nv">may</span> <span class="nv">be</span> <span class="nv">set</span> <span class="nv">like</span> <span class="nv">this</span>:

            <span class="nv">on_train_iteration</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="mi">123</span>,<span class="nv">actor_loss</span><span class="o">=</span><span class="mi">456</span>,<span class="nv">critic_loss</span><span class="o">=</span><span class="mi">789</span><span class="ss">)</span>

        <span class="nv">Args</span>:

            <span class="nv">loss</span>: <span class="nv">loss</span> <span class="nv">after</span> <span class="nv">the</span> <span class="nv">training</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">model</span> <span class="nv">in</span> <span class="nv">this</span> <span class="nv">iteration</span> <span class="nv">or</span> <span class="nv">math</span>.<span class="nv">nan</span> <span class="k">if</span> <span class="nv">the</span> <span class="nv">loss</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">available</span>

            <span class="o">**</span><span class="nv">kwargs</span>: <span class="k">if</span> <span class="nv">a</span> <span class="nv">keyword</span> <span class="nv">matches</span> <span class="nv">a</span> <span class="nv">dict</span> <span class="nv">property</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">TrainContext</span> <span class="nv">instance</span>, <span class="k">then</span>

                        <span class="nv">the</span> <span class="nv">dict</span>[<span class="nv">episodes_done_in_training</span>] <span class="nv">is</span> <span class="nv">set</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">arg</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">totals</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="ss">(</span><span class="nv">totals</span>.<span class="nv">episodes_done</span> <span class="o">-</span> <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span><span class="ss">)</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span> <span class="o">+=</span> <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span>

        <span class="nv">tc</span>.<span class="nv">loss</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">loss</span>

        # <span class="nv">set</span> <span class="nv">traincontext</span> <span class="nv">dict</span> <span class="nv">from</span> <span class="nv">kwargs</span>:

        <span class="k">for</span> <span class="nv">prop_name</span> <span class="nv">in</span> <span class="nv">kwargs</span>:

            <span class="nv">prop_instance</span> <span class="o">=</span> <span class="nv">getattr</span><span class="ss">(</span><span class="nv">tc</span>, <span class="nv">prop_name</span>, <span class="nv">None</span><span class="ss">)</span>

            <span class="nv">prop_value</span> <span class="o">=</span> <span class="nv">kwargs</span>[<span class="nv">prop_name</span>]

            <span class="k">if</span> <span class="nv">prop_instance</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span> <span class="nv">and</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">prop_instance</span>, <span class="nv">dict</span><span class="ss">)</span>:

                <span class="nv">prop_instance</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">prop_value</span>

        <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span>:

            <span class="nv">tc</span>.<span class="nv">training_done</span> <span class="o">=</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">&gt;=</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="nv">and</span> <span class="ss">(</span><span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">%</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="o">==</span> <span class="mi">0</span><span class="ss">)</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="play_3">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to play_implementation overriden by the subclass.</p>
<p>Args:
    play_context: play configuration to be used
    callbacks: list of callbacks called during play.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to play_implementation overriden by the subclass.</span>

<span class="ss">            Args:</span>

<span class="ss">                play_context: play configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during play.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">play_context</span><span class="p">,</span> <span class="ss">&quot;play_context not set&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="k">is</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;play_context already set in agent_context&quot;</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="n">play_context</span>

        <span class="n">old_callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">play_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">old_callbacks</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="play_implementation_3">play_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span>
<span class="p">)</span>
</pre></div>


<p>Agent specific implementation of playing a single episodes with the current policy.</p>
<p>Args:
    play_context: play configuration to be used</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">play_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">play_context</span>: <span class="nv">core</span>.<span class="nv">PlayContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Agent specific implementation of playing a single episodes with the current policy.</span>

            <span class="nv">Args</span>:

                <span class="nv">play_context</span>: <span class="nv">play</span> <span class="nv">configuration</span> <span class="nv">to</span> <span class="nv">be</span> <span class="nv">used</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">play_context</span>, <span class="s2">&quot;</span><span class="s">play_context not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>, <span class="s2">&quot;</span><span class="s">trained_policy not set. call train() first.</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_get_gym_env</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_play_env</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">gym_env</span><span class="ss">)</span>

            <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">reset</span><span class="ss">()</span>

            <span class="k">while</span> <span class="nv">not</span> <span class="nv">time_step</span>.<span class="nv">is_last</span><span class="ss">()</span>:

                <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

                <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_play_episode_end</span><span class="ss">()</span>

            <span class="k">if</span> <span class="nv">play_context</span>.<span class="nv">play_done</span>:

                <span class="k">break</span>
</pre></div>


</details>
<h5 id="train_3">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to train_implementation overriden by the subclass</p>
<p>Args:
    train_context: training configuration to be used
    callbacks: list of callbacks called during the training and evaluation.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to train_implementation overriden by the subclass</span>

<span class="ss">            Args:</span>

<span class="ss">                train_context: training configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during the training and evaluation.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">train_context</span><span class="p">,</span> <span class="ss">&quot;train_context not set&quot;</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="k">self</span><span class="p">.</span><span class="n">log_api</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;backend_name&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;{self._backend_name}&#39;</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_set_seed</span><span class="p">()</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">train_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="train_implementation_3">train_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span>
<span class="p">)</span>
</pre></div>


<p>Agent specific implementation of the train loop.</p>
<p>The implementation should have the form:</p>
<p>while True:
    on_iteration_begin
    for e in num_episodes_per_iterations
        play episode and record steps (while steps_in_episode &lt; max_steps_per_episode and)
    train policy for num_epochs_per_iteration epochs
    on_iteration_end( loss )
    if training_done
        break</p>
<p>Args:
    train_context: context configuring the train loop</p>
<p>Hints:
o the subclasses training loss is passed through to BackendAgent by on_iteration_end.
  Thus the subclass must not add the experienced loss to the TrainContext.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="nv">self</span>.<span class="nv">log</span><span class="ss">(</span><span class="s2">&quot;</span><span class="s">Training...</span><span class="s2">&quot;</span><span class="ss">)</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            # <span class="nv">ensure</span> <span class="nv">that</span> <span class="mi">1</span> <span class="nv">episode</span> <span class="nv">is</span> <span class="nv">played</span> <span class="nv">during</span> <span class="nv">the</span> <span class="nv">iteration</span>

            <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">reset</span><span class="ss">()</span>

            <span class="k">while</span> <span class="nv">not</span> <span class="nv">time_step</span>.<span class="nv">is_last</span><span class="ss">()</span>:

                <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

                <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">math</span>.<span class="nv">nan</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">train_context</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
<h3 id="tfreinforceagent">TfReinforceAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">TfReinforceAgent</span><span class="p">(</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span>
<span class="p">)</span>
</pre></div>


<p>creates a new agent based on the Reinforce algorithm using the tfagents implementation.
Reinforce is a vanilla policy gradient algorithm using a single neural networks to predict
the actions.</p>
<p>Args:
    model_config: the model configuration including the name of the target gym environment
        as well as the neural network architecture.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="nv">class</span> <span class="nv">TfReinforceAgent</span><span class="ss">(</span><span class="nv">TfAgent</span><span class="ss">)</span>:

    <span class="s2">&quot;&quot;&quot;</span><span class="s"> creates a new agent based on the Reinforce algorithm using the tfagents implementation.</span>

        <span class="nv">Reinforce</span> <span class="nv">is</span> <span class="nv">a</span> <span class="nv">vanilla</span> <span class="nv">policy</span> <span class="nv">gradient</span> <span class="nv">algorithm</span> <span class="nv">using</span> <span class="nv">a</span> <span class="nv">single</span> <span class="nv">neural</span> <span class="nv">networks</span> <span class="nv">to</span> <span class="nv">predict</span>

        <span class="nv">the</span> <span class="nv">actions</span>.

        <span class="nv">Args</span>:

            <span class="nv">model_config</span>: <span class="nv">the</span> <span class="nv">model</span> <span class="nv">configuration</span> <span class="nv">including</span> <span class="nv">the</span> <span class="nv">name</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">target</span> <span class="nv">gym</span> <span class="nv">environment</span>

                <span class="nv">as</span> <span class="nv">well</span> <span class="nv">as</span> <span class="nv">the</span> <span class="nv">neural</span> <span class="nv">network</span> <span class="nv">architecture</span>.

    <span class="s2">&quot;&quot;&quot;</span>

    <span class="nv">def</span> <span class="nv">__init__</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">model_config</span>: <span class="nv">core</span>.<span class="nv">ModelConfig</span><span class="ss">)</span>:

        <span class="nv">super</span><span class="ss">()</span>.<span class="nv">__init__</span><span class="ss">(</span><span class="nv">model_config</span><span class="o">=</span><span class="nv">model_config</span><span class="ss">)</span>

    # <span class="nv">noinspection</span> <span class="nv">DuplicatedCode</span>

    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Tf-Agents Reinforce Implementation of the train loop.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">train_context</span>, <span class="nv">core</span>.<span class="nv">EpisodesTrainContext</span><span class="ss">)</span>

        <span class="nv">tc</span>: <span class="nv">core</span>.<span class="nv">EpisodesTrainContext</span> <span class="o">=</span> <span class="nv">train_context</span>

        <span class="nv">self</span>.<span class="nv">log</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">Creating environment...</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">(</span><span class="nv">discount</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        <span class="nv">observation_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">observation_spec</span><span class="ss">()</span>

        <span class="nv">action_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">action_spec</span><span class="ss">()</span>

        <span class="nv">timestep_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">time_step_spec</span><span class="ss">()</span>

        # <span class="nv">SetUp</span> <span class="nv">Optimizer</span>, <span class="nv">Networks</span> <span class="nv">and</span> <span class="nv">PpoAgent</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">AdamOptimizer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">optimizer</span> <span class="o">=</span> <span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ActorDistributionNetwork</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">actor_net</span> <span class="o">=</span> <span class="nv">actor_distribution_network</span>.<span class="nv">ActorDistributionNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span>,

                                                                        <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ReinforceAgent</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span> <span class="o">=</span> <span class="nv">reinforce_agent</span>.<span class="nv">ReinforceAgent</span><span class="ss">(</span><span class="nv">timestep_spec</span>, <span class="nv">action_spec</span>, <span class="nv">actor_network</span><span class="o">=</span><span class="nv">actor_net</span>,

                                                  <span class="nv">optimizer</span><span class="o">=</span><span class="nv">optimizer</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.initialize()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">initialize</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">policy</span>

        # <span class="nv">SetUp</span> <span class="nv">Data</span> <span class="nv">collection</span> <span class="o">&amp;</span> <span class="nv">Buffering</span>

        <span class="nv">collect_data_spec</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">collect_data_spec</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">TFUniformReplayBuffer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">replay_buffer</span> <span class="o">=</span> <span class="nv">TFUniformReplayBuffer</span><span class="ss">(</span><span class="nv">collect_data_spec</span>, <span class="nv">batch_size</span><span class="o">=</span><span class="mi">1</span>, <span class="nv">max_length</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">max_steps_in_buffer</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DynamicEpisodeDriver</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">collect_driver</span> <span class="o">=</span> <span class="nv">DynamicEpisodeDriver</span><span class="ss">(</span><span class="nv">train_env</span>, <span class="nv">tf_agent</span>.<span class="nv">collect_policy</span>,

                                              <span class="nv">observers</span><span class="o">=</span>[<span class="nv">replay_buffer</span>.<span class="nv">add_batch</span>],

                                              <span class="nv">num_episodes</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_episodes_per_iteration</span><span class="ss">)</span>

        # <span class="nv">Train</span>

        <span class="nv">collect_driver</span>.<span class="nv">run</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">collect_driver</span>.<span class="nv">run</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">train</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">train</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">Starting training...</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            <span class="nv">msg</span> <span class="o">=</span> <span class="nv">f</span><span class="s1">&#39;</span><span class="s">iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:&lt;4}</span><span class="s1">&#39;</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">collect_driver.run</span><span class="s1">&#39;</span>, <span class="nv">msg</span><span class="ss">)</span>

            <span class="nv">collect_driver</span>.<span class="nv">run</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.gather_all</span><span class="s1">&#39;</span>, <span class="nv">msg</span><span class="ss">)</span>

            <span class="nv">trajectories</span> <span class="o">=</span> <span class="nv">replay_buffer</span>.<span class="nv">gather_all</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.train</span><span class="s1">&#39;</span>, <span class="nv">msg</span><span class="ss">)</span>

            <span class="nv">loss_info</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">(</span><span class="nv">experience</span><span class="o">=</span><span class="nv">trajectories</span><span class="ss">)</span>

            <span class="nv">total_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">loss={total_loss:&lt;7.1f}</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.clear</span><span class="s1">&#39;</span>, <span class="nv">msg</span><span class="ss">)</span>

            <span class="nv">replay_buffer</span>.<span class="nv">clear</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="nv">total_loss</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_5">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.backends.tfagents.TfAgent</li>
<li>easyagents.backends.core.BackendAgent</li>
<li>easyagents.backends.core._BackendAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_5">Methods</h4>
<h5 id="log_4">log</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span>
</pre></div>


<p>Logs msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">log_msg</span>: <span class="nv">str</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="log_api_4">log_api</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log_api</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">api_target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Logs a call to api_target with additional log_msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log_api</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">api_target</span>: <span class="nv">str</span>, <span class="nv">log_msg</span>: <span class="nv">Optional</span>[<span class="nv">str</span>] <span class="o">=</span> <span class="nv">None</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs a call to api_target with additional log_msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">api_target</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">api_target</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_api_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">api_target</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_begin_4">on_play_episode_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_begin</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Env</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the beginning of a new episode</p>
<p>Args:
    env: the gym environment used to play the episode.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">env</span>: <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the beginning of a new episode</span>

        <span class="nv">Args</span>:

            <span class="nv">env</span>: <span class="nv">the</span> <span class="nv">gym</span> <span class="nv">environment</span> <span class="nv">used</span> <span class="nv">to</span> <span class="nv">play</span> <span class="nv">the</span> <span class="nv">episode</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">env</span>, <span class="s2">&quot;</span><span class="s">env not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">env</span>, <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>, <span class="s2">&quot;</span><span class="s">env not an an instance of gym.Env.</span><span class="s2">&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">env</span>

        <span class="nv">pc</span>.<span class="nv">steps_done_in_episode</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">pc</span>.<span class="nv">actions</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">sum_of_rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_end_4">on_play_episode_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the end of an episode</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the end of an episode</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span> <span class="nv">and</span> <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">&gt;=</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span>:

            <span class="nv">pc</span>.<span class="nv">play_done</span> <span class="o">=</span> <span class="nv">True</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_begin_4">on_train_iteration_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_begin</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the begining of a new iteration</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the begining of a new iteration</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">tc</span>.<span class="nv">steps_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">==</span> <span class="mi">0</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>.<span class="nv">episodes_done</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_end_4">on_train_iteration_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the end of an iteration</p>
<p>Evaluates the current policy. Use kwargs to set additional dict values in train context.
Eg for an ActorCriticTrainContext the losses may be set like this:
    on_train_iteration(loss=123,actor_loss=456,critic_loss=789)</p>
<p>Args:
    loss: loss after the training of the model in this iteration or math.nan if the loss is not available
    **kwargs: if a keyword matches a dict property of the TrainContext instance, then
                the dict[episodes_done_in_training] is set to the arg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">loss</span>: <span class="nv">float</span>, <span class="o">**</span><span class="nv">kwargs</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the end of an iteration</span>

        <span class="nv">Evaluates</span> <span class="nv">the</span> <span class="nv">current</span> <span class="nv">policy</span>. <span class="nv">Use</span> <span class="nv">kwargs</span> <span class="nv">to</span> <span class="nv">set</span> <span class="nv">additional</span> <span class="nv">dict</span> <span class="nv">values</span> <span class="nv">in</span> <span class="nv">train</span> <span class="nv">context</span>.

        <span class="nv">Eg</span> <span class="k">for</span> <span class="nv">an</span> <span class="nv">ActorCriticTrainContext</span> <span class="nv">the</span> <span class="nv">losses</span> <span class="nv">may</span> <span class="nv">be</span> <span class="nv">set</span> <span class="nv">like</span> <span class="nv">this</span>:

            <span class="nv">on_train_iteration</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="mi">123</span>,<span class="nv">actor_loss</span><span class="o">=</span><span class="mi">456</span>,<span class="nv">critic_loss</span><span class="o">=</span><span class="mi">789</span><span class="ss">)</span>

        <span class="nv">Args</span>:

            <span class="nv">loss</span>: <span class="nv">loss</span> <span class="nv">after</span> <span class="nv">the</span> <span class="nv">training</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">model</span> <span class="nv">in</span> <span class="nv">this</span> <span class="nv">iteration</span> <span class="nv">or</span> <span class="nv">math</span>.<span class="nv">nan</span> <span class="k">if</span> <span class="nv">the</span> <span class="nv">loss</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">available</span>

            <span class="o">**</span><span class="nv">kwargs</span>: <span class="k">if</span> <span class="nv">a</span> <span class="nv">keyword</span> <span class="nv">matches</span> <span class="nv">a</span> <span class="nv">dict</span> <span class="nv">property</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">TrainContext</span> <span class="nv">instance</span>, <span class="k">then</span>

                        <span class="nv">the</span> <span class="nv">dict</span>[<span class="nv">episodes_done_in_training</span>] <span class="nv">is</span> <span class="nv">set</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">arg</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">totals</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="ss">(</span><span class="nv">totals</span>.<span class="nv">episodes_done</span> <span class="o">-</span> <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span><span class="ss">)</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span> <span class="o">+=</span> <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span>

        <span class="nv">tc</span>.<span class="nv">loss</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">loss</span>

        # <span class="nv">set</span> <span class="nv">traincontext</span> <span class="nv">dict</span> <span class="nv">from</span> <span class="nv">kwargs</span>:

        <span class="k">for</span> <span class="nv">prop_name</span> <span class="nv">in</span> <span class="nv">kwargs</span>:

            <span class="nv">prop_instance</span> <span class="o">=</span> <span class="nv">getattr</span><span class="ss">(</span><span class="nv">tc</span>, <span class="nv">prop_name</span>, <span class="nv">None</span><span class="ss">)</span>

            <span class="nv">prop_value</span> <span class="o">=</span> <span class="nv">kwargs</span>[<span class="nv">prop_name</span>]

            <span class="k">if</span> <span class="nv">prop_instance</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span> <span class="nv">and</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">prop_instance</span>, <span class="nv">dict</span><span class="ss">)</span>:

                <span class="nv">prop_instance</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">prop_value</span>

        <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span>:

            <span class="nv">tc</span>.<span class="nv">training_done</span> <span class="o">=</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">&gt;=</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="nv">and</span> <span class="ss">(</span><span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">%</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="o">==</span> <span class="mi">0</span><span class="ss">)</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="play_4">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to play_implementation overriden by the subclass.</p>
<p>Args:
    play_context: play configuration to be used
    callbacks: list of callbacks called during play.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to play_implementation overriden by the subclass.</span>

<span class="ss">            Args:</span>

<span class="ss">                play_context: play configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during play.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">play_context</span><span class="p">,</span> <span class="ss">&quot;play_context not set&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="k">is</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;play_context already set in agent_context&quot;</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="n">play_context</span>

        <span class="n">old_callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">play_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">old_callbacks</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="play_implementation_4">play_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span>
<span class="p">)</span>
</pre></div>


<p>Agent specific implementation of playing a single episodes with the current policy.</p>
<p>Args:
    play_context: play configuration to be used</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">play_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">play_context</span>: <span class="nv">core</span>.<span class="nv">PlayContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Agent specific implementation of playing a single episodes with the current policy.</span>

            <span class="nv">Args</span>:

                <span class="nv">play_context</span>: <span class="nv">play</span> <span class="nv">configuration</span> <span class="nv">to</span> <span class="nv">be</span> <span class="nv">used</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">play_context</span>, <span class="s2">&quot;</span><span class="s">play_context not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>, <span class="s2">&quot;</span><span class="s">trained_policy not set. call train() first.</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_get_gym_env</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_play_env</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">gym_env</span><span class="ss">)</span>

            <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">reset</span><span class="ss">()</span>

            <span class="k">while</span> <span class="nv">not</span> <span class="nv">time_step</span>.<span class="nv">is_last</span><span class="ss">()</span>:

                <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

                <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_play_episode_end</span><span class="ss">()</span>

            <span class="k">if</span> <span class="nv">play_context</span>.<span class="nv">play_done</span>:

                <span class="k">break</span>
</pre></div>


</details>
<h5 id="train_4">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to train_implementation overriden by the subclass</p>
<p>Args:
    train_context: training configuration to be used
    callbacks: list of callbacks called during the training and evaluation.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to train_implementation overriden by the subclass</span>

<span class="ss">            Args:</span>

<span class="ss">                train_context: training configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during the training and evaluation.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">train_context</span><span class="p">,</span> <span class="ss">&quot;train_context not set&quot;</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="k">self</span><span class="p">.</span><span class="n">log_api</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;backend_name&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;{self._backend_name}&#39;</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_set_seed</span><span class="p">()</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">train_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="train_implementation_4">train_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span>
<span class="p">)</span>
</pre></div>


<p>Tf-Agents Reinforce Implementation of the train loop.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Tf-Agents Reinforce Implementation of the train loop.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">train_context</span>, <span class="nv">core</span>.<span class="nv">EpisodesTrainContext</span><span class="ss">)</span>

        <span class="nv">tc</span>: <span class="nv">core</span>.<span class="nv">EpisodesTrainContext</span> <span class="o">=</span> <span class="nv">train_context</span>

        <span class="nv">self</span>.<span class="nv">log</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">Creating environment...</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">(</span><span class="nv">discount</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        <span class="nv">observation_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">observation_spec</span><span class="ss">()</span>

        <span class="nv">action_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">action_spec</span><span class="ss">()</span>

        <span class="nv">timestep_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">time_step_spec</span><span class="ss">()</span>

        # <span class="nv">SetUp</span> <span class="nv">Optimizer</span>, <span class="nv">Networks</span> <span class="nv">and</span> <span class="nv">PpoAgent</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">AdamOptimizer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">optimizer</span> <span class="o">=</span> <span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ActorDistributionNetwork</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">actor_net</span> <span class="o">=</span> <span class="nv">actor_distribution_network</span>.<span class="nv">ActorDistributionNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span>,

                                                                        <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ReinforceAgent</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span> <span class="o">=</span> <span class="nv">reinforce_agent</span>.<span class="nv">ReinforceAgent</span><span class="ss">(</span><span class="nv">timestep_spec</span>, <span class="nv">action_spec</span>, <span class="nv">actor_network</span><span class="o">=</span><span class="nv">actor_net</span>,

                                                  <span class="nv">optimizer</span><span class="o">=</span><span class="nv">optimizer</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.initialize()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">initialize</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">policy</span>

        # <span class="nv">SetUp</span> <span class="nv">Data</span> <span class="nv">collection</span> <span class="o">&amp;</span> <span class="nv">Buffering</span>

        <span class="nv">collect_data_spec</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">collect_data_spec</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">TFUniformReplayBuffer</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">replay_buffer</span> <span class="o">=</span> <span class="nv">TFUniformReplayBuffer</span><span class="ss">(</span><span class="nv">collect_data_spec</span>, <span class="nv">batch_size</span><span class="o">=</span><span class="mi">1</span>, <span class="nv">max_length</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">max_steps_in_buffer</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DynamicEpisodeDriver</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">create</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">collect_driver</span> <span class="o">=</span> <span class="nv">DynamicEpisodeDriver</span><span class="ss">(</span><span class="nv">train_env</span>, <span class="nv">tf_agent</span>.<span class="nv">collect_policy</span>,

                                              <span class="nv">observers</span><span class="o">=</span>[<span class="nv">replay_buffer</span>.<span class="nv">add_batch</span>],

                                              <span class="nv">num_episodes</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_episodes_per_iteration</span><span class="ss">)</span>

        # <span class="nv">Train</span>

        <span class="nv">collect_driver</span>.<span class="nv">run</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">collect_driver</span>.<span class="nv">run</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">train</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">train</span>, <span class="nv">autograph</span><span class="o">=</span><span class="nv">False</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">Starting training...</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            <span class="nv">msg</span> <span class="o">=</span> <span class="nv">f</span><span class="s1">&#39;</span><span class="s">iteration {tc.iterations_done_in_training:4} of {tc.num_iterations:&lt;4}</span><span class="s1">&#39;</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">collect_driver.run</span><span class="s1">&#39;</span>, <span class="nv">msg</span><span class="ss">)</span>

            <span class="nv">collect_driver</span>.<span class="nv">run</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.gather_all</span><span class="s1">&#39;</span>, <span class="nv">msg</span><span class="ss">)</span>

            <span class="nv">trajectories</span> <span class="o">=</span> <span class="nv">replay_buffer</span>.<span class="nv">gather_all</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.train</span><span class="s1">&#39;</span>, <span class="nv">msg</span><span class="ss">)</span>

            <span class="nv">loss_info</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">(</span><span class="nv">experience</span><span class="o">=</span><span class="nv">trajectories</span><span class="ss">)</span>

            <span class="nv">total_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">loss={total_loss:&lt;7.1f}</span><span class="s1">&#39;</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">replay_buffer.clear</span><span class="s1">&#39;</span>, <span class="nv">msg</span><span class="ss">)</span>

            <span class="nv">replay_buffer</span>.<span class="nv">clear</span><span class="ss">()</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="nv">total_loss</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
<h3 id="tfsacagent">TfSacAgent</h3>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">TfSacAgent</span><span class="p">(</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ModelConfig</span>
<span class="p">)</span>
</pre></div>


<p>creates a new agent based on the SAC algorithm using the tfagents implementation.</p>
<p>adapted from
    https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/7_SAC_minitaur_tutorial.ipynb</p>
<p>Args:
    model_config: the model configuration including the name of the target gym environment
        as well as the neural network architecture.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span><span class="nv">class</span> <span class="nv">TfSacAgent</span><span class="ss">(</span><span class="nv">TfAgent</span><span class="ss">)</span>:

    <span class="s2">&quot;&quot;&quot;</span><span class="s"> creates a new agent based on the SAC algorithm using the tfagents implementation.</span>

        <span class="nv">adapted</span> <span class="nv">from</span>

            <span class="nv">https</span>:<span class="o">//</span><span class="nv">github</span>.<span class="nv">com</span><span class="o">/</span><span class="nv">tensorflow</span><span class="o">/</span><span class="nv">agents</span><span class="o">/</span><span class="nv">blob</span><span class="o">/</span><span class="nv">master</span><span class="o">/</span><span class="nv">tf_agents</span><span class="o">/</span><span class="nv">colabs</span><span class="o">/</span><span class="mi">7</span><span class="nv">_SAC_minitaur_tutorial</span>.<span class="nv">ipynb</span>

        <span class="nv">Args</span>:

            <span class="nv">model_config</span>: <span class="nv">the</span> <span class="nv">model</span> <span class="nv">configuration</span> <span class="nv">including</span> <span class="nv">the</span> <span class="nv">name</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">target</span> <span class="nv">gym</span> <span class="nv">environment</span>

                <span class="nv">as</span> <span class="nv">well</span> <span class="nv">as</span> <span class="nv">the</span> <span class="nv">neural</span> <span class="nv">network</span> <span class="nv">architecture</span>.

    <span class="s2">&quot;&quot;&quot;</span>

    <span class="nv">def</span> <span class="nv">__init__</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">model_config</span>: <span class="nv">core</span>.<span class="nv">ModelConfig</span><span class="ss">)</span>:

        <span class="nv">super</span><span class="ss">()</span>.<span class="nv">__init__</span><span class="ss">(</span><span class="nv">model_config</span><span class="o">=</span><span class="nv">model_config</span><span class="ss">)</span>

    # <span class="nv">noinspection</span> <span class="nv">DuplicatedCode</span>

    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Tf-Agents Ppo Implementation of the train loop.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">train_context</span>, <span class="nv">core</span>.<span class="nv">StepsTrainContext</span><span class="ss">)</span>

        <span class="nv">tc</span>: <span class="nv">core</span>.<span class="nv">StepsTrainContext</span> <span class="o">=</span> <span class="nv">train_context</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">(</span><span class="nv">discount</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        <span class="nv">observation_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">observation_spec</span><span class="ss">()</span>

        <span class="nv">action_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">action_spec</span><span class="ss">()</span>

        <span class="nv">timestep_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">time_step_spec</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">CriticNetwork</span><span class="s1">&#39;</span>,

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(observation_spec, action_spec), observation_fc_layer_params=None, </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">critic_net</span> <span class="o">=</span> <span class="nv">critic_network</span>.<span class="nv">CriticNetwork</span><span class="ss">((</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span><span class="ss">)</span>,

                                                  <span class="nv">observation_fc_layer_params</span><span class="o">=</span><span class="nv">None</span>, <span class="nv">action_fc_layer_params</span><span class="o">=</span><span class="nv">None</span>,

                                                  <span class="nv">joint_fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">def</span> <span class="nv">normal_projection_net</span><span class="ss">(</span><span class="nv">action_spec_arg</span>, <span class="nv">init_means_output_factor</span><span class="o">=</span><span class="mi">0</span>.<span class="mi">1</span><span class="ss">)</span>:

            <span class="k">return</span> <span class="nv">normal_projection_network</span>.<span class="nv">NormalProjectionNetwork</span><span class="ss">(</span><span class="nv">action_spec_arg</span>,

                                                                     <span class="nv">mean_transform</span><span class="o">=</span><span class="nv">None</span>,

                                                                     <span class="nv">state_dependent_std</span><span class="o">=</span><span class="nv">True</span>,

                                                                     <span class="nv">init_means_output_factor</span><span class="o">=</span><span class="nv">init_means_output_factor</span>,

                                                                     <span class="nv">std_transform</span><span class="o">=</span><span class="nv">sac_agent</span>.<span class="nv">std_clip_transform</span>,

                                                                     <span class="nv">scale_distribution</span><span class="o">=</span><span class="nv">True</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ActorDistributionNetwork</span><span class="s1">&#39;</span>,

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">continuous_projection_net=...)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">actor_net</span> <span class="o">=</span> <span class="nv">actor_distribution_network</span>.<span class="nv">ActorDistributionNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span>,

                                                                        <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span>,

                                                                        <span class="nv">continuous_projection_net</span><span class="o">=</span><span class="nv">normal_projection_net</span><span class="ss">)</span>

        # <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf.compat.v1.train.get_or_create_global_step</span><span class="s1">&#39;</span>,<span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        # <span class="nv">global_step</span> <span class="o">=</span> <span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">get_or_create_global_step</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">SacAgent</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(timestep_spec, action_spec, actor_network=..., critic_network=..., </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">gamma={tc.reward_discount_gamma})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span> <span class="o">=</span> <span class="nv">sac_agent</span>.<span class="nv">SacAgent</span><span class="ss">(</span>

            <span class="nv">timestep_spec</span>,

            <span class="nv">action_spec</span>,

            <span class="nv">actor_network</span><span class="o">=</span><span class="nv">actor_net</span>,

            <span class="nv">critic_network</span><span class="o">=</span><span class="nv">critic_net</span>,

            <span class="nv">actor_optimizer</span><span class="o">=</span><span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>,

            <span class="nv">critic_optimizer</span><span class="o">=</span><span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>,

            <span class="nv">alpha_optimizer</span><span class="o">=</span><span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>,

            # <span class="nv">target_update_tau</span><span class="o">=</span><span class="mi">0</span>.<span class="mi">005</span>,

            # <span class="nv">target_update_period</span><span class="o">=</span><span class="mi">1</span>,

            # <span class="nv">td_errors_loss_fn</span><span class="o">=</span><span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">losses</span>.<span class="nv">mean_squared_error</span>,

            <span class="nv">gamma</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        # <span class="nv">reward_scale_factor</span><span class="o">=</span><span class="mi">1</span>.<span class="mi">0</span>,

        # <span class="nv">gradient_clipping</span><span class="o">=</span><span class="nv">None</span>,

        # <span class="nv">train_step_counter</span><span class="o">=</span><span class="nv">global_step</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.initialize</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">initialize</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">greedy_policy</span>.<span class="nv">GreedyPolicy</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">policy</span><span class="ss">)</span>

        <span class="nv">collect_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">collect_policy</span>

        # <span class="nv">setup</span> <span class="nv">and</span> <span class="nv">preload</span> <span class="nv">replay</span> <span class="nv">buffer</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">TFUniformReplayBuffer</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(data_spec=tf_agent.collect_data_spec, </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">replay_buffer</span> <span class="o">=</span> <span class="nv">tf_uniform_replay_buffer</span>.<span class="nv">TFUniformReplayBuffer</span><span class="ss">(</span><span class="nv">data_spec</span><span class="o">=</span><span class="nv">tf_agent</span>.<span class="nv">collect_data_spec</span>,

                                                                       <span class="nv">batch_size</span><span class="o">=</span><span class="nv">train_env</span>.<span class="nv">batch_size</span>,

                                                                       <span class="nv">max_length</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">max_steps_in_buffer</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DynamicStepDriver</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(env, collect_policy, observers=[replay_buffer.add_batch], </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">num_steps={tc.num_steps_buffer_preload})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">initial_collect_driver</span> <span class="o">=</span> <span class="nv">dynamic_step_driver</span>.<span class="nv">DynamicStepDriver</span><span class="ss">(</span><span class="nv">train_env</span>,

                                                                       <span class="nv">collect_policy</span>,

                                                                       <span class="nv">observers</span><span class="o">=</span>[<span class="nv">replay_buffer</span>.<span class="nv">add_batch</span>],

                                                                       <span class="nv">num_steps</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_steps_buffer_preload</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">initial_collect_driver.run()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">initial_collect_driver</span>.<span class="nv">run</span><span class="ss">()</span>

        # <span class="nv">Dataset</span> <span class="nv">generates</span> <span class="nv">trajectories</span> <span class="nv">with</span> <span class="nv">shape</span> [<span class="nv">Bx2x</span>...]

        <span class="nv">dataset</span> <span class="o">=</span> <span class="nv">replay_buffer</span>.<span class="nv">as_dataset</span><span class="ss">(</span><span class="nv">num_parallel_calls</span><span class="o">=</span><span class="mi">3</span>, <span class="nv">sample_batch_size</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_steps_sampled_from_buffer</span>,

                                           <span class="nv">num_steps</span><span class="o">=</span><span class="mi">2</span><span class="ss">)</span>.<span class="nv">prefetch</span><span class="ss">(</span><span class="mi">3</span><span class="ss">)</span>

        <span class="nv">iterator</span> <span class="o">=</span> <span class="nv">iter</span><span class="ss">(</span><span class="nv">dataset</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DynamicStepDriver</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(env, collect_policy, observers=[replay_buffer.add_batch], </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">num_steps={tc.num_steps_per_iteration})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">collect_driver</span> <span class="o">=</span> <span class="nv">dynamic_step_driver</span>.<span class="nv">DynamicStepDriver</span><span class="ss">(</span><span class="nv">train_env</span>,

                                                               <span class="nv">collect_policy</span>,

                                                               <span class="nv">observers</span><span class="o">=</span>[<span class="nv">replay_buffer</span>.<span class="nv">add_batch</span>],

                                                               <span class="nv">num_steps</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_steps_per_iteration</span><span class="ss">)</span>

        # <span class="ss">(</span><span class="nv">Optional</span><span class="ss">)</span> <span class="nv">Optimize</span> <span class="nv">by</span> <span class="nv">wrapping</span> <span class="nv">some</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">code</span> <span class="nv">in</span> <span class="nv">a</span> <span class="nv">graph</span> <span class="nv">using</span> <span class="nv">TF</span> <span class="nv">function</span>.

        <span class="nv">tf_agent</span>.<span class="nv">train</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">)</span>

        <span class="nv">collect_driver</span>.<span class="nv">run</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">collect_driver</span>.<span class="nv">run</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">for each iteration</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">  collect_driver.run</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">  tf_agent.train</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(experience=...)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            # <span class="nv">Collect</span> <span class="nv">a</span> <span class="nv">few</span> <span class="nv">steps</span> <span class="nv">using</span> <span class="nv">collect_policy</span> <span class="nv">and</span> <span class="nv">save</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">replay</span> <span class="nv">buffer</span>.

            <span class="k">for</span> <span class="nv">_</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="nv">tc</span>.<span class="nv">num_steps_per_iteration</span><span class="ss">)</span>:

                <span class="nv">collect_driver</span>.<span class="nv">run</span><span class="ss">()</span>

            # <span class="nv">Sample</span> <span class="nv">a</span> <span class="nv">batch</span> <span class="nv">of</span> <span class="nv">data</span> <span class="nv">from</span> <span class="nv">the</span> <span class="nv">buffer</span> <span class="nv">and</span> <span class="nv">update</span> <span class="nv">the</span> <span class="nv">agent</span><span class="s1">&#39;</span><span class="s">s network.</span>

            <span class="nv">experience</span>, <span class="nv">_</span> <span class="o">=</span> <span class="k">next</span><span class="ss">(</span><span class="nv">iterator</span><span class="ss">)</span>

            <span class="nv">loss_info</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">(</span><span class="nv">experience</span><span class="ss">)</span>

            <span class="nv">total_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">actor_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">actor_loss</span>

            <span class="nv">alpha_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">alpha_loss</span>

            <span class="nv">critic_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">critic_loss</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="nv">total_loss</span>, <span class="nv">actor_loss</span><span class="o">=</span><span class="nv">actor_loss</span>, <span class="nv">critic_loss</span><span class="o">=</span><span class="nv">critic_loss</span>,

                                        <span class="nv">alpha_loss</span><span class="o">=</span><span class="nv">alpha_loss</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
<hr />
<h4 id="ancestors-in-mro_6">Ancestors (in MRO)</h4>
<ul>
<li>easyagents.backends.tfagents.TfAgent</li>
<li>easyagents.backends.core.BackendAgent</li>
<li>easyagents.backends.core._BackendAgent</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_6">Methods</h4>
<h5 id="log_5">log</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span>
</pre></div>


<p>Logs msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">log_msg</span>: <span class="nv">str</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="log_api_5">log_api</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">log_api</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">api_target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">log_msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Logs a call to api_target with additional log_msg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">log_api</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">api_target</span>: <span class="nv">str</span>, <span class="nv">log_msg</span>: <span class="nv">Optional</span>[<span class="nv">str</span>] <span class="o">=</span> <span class="nv">None</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Logs a call to api_target with additional log_msg.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_monitor_env</span> <span class="o">=</span> <span class="nv">None</span>

        <span class="k">if</span> <span class="nv">api_target</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">api_target</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">if</span> <span class="nv">log_msg</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">log_msg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_api_log</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span>, <span class="nv">api_target</span>, <span class="nv">log_msg</span><span class="o">=</span><span class="nv">log_msg</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_begin_5">on_play_episode_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_begin</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Env</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the beginning of a new episode</p>
<p>Args:
    env: the gym environment used to play the episode.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">env</span>: <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the beginning of a new episode</span>

        <span class="nv">Args</span>:

            <span class="nv">env</span>: <span class="nv">the</span> <span class="nv">gym</span> <span class="nv">environment</span> <span class="nv">used</span> <span class="nv">to</span> <span class="nv">play</span> <span class="nv">the</span> <span class="nv">episode</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">env</span>, <span class="s2">&quot;</span><span class="s">env not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">env</span>, <span class="nv">gym</span>.<span class="nv">core</span>.<span class="nv">Env</span><span class="ss">)</span>, <span class="s2">&quot;</span><span class="s">env not an an instance of gym.Env.</span><span class="s2">&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">env</span>

        <span class="nv">pc</span>.<span class="nv">steps_done_in_episode</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">pc</span>.<span class="nv">actions</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> []

        <span class="nv">pc</span>.<span class="nv">sum_of_rewards</span>[<span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+</span> <span class="mi">1</span>] <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_play_episode_end_5">on_play_episode_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_play_episode_end</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by play_implementation at the end of an episode</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by play_implementation at the end of an episode</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">pc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">play</span>

        <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span> <span class="nv">and</span> <span class="nv">pc</span>.<span class="nv">episodes_done</span> <span class="o">&gt;=</span> <span class="nv">pc</span>.<span class="nv">num_episodes</span>:

            <span class="nv">pc</span>.<span class="nv">play_done</span> <span class="o">=</span> <span class="nv">True</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_play_episode_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_begin_5">on_train_iteration_begin</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_begin</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the begining of a new iteration</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the begining of a new iteration</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="nv">tc</span>.<span class="nv">steps_done_in_iteration</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">==</span> <span class="mi">0</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>.<span class="nv">episodes_done</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="on_train_iteration_end_5">on_train_iteration_end</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">on_train_iteration_end</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<p>Must be called by train_implementation at the end of an iteration</p>
<p>Evaluates the current policy. Use kwargs to set additional dict values in train context.
Eg for an ActorCriticTrainContext the losses may be set like this:
    on_train_iteration(loss=123,actor_loss=456,critic_loss=789)</p>
<p>Args:
    loss: loss after the training of the model in this iteration or math.nan if the loss is not available
    **kwargs: if a keyword matches a dict property of the TrainContext instance, then
                the dict[episodes_done_in_training] is set to the arg.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">loss</span>: <span class="nv">float</span>, <span class="o">**</span><span class="nv">kwargs</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Must be called by train_implementation at the end of an iteration</span>

        <span class="nv">Evaluates</span> <span class="nv">the</span> <span class="nv">current</span> <span class="nv">policy</span>. <span class="nv">Use</span> <span class="nv">kwargs</span> <span class="nv">to</span> <span class="nv">set</span> <span class="nv">additional</span> <span class="nv">dict</span> <span class="nv">values</span> <span class="nv">in</span> <span class="nv">train</span> <span class="nv">context</span>.

        <span class="nv">Eg</span> <span class="k">for</span> <span class="nv">an</span> <span class="nv">ActorCriticTrainContext</span> <span class="nv">the</span> <span class="nv">losses</span> <span class="nv">may</span> <span class="nv">be</span> <span class="nv">set</span> <span class="nv">like</span> <span class="nv">this</span>:

            <span class="nv">on_train_iteration</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="mi">123</span>,<span class="nv">actor_loss</span><span class="o">=</span><span class="mi">456</span>,<span class="nv">critic_loss</span><span class="o">=</span><span class="mi">789</span><span class="ss">)</span>

        <span class="nv">Args</span>:

            <span class="nv">loss</span>: <span class="nv">loss</span> <span class="nv">after</span> <span class="nv">the</span> <span class="nv">training</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">model</span> <span class="nv">in</span> <span class="nv">this</span> <span class="nv">iteration</span> <span class="nv">or</span> <span class="nv">math</span>.<span class="nv">nan</span> <span class="k">if</span> <span class="nv">the</span> <span class="nv">loss</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">available</span>

            <span class="o">**</span><span class="nv">kwargs</span>: <span class="k">if</span> <span class="nv">a</span> <span class="nv">keyword</span> <span class="nv">matches</span> <span class="nv">a</span> <span class="nv">dict</span> <span class="nv">property</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">TrainContext</span> <span class="nv">instance</span>, <span class="k">then</span>

                        <span class="nv">the</span> <span class="nv">dict</span>[<span class="nv">episodes_done_in_training</span>] <span class="nv">is</span> <span class="nv">set</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">arg</span>.

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">tc</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">train</span>

        <span class="nv">totals</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_agent_context</span>.<span class="nv">gym</span>.<span class="nv">_totals</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span> <span class="o">=</span> <span class="ss">(</span><span class="nv">totals</span>.<span class="nv">episodes_done</span> <span class="o">-</span> <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span><span class="ss">)</span>

        <span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span> <span class="o">+=</span> <span class="nv">tc</span>.<span class="nv">episodes_done_in_iteration</span>

        <span class="nv">tc</span>.<span class="nv">loss</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">loss</span>

        # <span class="nv">set</span> <span class="nv">traincontext</span> <span class="nv">dict</span> <span class="nv">from</span> <span class="nv">kwargs</span>:

        <span class="k">for</span> <span class="nv">prop_name</span> <span class="nv">in</span> <span class="nv">kwargs</span>:

            <span class="nv">prop_instance</span> <span class="o">=</span> <span class="nv">getattr</span><span class="ss">(</span><span class="nv">tc</span>, <span class="nv">prop_name</span>, <span class="nv">None</span><span class="ss">)</span>

            <span class="nv">prop_value</span> <span class="o">=</span> <span class="nv">kwargs</span>[<span class="nv">prop_name</span>]

            <span class="k">if</span> <span class="nv">prop_instance</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span> <span class="nv">and</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">prop_instance</span>, <span class="nv">dict</span><span class="ss">)</span>:

                <span class="nv">prop_instance</span>[<span class="nv">tc</span>.<span class="nv">episodes_done_in_training</span>] <span class="o">=</span> <span class="nv">prop_value</span>

        <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span> <span class="nv">is</span> <span class="nv">not</span> <span class="nv">None</span>:

            <span class="nv">tc</span>.<span class="nv">training_done</span> <span class="o">=</span> <span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">&gt;=</span> <span class="nv">tc</span>.<span class="nv">num_iterations</span>

        <span class="nv">self</span>.<span class="nv">_train_total_episodes_on_iteration_begin</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="nv">and</span> <span class="ss">(</span><span class="nv">tc</span>.<span class="nv">iterations_done_in_training</span> <span class="o">%</span> <span class="nv">tc</span>.<span class="nv">num_iterations_between_eval</span> <span class="o">==</span> <span class="mi">0</span><span class="ss">)</span>:

            <span class="nv">self</span>.<span class="nv">_eval_current_policy</span><span class="ss">()</span>

        <span class="k">for</span> <span class="nv">c</span> <span class="nv">in</span> <span class="nv">self</span>.<span class="nv">_callbacks</span>:

            <span class="nv">c</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_agent_context</span><span class="ss">)</span>
</pre></div>


</details>
<h5 id="play_5">play</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to play_implementation overriden by the subclass.</p>
<p>Args:
    play_context: play configuration to be used
    callbacks: list of callbacks called during play.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">play</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">play_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">PlayContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to play_implementation overriden by the subclass.</span>

<span class="ss">            Args:</span>

<span class="ss">                play_context: play configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during play.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">play_context</span><span class="p">,</span> <span class="ss">&quot;play_context not set&quot;</span>

        <span class="n">assert</span> <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="k">is</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;play_context already set in agent_context&quot;</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">play_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="n">play_context</span>

        <span class="n">old_callbacks</span> <span class="o">=</span> <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">play_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_play_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">old_callbacks</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="play_implementation_5">play_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">play_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">play_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">PlayContext</span>
<span class="p">)</span>
</pre></div>


<p>Agent specific implementation of playing a single episodes with the current policy.</p>
<p>Args:
    play_context: play configuration to be used</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">play_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">play_context</span>: <span class="nv">core</span>.<span class="nv">PlayContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Agent specific implementation of playing a single episodes with the current policy.</span>

            <span class="nv">Args</span>:

                <span class="nv">play_context</span>: <span class="nv">play</span> <span class="nv">configuration</span> <span class="nv">to</span> <span class="nv">be</span> <span class="nv">used</span>

        <span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">play_context</span>, <span class="s2">&quot;</span><span class="s">play_context not set.</span><span class="s2">&quot;</span>

        <span class="nv">assert</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>, <span class="s2">&quot;</span><span class="s">trained_policy not set. call train() first.</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="nv">is</span> <span class="nv">None</span>:

            <span class="nv">self</span>.<span class="nv">_play_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">()</span>

        <span class="nv">gym_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_get_gym_env</span><span class="ss">(</span><span class="nv">self</span>.<span class="nv">_play_env</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_play_episode_begin</span><span class="ss">(</span><span class="nv">env</span><span class="o">=</span><span class="nv">gym_env</span><span class="ss">)</span>

            <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">reset</span><span class="ss">()</span>

            <span class="k">while</span> <span class="nv">not</span> <span class="nv">time_step</span>.<span class="nv">is_last</span><span class="ss">()</span>:

                <span class="nv">action_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_trained_policy</span>.<span class="nv">action</span><span class="ss">(</span><span class="nv">time_step</span><span class="ss">)</span>

                <span class="nv">time_step</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_play_env</span>.<span class="nv">step</span><span class="ss">(</span><span class="nv">action_step</span>.<span class="nv">action</span><span class="ss">)</span>

            <span class="nv">self</span>.<span class="nv">on_play_episode_end</span><span class="ss">()</span>

            <span class="k">if</span> <span class="nv">play_context</span>.<span class="nv">play_done</span>:

                <span class="k">break</span>
</pre></div>


</details>
<h5 id="train_5">train</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">AgentCallback</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>


<p>Forwarding to train_implementation overriden by the subclass</p>
<p>Args:
    train_context: training configuration to be used
    callbacks: list of callbacks called during the training and evaluation.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="n">def</span> <span class="n">train</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">train_context</span><span class="p">:</span> <span class="n">core</span><span class="p">.</span><span class="n">TrainContext</span><span class="p">,</span> <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">core</span><span class="p">.</span><span class="n">AgentCallback</span><span class="p">]):</span>

        <span class="ss">&quot;&quot;&quot;Forwarding to train_implementation overriden by the subclass</span>

<span class="ss">            Args:</span>

<span class="ss">                train_context: training configuration to be used</span>

<span class="ss">                callbacks: list of callbacks called during the training and evaluation.</span>

<span class="ss">        &quot;&quot;&quot;</span>

        <span class="n">assert</span> <span class="n">callbacks</span> <span class="k">is</span> <span class="k">not</span> <span class="k">None</span><span class="p">,</span> <span class="ss">&quot;callbacks not set&quot;</span>

        <span class="n">assert</span> <span class="n">train_context</span><span class="p">,</span> <span class="ss">&quot;train_context not set&quot;</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_reset</span><span class="p">()</span>

        <span class="n">train_context</span><span class="p">.</span><span class="n">_validate</span><span class="p">()</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">train_context</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

        <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>

        <span class="n">try</span><span class="p">:</span>

            <span class="k">self</span><span class="p">.</span><span class="n">log_api</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;backend_name&#39;</span><span class="p">,</span> <span class="n">f</span><span class="s1">&#39;{self._backend_name}&#39;</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_set_seed</span><span class="p">()</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">self</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_begin</span><span class="p">()</span>

            <span class="k">self</span><span class="p">.</span><span class="n">train_implementation</span><span class="p">(</span><span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_on_train_end</span><span class="p">()</span>

        <span class="n">finally</span><span class="p">:</span>

            <span class="n">monitor</span><span class="p">.</span><span class="n">_MonitorEnv</span><span class="p">.</span><span class="n">_register_backend_agent</span><span class="p">(</span><span class="k">None</span><span class="p">)</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_callbacks</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">play</span> <span class="o">=</span> <span class="k">None</span>

            <span class="k">self</span><span class="p">.</span><span class="n">_agent_context</span><span class="p">.</span><span class="n">train</span> <span class="o">=</span> <span class="k">None</span>
</pre></div>


</details>
<h5 id="train_implementation_5">train_implementation</h5>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">train_implementation</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train_context</span><span class="p">:</span> <span class="n">easyagents</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">TrainContext</span>
<span class="p">)</span>
</pre></div>


<p>Tf-Agents Ppo Implementation of the train loop.</p>
<details class="example"><summary>View Source</summary><div class="codehilite"><pre><span></span>    <span class="nv">def</span> <span class="nv">train_implementation</span><span class="ss">(</span><span class="nv">self</span>, <span class="nv">train_context</span>: <span class="nv">core</span>.<span class="nv">TrainContext</span><span class="ss">)</span>:

        <span class="s2">&quot;&quot;&quot;</span><span class="s">Tf-Agents Ppo Implementation of the train loop.</span><span class="s2">&quot;&quot;&quot;</span>

        <span class="nv">assert</span> <span class="nv">isinstance</span><span class="ss">(</span><span class="nv">train_context</span>, <span class="nv">core</span>.<span class="nv">StepsTrainContext</span><span class="ss">)</span>

        <span class="nv">tc</span>: <span class="nv">core</span>.<span class="nv">StepsTrainContext</span> <span class="o">=</span> <span class="nv">train_context</span>

        <span class="nv">train_env</span> <span class="o">=</span> <span class="nv">self</span>.<span class="nv">_create_env</span><span class="ss">(</span><span class="nv">discount</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        <span class="nv">observation_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">observation_spec</span><span class="ss">()</span>

        <span class="nv">action_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">action_spec</span><span class="ss">()</span>

        <span class="nv">timestep_spec</span> <span class="o">=</span> <span class="nv">train_env</span>.<span class="nv">time_step_spec</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">CriticNetwork</span><span class="s1">&#39;</span>,

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(observation_spec, action_spec), observation_fc_layer_params=None, </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">action_fc_layer_params=None, joint_fc_layer_params={self.model_config.fc_layers})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">critic_net</span> <span class="o">=</span> <span class="nv">critic_network</span>.<span class="nv">CriticNetwork</span><span class="ss">((</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span><span class="ss">)</span>,

                                                  <span class="nv">observation_fc_layer_params</span><span class="o">=</span><span class="nv">None</span>, <span class="nv">action_fc_layer_params</span><span class="o">=</span><span class="nv">None</span>,

                                                  <span class="nv">joint_fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span><span class="ss">)</span>

        <span class="nv">def</span> <span class="nv">normal_projection_net</span><span class="ss">(</span><span class="nv">action_spec_arg</span>, <span class="nv">init_means_output_factor</span><span class="o">=</span><span class="mi">0</span>.<span class="mi">1</span><span class="ss">)</span>:

            <span class="k">return</span> <span class="nv">normal_projection_network</span>.<span class="nv">NormalProjectionNetwork</span><span class="ss">(</span><span class="nv">action_spec_arg</span>,

                                                                     <span class="nv">mean_transform</span><span class="o">=</span><span class="nv">None</span>,

                                                                     <span class="nv">state_dependent_std</span><span class="o">=</span><span class="nv">True</span>,

                                                                     <span class="nv">init_means_output_factor</span><span class="o">=</span><span class="nv">init_means_output_factor</span>,

                                                                     <span class="nv">std_transform</span><span class="o">=</span><span class="nv">sac_agent</span>.<span class="nv">std_clip_transform</span>,

                                                                     <span class="nv">scale_distribution</span><span class="o">=</span><span class="nv">True</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">ActorDistributionNetwork</span><span class="s1">&#39;</span>,

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">observation_spec, action_spec, fc_layer_params={self.model_config.fc_layers}), </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">continuous_projection_net=...)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">actor_net</span> <span class="o">=</span> <span class="nv">actor_distribution_network</span>.<span class="nv">ActorDistributionNetwork</span><span class="ss">(</span><span class="nv">observation_spec</span>, <span class="nv">action_spec</span>,

                                                                        <span class="nv">fc_layer_params</span><span class="o">=</span><span class="nv">self</span>.<span class="nv">model_config</span>.<span class="nv">fc_layers</span>,

                                                                        <span class="nv">continuous_projection_net</span><span class="o">=</span><span class="nv">normal_projection_net</span><span class="ss">)</span>

        # <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf.compat.v1.train.get_or_create_global_step</span><span class="s1">&#39;</span>,<span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        # <span class="nv">global_step</span> <span class="o">=</span> <span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">get_or_create_global_step</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">SacAgent</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(timestep_spec, action_spec, actor_network=..., critic_network=..., </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">actor_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">critic_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">alpha_optimizer=AdamOptimizer(learning_rate={tc.learning_rate}), </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">gamma={tc.reward_discount_gamma})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span> <span class="o">=</span> <span class="nv">sac_agent</span>.<span class="nv">SacAgent</span><span class="ss">(</span>

            <span class="nv">timestep_spec</span>,

            <span class="nv">action_spec</span>,

            <span class="nv">actor_network</span><span class="o">=</span><span class="nv">actor_net</span>,

            <span class="nv">critic_network</span><span class="o">=</span><span class="nv">critic_net</span>,

            <span class="nv">actor_optimizer</span><span class="o">=</span><span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>,

            <span class="nv">critic_optimizer</span><span class="o">=</span><span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>,

            <span class="nv">alpha_optimizer</span><span class="o">=</span><span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">train</span>.<span class="nv">AdamOptimizer</span><span class="ss">(</span><span class="nv">learning_rate</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">learning_rate</span><span class="ss">)</span>,

            # <span class="nv">target_update_tau</span><span class="o">=</span><span class="mi">0</span>.<span class="mi">005</span>,

            # <span class="nv">target_update_period</span><span class="o">=</span><span class="mi">1</span>,

            # <span class="nv">td_errors_loss_fn</span><span class="o">=</span><span class="nv">tf</span>.<span class="nv">compat</span>.<span class="nv">v1</span>.<span class="nv">losses</span>.<span class="nv">mean_squared_error</span>,

            <span class="nv">gamma</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">reward_discount_gamma</span><span class="ss">)</span>

        # <span class="nv">reward_scale_factor</span><span class="o">=</span><span class="mi">1</span>.<span class="mi">0</span>,

        # <span class="nv">gradient_clipping</span><span class="o">=</span><span class="nv">None</span>,

        # <span class="nv">train_step_counter</span><span class="o">=</span><span class="nv">global_step</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">tf_agent.initialize</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">tf_agent</span>.<span class="nv">initialize</span><span class="ss">()</span>

        <span class="nv">self</span>.<span class="nv">_trained_policy</span> <span class="o">=</span> <span class="nv">greedy_policy</span>.<span class="nv">GreedyPolicy</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">policy</span><span class="ss">)</span>

        <span class="nv">collect_policy</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">collect_policy</span>

        # <span class="nv">setup</span> <span class="nv">and</span> <span class="nv">preload</span> <span class="nv">replay</span> <span class="nv">buffer</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">TFUniformReplayBuffer</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(data_spec=tf_agent.collect_data_spec, </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">batch_size={train_env.batch_size}, max_length={tc.max_steps_in_buffer})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">replay_buffer</span> <span class="o">=</span> <span class="nv">tf_uniform_replay_buffer</span>.<span class="nv">TFUniformReplayBuffer</span><span class="ss">(</span><span class="nv">data_spec</span><span class="o">=</span><span class="nv">tf_agent</span>.<span class="nv">collect_data_spec</span>,

                                                                       <span class="nv">batch_size</span><span class="o">=</span><span class="nv">train_env</span>.<span class="nv">batch_size</span>,

                                                                       <span class="nv">max_length</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">max_steps_in_buffer</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DynamicStepDriver</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(env, collect_policy, observers=[replay_buffer.add_batch], </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">num_steps={tc.num_steps_buffer_preload})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">initial_collect_driver</span> <span class="o">=</span> <span class="nv">dynamic_step_driver</span>.<span class="nv">DynamicStepDriver</span><span class="ss">(</span><span class="nv">train_env</span>,

                                                                       <span class="nv">collect_policy</span>,

                                                                       <span class="nv">observers</span><span class="o">=</span>[<span class="nv">replay_buffer</span>.<span class="nv">add_batch</span>],

                                                                       <span class="nv">num_steps</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_steps_buffer_preload</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">initial_collect_driver.run()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">initial_collect_driver</span>.<span class="nv">run</span><span class="ss">()</span>

        # <span class="nv">Dataset</span> <span class="nv">generates</span> <span class="nv">trajectories</span> <span class="nv">with</span> <span class="nv">shape</span> [<span class="nv">Bx2x</span>...]

        <span class="nv">dataset</span> <span class="o">=</span> <span class="nv">replay_buffer</span>.<span class="nv">as_dataset</span><span class="ss">(</span><span class="nv">num_parallel_calls</span><span class="o">=</span><span class="mi">3</span>, <span class="nv">sample_batch_size</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_steps_sampled_from_buffer</span>,

                                           <span class="nv">num_steps</span><span class="o">=</span><span class="mi">2</span><span class="ss">)</span>.<span class="nv">prefetch</span><span class="ss">(</span><span class="mi">3</span><span class="ss">)</span>

        <span class="nv">iterator</span> <span class="o">=</span> <span class="nv">iter</span><span class="ss">(</span><span class="nv">dataset</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">DynamicStepDriver</span><span class="s1">&#39;</span>, <span class="nv">f</span><span class="s1">&#39;</span><span class="s">(env, collect_policy, observers=[replay_buffer.add_batch], </span><span class="s1">&#39;</span> <span class="o">+</span>

                     <span class="nv">f</span><span class="s1">&#39;</span><span class="s">num_steps={tc.num_steps_per_iteration})</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">collect_driver</span> <span class="o">=</span> <span class="nv">dynamic_step_driver</span>.<span class="nv">DynamicStepDriver</span><span class="ss">(</span><span class="nv">train_env</span>,

                                                               <span class="nv">collect_policy</span>,

                                                               <span class="nv">observers</span><span class="o">=</span>[<span class="nv">replay_buffer</span>.<span class="nv">add_batch</span>],

                                                               <span class="nv">num_steps</span><span class="o">=</span><span class="nv">tc</span>.<span class="nv">num_steps_per_iteration</span><span class="ss">)</span>

        # <span class="ss">(</span><span class="nv">Optional</span><span class="ss">)</span> <span class="nv">Optimize</span> <span class="nv">by</span> <span class="nv">wrapping</span> <span class="nv">some</span> <span class="nv">of</span> <span class="nv">the</span> <span class="nv">code</span> <span class="nv">in</span> <span class="nv">a</span> <span class="nv">graph</span> <span class="nv">using</span> <span class="nv">TF</span> <span class="nv">function</span>.

        <span class="nv">tf_agent</span>.<span class="nv">train</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">)</span>

        <span class="nv">collect_driver</span>.<span class="nv">run</span> <span class="o">=</span> <span class="nv">common</span>.<span class="nv">function</span><span class="ss">(</span><span class="nv">collect_driver</span>.<span class="nv">run</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">for each iteration</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">  collect_driver.run</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">()</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="nv">self</span>.<span class="nv">log_api</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">  tf_agent.train</span><span class="s1">&#39;</span>, <span class="s1">&#39;</span><span class="s">(experience=...)</span><span class="s1">&#39;</span><span class="ss">)</span>

        <span class="k">while</span> <span class="nv">True</span>:

            <span class="nv">self</span>.<span class="nv">on_train_iteration_begin</span><span class="ss">()</span>

            # <span class="nv">Collect</span> <span class="nv">a</span> <span class="nv">few</span> <span class="nv">steps</span> <span class="nv">using</span> <span class="nv">collect_policy</span> <span class="nv">and</span> <span class="nv">save</span> <span class="nv">to</span> <span class="nv">the</span> <span class="nv">replay</span> <span class="nv">buffer</span>.

            <span class="k">for</span> <span class="nv">_</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="nv">tc</span>.<span class="nv">num_steps_per_iteration</span><span class="ss">)</span>:

                <span class="nv">collect_driver</span>.<span class="nv">run</span><span class="ss">()</span>

            # <span class="nv">Sample</span> <span class="nv">a</span> <span class="nv">batch</span> <span class="nv">of</span> <span class="nv">data</span> <span class="nv">from</span> <span class="nv">the</span> <span class="nv">buffer</span> <span class="nv">and</span> <span class="nv">update</span> <span class="nv">the</span> <span class="nv">agent</span><span class="s1">&#39;</span><span class="s">s network.</span>

            <span class="nv">experience</span>, <span class="nv">_</span> <span class="o">=</span> <span class="k">next</span><span class="ss">(</span><span class="nv">iterator</span><span class="ss">)</span>

            <span class="nv">loss_info</span> <span class="o">=</span> <span class="nv">tf_agent</span>.<span class="nv">train</span><span class="ss">(</span><span class="nv">experience</span><span class="ss">)</span>

            <span class="nv">total_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">loss</span>.<span class="nv">numpy</span><span class="ss">()</span>

            <span class="nv">actor_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">actor_loss</span>

            <span class="nv">alpha_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">alpha_loss</span>

            <span class="nv">critic_loss</span> <span class="o">=</span> <span class="nv">loss_info</span>.<span class="nv">extra</span>.<span class="nv">critic_loss</span>

            <span class="nv">self</span>.<span class="nv">on_train_iteration_end</span><span class="ss">(</span><span class="nv">loss</span><span class="o">=</span><span class="nv">total_loss</span>, <span class="nv">actor_loss</span><span class="o">=</span><span class="nv">actor_loss</span>, <span class="nv">critic_loss</span><span class="o">=</span><span class="nv">critic_loss</span>,

                                        <span class="nv">alpha_loss</span><span class="o">=</span><span class="nv">alpha_loss</span><span class="ss">)</span>

            <span class="k">if</span> <span class="nv">tc</span>.<span class="nv">training_done</span>:

                <span class="k">break</span>

        <span class="k">return</span>
</pre></div>


</details>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../kerasrl/" title="Kerasrl" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Kerasrl
              </span>
            </div>
          </a>
        
        
          <a href="../tforce/" title="Tforce" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Tforce
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../../assets/javascripts/application.ac79c3b0.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../../../.."}})</script>
      
    
  </body>
</html>