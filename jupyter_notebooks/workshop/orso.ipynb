{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "orso.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zpzHtN3-kQ26",
        "w3OdHyWEEEwy",
        "bzoq0VM85p46"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/workshop/orso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "source": [
        "# Orso's live\n",
        "\n",
        "Make our bear Orso find all the honey in his home turf choosing the most convenient path.  \n",
        "\n",
        "<img src='https://github.com/christianhidber/easyagents/blob/master/jupyter_notebooks/workshop/orso.png?raw=1'>\n",
        "\n",
        "\n",
        "<img src='https://github.com/christianhidber/easyagents/blob/master/jupyter_notebooks/workshop/turf.png?raw=1'>\n",
        "\n",
        "\n",
        "https://opendatascience.com/bears-need-to-learn-as-well-practical-reinforcement-learning-with-tensorflow-2-0-tf-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wncrMgecnGT0",
        "colab_type": "text"
      },
      "source": [
        "## Structure of Observation and Reward are crucial\n",
        "* agents can learn short cuts and unexpected behavior\n",
        "* Funny list of machines learned as opposed what the designers intended them to learn\n",
        "  * `Reward-shaping a soccer robot for touching the ball caused it to learn to get to the ball and vibrate touching it as fast as possible`\n",
        "  * `Robot hand pretending to grasp an object by moving between the camera and the object`\n",
        "  * `Simulated pancake making robot learned to throw the pancake as high in the air as possible in order to maximize time away from the ground`\n",
        "  * `Agent pauses the game indefinitely to avoid losing`\n",
        "  * `In an artificial life simulation where survival required energy but giving birth had no energy cost, one species evolved a sedentary lifestyle that consisted mostly of mating in order to produce new children which could be eaten (or used as mates to produce more edible children).`\n",
        "  * `Agent kills itself at the end of level 1 to avoid losing in level 2`\n",
        "  * `Evolved player makes invalid moves far away in the board, causing opponent players to run out of memory and crash`\n",
        "  * `Creatures exploited physics simulation bugs by twitching, which accumulated simulator errors and allowed them to travel at unrealistic speeds`\n",
        "  * `Reward-shaping a bicycle agent for not falling over & making progress towards a goal point (but not punishing for moving away) leads it to learn to circle around the goal in a physically stable loop.`\n",
        "  * `... algorithm learns to bait an opponent into following it off a cliff, which gives it enough points for an extra life, which it does forever in an infinite loop.`\n",
        "  * `The PPO algorithm discovers that it can slip through the walls of a level to move right and attain a higher score.`\n",
        "\n",
        "Source: https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/\n",
        "  * https://arxiv.org/abs/1803.03453 \n",
        "  * https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml\n",
        "  * https://twitter.com/mogwai_poet/status/1060286856493813760\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sQ8Nfk3MKgLt"
      },
      "source": [
        "### Install gym, tensorflow, tf-agents,..., setup display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8nTX9SQNkWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plt.xkcd()\n",
        "# plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 9)\n",
        "# mpl.rcParams[\"figure.dpi\"] = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QFQXRNBFI5Re",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install -q easyagents 2>/dev/null\n",
        "    \n",
        "!pip install -q networkx==2.3.0 2>/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tu_0F0pf8_RI",
        "colab": {}
      },
      "source": [
        "import easyagents\n",
        "easyagents.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aS8yqznR8UL7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "##### suppress package warnings, in colab: load additional packages for rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DcilvDdeI5Ri",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "if 'google.colab' in sys.modules:\n",
        "    !apt-get install xvfb >/dev/null\n",
        "    !pip install pyvirtualdisplay >/dev/null    \n",
        "    \n",
        "    from pyvirtualdisplay import Display\n",
        "    Display(visible=0, size=(960, 720)).start() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w3OdHyWEEEwy"
      },
      "source": [
        "# Define Gym Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0KmQfa88sQki"
      },
      "source": [
        "<img src='https://github.com/christianhidber/easyagents/blob/master/jupyter_notebooks/workshop/gym.png?raw=1'>\n",
        "\n",
        "http://gym.openai.com/\n",
        "\n",
        "## Our focus is a bit different\n",
        "\n",
        "* the gym has originally been designed by reinforcement learning researchers as a suite of environments to comapre difference algorithms\n",
        "* it has a nice API supported by all major reinforcement learning libraries\n",
        "* since we are practitioners we use that same API to build our own environments\n",
        "\n",
        "## How does the API look like?\n",
        "* an environment is a Python class\n",
        "* `step` as its main method\n",
        "* it takes the action and returns the new observation, the reward, and if the game is done\n",
        "* the environment also makes you defined how an observation looks like and what actions can be performed\n",
        "* this is important to configure the agent\n",
        "* in our case the agents strategy is always defined as a TensorFlow neural network\n",
        "* it is configured to take the observationa as input and outputs a probability for each action\n",
        "* this probability describes the chance of this action to be the most successful\n",
        "* you would also defined an `__init__` (constructor), a `reset` method, and a `render` method\n",
        "\n",
        "## Skeleton of an Environment\n",
        "```python\n",
        "class MyEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "      # set up the environment\n",
        "\n",
        "    def step(self, action):\n",
        "        # make the action have an impact on the environment\n",
        "        # and return the information the algorithms need\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        # reset this environment to the initial state and return the according observation\n",
        "        return observation\n",
        "\n",
        "    def render(self, mode='ansi'):\n",
        "        # spits out a human readable rendering of the environment\n",
        "        if mode == 'ansi':\n",
        "            return self._render_ansi()\n",
        "        elif mode == 'rgb_array':\n",
        "            return self._render_rgb()\n",
        "        else:\n",
        "            super().render(mode=mode)\n",
        "  \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9k_obuGGI5Rk"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HQyb_Aq8Kg9j",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "import matplotlib.image as mpi\n",
        "from matplotlib.offsetbox import (OffsetImage, AnnotationBbox)\n",
        "from IPython.display import display, clear_output\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OsJ6zcXvwN53"
      },
      "source": [
        "### Helper methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "        'D': 4,\n",
        "        'E': 5,\n",
        "        'F': 6,\n",
        "        'G': 7,\n",
        "        'H': 8,\n",
        "        'K': 9,\n",
        "        'L': 10,\n",
        "        'M': 11,\n",
        "        'N': 12,\n",
        "        'O': 13,\n",
        "        'P': 14,\n",
        "        'Q': 15,\n",
        "        'R': 16,\n",
        "        'T': 17,\n",
        "        'U': 18,\n",
        "        'V': 19,\n",
        "        'W': 20,\n",
        "        'X': 21,\n",
        "        'Y': 22,\n",
        "        'Z': 23\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C',\n",
        "        4: 'D',\n",
        "        5: 'E',\n",
        "        6: 'F',\n",
        "        7: 'G',\n",
        "        8: 'H',\n",
        "        9: 'K',\n",
        "        10: 'L',\n",
        "        11: 'M',\n",
        "        12: 'N',\n",
        "        13: 'O',\n",
        "        14: 'P',\n",
        "        15: 'Q',\n",
        "        16: 'R',\n",
        "        17: 'T',\n",
        "        18: 'U',\n",
        "        19: 'V',\n",
        "        20: 'W',\n",
        "        21: 'X',\n",
        "        22: 'Y',\n",
        "        23: 'Z'\n",
        "    }\n",
        "    return state_map[state_as_int]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4I5HgrPa99kh"
      },
      "source": [
        "# This will be the cell where you redefine Orso's world (not now, later!)\n",
        "\n",
        "* Each row defines one location in Orso's world\n",
        "* S stands for start and is the cave Orso lives in\n",
        "* Each location has access to up to 4 other locations defined in each row\n",
        "* Each connection is attributed with a value which is the cost of traveling the connection\n",
        "* You can change this graph in the first hands-on of this notebook described later in this notebook \n",
        "* Anything preceded by the \\# is a comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kfX4K3--9Me-",
        "colab": {}
      },
      "source": [
        "# this is a comment\n",
        "\n",
        "graph = {\n",
        "            'S': [('A', 300), ('B', 100), ('C', 200)],\n",
        "            'A': [('S', 300), ('B', 100), ('E', 100), ('D', 100)],\n",
        "            'B': [('S', 100), ('A', 100), ('C', 50), ('K', 200)],\n",
        "            'C': [('S', 200), ('B', 50), ('M', 100), ('L', 200)],\n",
        "            'D': [('A', 100), ('F', 50)],\n",
        "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
        "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
        "            'G': [('F', 200), ('O', 300)],\n",
        "            'H': [('E', 100), ('K', 300)],\n",
        "            'K': [('B', 200), ('H', 300)],\n",
        "            'L': [('C', 200), ('M', 50)],\n",
        "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
        "            'N': [('M', 100), ('O', 100)],\n",
        "            'O': [('N', 100), ('G', 300)]\n",
        "# this is how you could add a new node (make sure to remove N and O node above)\n",
        "            # 'N': [('M', 100), ('O', 100), ('Z', 100)],\n",
        "            # 'O': [('N', 100), ('G', 300), ('Z', 300)],\n",
        "            # 'Z': [('O', 300), ('N', 100)]\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x-olom0nwiSX"
      },
      "source": [
        "### Orso's Environment (OpenAI Gym)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3plH2u3Swotj",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class OrsoEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    showStep = False\n",
        "\n",
        "    def __init__(self):\n",
        "        self.map = graph\n",
        "        max_paths = 4\n",
        "        self.action_space = spaces.Discrete(max_paths)\n",
        "\n",
        "        positions = len(self.map)\n",
        "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
        "        # non existing path is -1000 and no position change\n",
        "        # look at what #getObservation returns if you are confused\n",
        "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
        "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
        "        self.observation_space = spaces.Box(low=low,\n",
        "                                            high=high,\n",
        "                                            dtype=np.float32)\n",
        "        self.reward_range = (-1, 1)\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "        self._figure = None\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "        base = \"https://raw.githubusercontent.com/christianhidber/easyagents/master/images/\"\n",
        "        self.image_orso = mpi.imread(base + \"Bear.png\")\n",
        "        self.image_cave = mpi.imread(base + \"Cave.png\")\n",
        "        self.image_honey = mpi.imread(base + \"Honey.png\")\n",
        "        self.image_empty_pot = mpi.imread(base + \"EmptyPot.png\")\n",
        "        self.nx_graph, self.nx_pos = self._create_nx_graph()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def iterate_path(self, state, action):\n",
        "        paths = self.map[state]\n",
        "        if action < len(paths):\n",
        "            return paths[action]\n",
        "        else:\n",
        "            # sorry, no such action, stay where you are and pay a high penalty\n",
        "            return (state, 1000)\n",
        "\n",
        "    def step(self, action):\n",
        "        destination, cost = self.iterate_path(self.state, action)\n",
        "\n",
        "        self.cost = cost\n",
        "        self.action = action\n",
        "        self.lastStep_state = self.state\n",
        "        self.state = destination\n",
        "        self.customerReward = self.customer_reward[destination]\n",
        "        self.reward = 0\n",
        "        self.reward = (self.customerReward - self.cost) / self.optimum\n",
        "\n",
        "        self.customer_visited(destination)\n",
        "        done = (destination == 'S' and self.all_customers_visited())\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += self.reward\n",
        "        self.stepCount += 1\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "        return observation, self.reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = np.array([position,\n",
        "                           self.getPathObservation(position, 0),\n",
        "                           self.getPathObservation(position, 1),\n",
        "                           self.getPathObservation(position, 2),\n",
        "                           self.getPathObservation(position, 3)\n",
        "                           ],\n",
        "                          dtype=np.float32)\n",
        "        all_rest_rewards = list(self.customer_reward.values())\n",
        "        result = np.append(result, all_rest_rewards)\n",
        "        return result\n",
        "\n",
        "    def getPathObservation(self, position, path):\n",
        "        paths = self.map[self.state]\n",
        "        if path < len(paths):\n",
        "            target, cost = paths[path]\n",
        "            reward = self.customer_reward[target]\n",
        "            result = reward - cost\n",
        "        else:\n",
        "            result = -1000\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "    # each node gets a reward, either 0 or 10000\n",
        "    def modulate_reward(self):\n",
        "        self.customer_reward = {}\n",
        "\n",
        "        node_names = list(self.map.keys())\n",
        "        # initialize all nodes with 0\n",
        "        for node_name in node_names:\n",
        "            self.customer_reward[node_name] = 0\n",
        "\n",
        "        # 10000 rewards are only at a few random places\n",
        "        number_of_customers = len(self.map) - 1\n",
        "        number_per_consultant = int(number_of_customers / 2)\n",
        "        self._honeypot_places = []\n",
        "\n",
        "        # starting from 1, not 0, so that 'S' (position of the cave) never gets a reward \n",
        "        samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
        "        for sample in samples:\n",
        "            self.customer_reward[node_names[sample]] = 1000\n",
        "            self._honeypot_places = self._honeypot_places + [node_names[sample]]\n",
        "\n",
        "    def reset(self):\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "        self.state = 'S'\n",
        "        self.cost = 0\n",
        "        self.action = 0\n",
        "        self.lastStep_state = ''\n",
        "        self.customerReward = None\n",
        "        self._honeypot_places = None\n",
        "        self.reward = 0\n",
        "        self.envEpisodeCount += 1\n",
        "        self.modulate_reward()\n",
        "        self._figure = None\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "\n",
        "    def _create_nx_graph(self):\n",
        "        \"\"\" generates the networkx graph representing orso's world with all its paths.\n",
        "\n",
        "        :return: graph, positions\n",
        "        \"\"\"\n",
        "        nx_graph = nx.Graph()\n",
        "        for node_id in self.map.keys():\n",
        "            zoom = 0.6\n",
        "            image = self.image_empty_pot\n",
        "            nx_graph.add_node(node_id, image=image, zoom=zoom)\n",
        "        for source, connections in self.map.items():\n",
        "            for action, (target, cost) in enumerate(connections):\n",
        "                if cost >= 300:\n",
        "                    color = 'dodgerblue'\n",
        "                elif cost >= 200:\n",
        "                    color = 'darkgoldenrod'\n",
        "                elif cost >= 100:\n",
        "                    color = 'forestgreen'\n",
        "                else:\n",
        "                    color = 'greenyellow'\n",
        "                directed_label = str(source) + \":\" + str(action)\n",
        "                existing_edge = nx_graph.get_edge_data(source, target)\n",
        "                if existing_edge is not None: \n",
        "                    directed_label = str(existing_edge['label']) + ' - ' + directed_label\n",
        "                nx_graph.add_edge(source, target, color=color, weight=6, cost=cost, label=directed_label, image=self.image_cave)\n",
        "        nx_pos = nx.kamada_kawai_layout(nx_graph)\n",
        "        return nx_graph, nx_pos\n",
        "\n",
        "    def _render_to_figure(self, render_graph_labels=False, render_costs=False):\n",
        "        \"\"\" Renders the current state as a graph with matplotlib\n",
        "        \"\"\"\n",
        "        # draw graph using matplotlib\n",
        "        if (self._figure is not None):\n",
        "            plt.close(self._figure)\n",
        "        self._figure = plt.figure()\n",
        "        if len(self._figure.axes) == 0:\n",
        "            self._figure.add_subplot(1, 1, 1)\n",
        "        self._figure.axes[0].cla()\n",
        "        ax = self._figure.axes[0]\n",
        "\n",
        "        edges = self.nx_graph.edges()\n",
        "        edge_colors = [self.nx_graph[u][v]['color'] for u, v in edges]\n",
        "        edge_weights = [self.nx_graph[u][v]['weight'] for u, v in edges]\n",
        "\n",
        "        nx.draw(self.nx_graph, pos=self.nx_pos, ax=ax, node_color='lightgrey',\n",
        "                edges=edges, edge_color=edge_colors, width=edge_weights, with_labels=render_graph_labels)\n",
        "\n",
        "        if render_graph_labels:\n",
        "          edge_labels = [self.nx_graph[u][v]['label'] for u, v in edges]\n",
        "          zipped_edge_labels = dict(zip(edges, edge_labels))\n",
        "          nx.draw_networkx_edge_labels(self.nx_graph, pos=self.nx_pos, ax=ax, edge_labels=zipped_edge_labels)\n",
        "\n",
        "        if render_costs:\n",
        "          edge_costs = [self.nx_graph[u][v]['cost'] for u, v in edges]\n",
        "          zipped_edge_labels = dict(zip(edges, edge_costs))\n",
        "          nx.draw_networkx_edge_labels(self.nx_graph, pos=self.nx_pos, ax=ax, edge_labels=zipped_edge_labels)\n",
        "\n",
        "        # draw images on graph nodes\n",
        "        # set image (according to the current state) and sizes (make orso's current position larger)\n",
        "        for node_id in self.nx_graph.nodes():\n",
        "            node = self.nx_graph.node[node_id]\n",
        "            node['zoom'] = 0.4\n",
        "            if node_id == self.state:\n",
        "                node['zoom'] = 0.6\n",
        "            if node_id in self._honeypot_places:\n",
        "                node['image'] = self.image_empty_pot\n",
        "                if self.customer_reward[node_id] > 0:\n",
        "                    node['image'] = self.image_honey\n",
        "            else:\n",
        "                node['image'] = None\n",
        "            if node_id == 'S':\n",
        "                node['image'] = self.image_cave\n",
        "            if self.state == node_id:\n",
        "                node['image'] = self.image_orso\n",
        "\n",
        "        # position images\n",
        "        for n in self.nx_pos:\n",
        "            node = self.nx_graph.node[n]\n",
        "            image = node['image']\n",
        "            if image is not None: \n",
        "                xp, yp = self.nx_pos[n]\n",
        "                offset_image = OffsetImage(image, node['zoom'])\n",
        "                offset_image.image.axes = ax\n",
        "                ab = AnnotationBbox(offset_image, (xp, yp),\n",
        "                                    xybox=(0, 0),\n",
        "                                    xycoords='data',\n",
        "                                    boxcoords=\"offset points\",\n",
        "                                    pad=0.0,\n",
        "                                    frameon=False\n",
        "                                    )\n",
        "                ax.add_artist(ab)\n",
        "\n",
        "        self._figure.canvas.draw()\n",
        "\n",
        "    def _render_ansi(self):\n",
        "        result = (\"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) +\n",
        "                  \" Step: \" + (\"%4.0f  \" % self.stepCount) +\n",
        "                  self.lastStep_state + ' --' + str(self.action) + '-> ' + self.state +\n",
        "                  ' R=' + (\"% 2.2f\" % self.reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) +\n",
        "                  ' cost=' + (\"%4.0f\" % self.cost) + ' customerR=' + (\"%4.0f\" % self.customerReward) + ' optimum=' + (\n",
        "                          \"%4.0f\" % self.optimum)\n",
        "                  )\n",
        "        return result\n",
        "\n",
        "    def _render_rgb(self):\n",
        "        self._render_to_figure()\n",
        "        self._figure.canvas.draw()\n",
        "        buf = self._figure.canvas.tostring_rgb()\n",
        "        num_cols, num_rows = self._figure.canvas.get_width_height()\n",
        "        plt.close(self._figure)\n",
        "        self._figure = None\n",
        "        result = np.fromstring(buf, dtype=np.uint8).reshape(num_rows, num_cols, 3)\n",
        "        return result\n",
        "\n",
        "    def render(self, mode='human', render_graph_labels=False, render_costs=False):\n",
        "        if mode == 'ansi':\n",
        "            return self._render_ansi()\n",
        "        elif mode == 'human':\n",
        "            clear_output(wait=True)\n",
        "            self._render_to_figure(render_graph_labels=render_graph_labels, render_costs=render_costs)\n",
        "            plt.pause(0.01)\n",
        "            return\n",
        "        elif mode == 'rgb_array':\n",
        "            return self._render_rgb()\n",
        "        else:\n",
        "            super().render(mode=mode)\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vKUZ3Pk8I5Rt",
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "# First Steps\n",
        "\n",
        "Orso - the bear - lives in his cave (position 'S'). Orso knows his area and where he can \n",
        "typically find some honey (positions 'A' to 'O'). The honey places are scattered all over his home turf and connected\n",
        "by some pathways. Some connections are easy like walking over a grassy field (light green, very low cost) or forests \n",
        "(dark green, low cost), a bit strenuous like walking over a hill (brown, higher costs) and very arduous like swimming \n",
        "through a lake (blue, very high costs).\n",
        "\n",
        "There are other bears around competing for the honey pots. So some spots have honey (yellow nodes) and some\n",
        "are already taken (brown nodes).\n",
        "\n",
        "Every morning, Orso awakes and starts looking for some honey. But every day, the honey pots are at a new place\n",
        "( env.reset() ). So Orso climbs a tree near his cave to find out where the honey is. Even with this knowledge it is not clear which route he should take. Being a bear also means being lazy, so Orso wants to get as much honey as possible while at the lowest costs possible. At each spot he has to decide where to go next (the action). His daily journey ends when his back in his own cave.\n",
        "\n",
        "Be god and create Orso's world:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fHrBrxxI5Rt",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "env = OrsoEnv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JD9SGkYxI5Rw",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "A new morning starts (rerun the cell below to see how the honey pots change places). But Orso \n",
        "(depicted in 'fuchsia', Marsha's favorite color ?) doesn't know \n",
        "about the sweetness of honey or his surroundings. So the climbs a tree to get an overview and this is what he sees:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I3Y0OaFpCBOl",
        "colab": {}
      },
      "source": [
        "# this is the observation the agent will see\n",
        "env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TOAwmZ4p8UMH",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "env.render(render_graph_labels=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3u9Rd6YcI5Ry",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Take Orso by his paws and go for a few steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s2RYX2_2I5Ry",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "env.step(1)\n",
        "# env.step(3)\n",
        "env.render(render_graph_labels=True)\n",
        "print(env.render(mode='ansi'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UjkFXVspI5R1"
      },
      "source": [
        "Note that the honey pot nodes turn brown once Orso has passed and taken the honey."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EYaTAvAyYO-U"
      },
      "source": [
        "Now Orso must start to learn by himself to take actions to gather all the honey while minimizing the effort:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGpag-boFxDc",
        "colab_type": "text"
      },
      "source": [
        "### Let's see how a random policy moves our dear Orso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqcEMhtmFxDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "from easyagents.env import register_with_gym\n",
        "from easyagents.agents import RandomAgent\n",
        "from easyagents.callbacks import plot\n",
        "\n",
        "env_name=\"Orso-v1\"\n",
        "register_with_gym(gym_env_name=env_name, entry_point=OrsoEnv, max_episode_steps=1000)\n",
        "\n",
        "rndAgent = RandomAgent(gym_env_name='Orso-v1')\n",
        "rndAgent.play([plot.State(),plot.Actions(), plot.StepRewards()], num_episodes=3, max_steps_per_episode=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHtWtte8FxDh",
        "colab_type": "text"
      },
      "source": [
        "### Let's see how our average score is doing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju3rcDMSFxDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean, std, min, max, all = rndAgent.score()\n",
        "mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T8rjblby9VI1"
      },
      "source": [
        "# Hands-On 1: Define Orso's world and make him move in it\n",
        "* run the notebook up to this point\n",
        "* define your version of Orso's world by changing the graph in the cell marked above\n",
        "* consider\n",
        "  * adding and connecting new nodes\n",
        "  * removing nodes and their connections\n",
        "  * adding connections between existing nodes\n",
        "* You can use letters A-Z as names for nodes, except for I and J\n",
        "* If you refer to a node, you need to define its outgoing connections as well\n",
        "* Typically connections are simmetrical, that means you specify them both ways\n",
        "* look at Orso move in this new world\n",
        "\n",
        "# STOP HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_uEfpOeeI5R8"
      },
      "source": [
        "# Train policy with tfagents PpoAgent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OS3pnnbAI5R9",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "from easyagents.agents import PpoAgent\n",
        "\n",
        "register_with_gym(gym_env_name=env_name, entry_point=OrsoEnv, max_episode_steps=1000)\n",
        "ppoAgent = PpoAgent(gym_env_name=env_name, fc_layers=(500,500,500))\n",
        "\n",
        "\n",
        "plots = [plot.State(), \n",
        "         plot.Actions(), \n",
        "         plot.StepRewards(), \n",
        "         plot.Rewards(), \n",
        "         plot.Steps(), \n",
        "         plot.Loss(),\n",
        "         plot.ToMovie()]\n",
        "ppoAgent.train(plots, \n",
        "               learning_rate=1e-4, # ignore this one\n",
        "               num_iterations = 600, \n",
        "               num_episodes_per_iteration = 10,\n",
        "               max_steps_per_episode = 25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4pyIWF2FxDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_r, std_r, min_r, max_r, all_r = ppoAgent.score()\n",
        "mean_r, std_r, min_r, max_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyI4bltIn9xe"
      },
      "source": [
        "# Hands-On 2: Run the cells above and watch the agent (Orso) learn\n",
        "\n",
        "* run the notebook up to this point\n",
        "* full training of the agent can take up to 25 minutes\n",
        "  * so you are advised to reduce the number of iterations to 100 or even 50 (from 500)\n",
        "  * note that depending on how your world looks like and for how long you train, you might not get good results\n",
        "* while the training is running watch how the graphs evolve and make sense out of it\n",
        "* answer the questions below\n",
        "\n",
        "Questions:\n",
        "* Why is the bear sometimes not ending up in its cave?\n",
        "* Why is the position of the honey pots constantly changing in the state plot (the one on the right)?\n",
        "* Is there any relation between the sum of rewards and number of steps?\n",
        "* Can you make sense of all the other plots?\n",
        "* Why do you get a negative reward at the beginning?\n",
        "* How good do you think the reward can get?\n",
        "* What is a good number of steps?\n",
        "\n",
        "\n",
        "# STOP HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1RbywHm43Itu"
      },
      "source": [
        "## Our Observations\n",
        "\n",
        "* Note from the histogram that Orso initially is stuck with action 0. As the training proceeds the other actions are also discovered, resulting on average in shorter episodes and more rewards. \n",
        "* Also the state plot (rendering the state of the last evaluation episodes last step) shows that Orso starts to find more honey pots on its way back home. \n",
        "* Note that action 3 is less likely than the other actions. That's because we only have a few positions with 3 or more valid directions but all positions have at least 2 directions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C0VqefK12wWG"
      },
      "source": [
        "## Play - see the bear cruise his turf (turn ouput into a movie)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6SSXO5H_2wWH",
        "colab": {}
      },
      "source": [
        "# ppoAgent.play([plot.State(), plot.StepRewards(num_steps_between_plot=1), plot.Actions(), plot.ToMovie(fps=3)], \n",
        "#               num_episodes=5, max_steps_per_episode=20)\n",
        "ppoAgent.play([plot.State()], \n",
        "              num_episodes=1, max_steps_per_episode=20);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oLXlllMd2wWL"
      },
      "source": [
        "Play uses the previously trained policy. We play for 5 episodes, each with max 20 steps. All steps are rendered into a movie with a frame rate of 3 steps per second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-NrpYJYo0LC",
        "colab_type": "text"
      },
      "source": [
        "### Play and log all steps textually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvt0QI6SfLxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from easyagents.callbacks import log\n",
        "\n",
        "\n",
        "ppoAgent.play([log.Step()], default_plots=False, num_episodes=1, max_steps_per_episode=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UjNNILItk-HC"
      },
      "source": [
        "# How good can you get? What is the optimum?\n",
        "\n",
        "* generally you just do not know\n",
        "* the higher the score the better\n",
        "* baselines can help in Reinforcement Learning as well\n",
        "* in our special case, there is a deterministic baseline\n",
        "  * best first search (https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)\n",
        "  * guarentees to find optimal solution\n",
        "  * exponential complexity\n",
        "  * but works fine for our size of the problem\n",
        "  * optimal score between _.73_ and _.74_ with very low variance\n",
        "  * number of steps to pass through complete turf around _17_\n",
        "* our reinforcement learning approach trades the perfect solution for\n",
        "  * nearly perfect solution \n",
        "  * linear time complexity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v9O5B9yD0u0X",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}