{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/easyagents_line.ipynb\" target=\"_parent\">\n",
    "   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Line World\n",
    "\n",
    "* an agent lives in a finite linear world of uneven elements\n",
    "* at each moment it is in a certain position\n",
    "* initial position is the middle\n",
    "* some positions gain rewards, some don't\n",
    "* rewards are between 0 and 15\n",
    "* agent can either move left or right\n",
    "* Objective: maximize total reward = sum(rewards) + sum(steps)\n",
    "* Cost per step: -1\n",
    "* Done Condition: agent is at pos 0 or total reward <= -20\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/christianhidber/easyagents/linebook/images/line-world.png'>\n",
    "\n",
    "## TODO\n",
    "1. Wie machen wir es mit sich entwickelnden Environments? Version hochzählen wie hier?\n",
    "  * Der easy agent scheint nicht klar zu kommen mit mehr einem wechselnden Environment\n",
    "1. OZ->CHH: Wie geht das mit dem einklinken in das Rendering, bin hier überfordert, Hilfe!\n",
    "1. Version mit Lücken für die Übung erstellen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQ8Nfk3MKgLt"
   },
   "source": [
    "### Install gym, tensorflow, tf-agents,..., setup display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFQXRNBFI5Re",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q easyagents\n",
    "!pip install -q networkx==2.3.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aS8yqznR8UL7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### suppress package warnings, in colab: load additional packages for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DcilvDdeI5Ri",
    "outputId": "ebcf640f-0254-442c-cdd8-062b61c12547",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get install xvfb >/dev/null\n",
    "    !pip install pyvirtualdisplay >/dev/null    \n",
    "    \n",
    "    from pyvirtualdisplay import Display\n",
    "    Display(visible=0, size=(960, 720)).start() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3OdHyWEEEwy"
   },
   "source": [
    "# Define Gym Environment\n",
    "\n",
    "## What you need to define\n",
    "1. Actions\n",
    "1. Observation\n",
    "1. Reward\n",
    "1. Game done?\n",
    "\n",
    "## Remember how an environment looks like\n",
    "\n",
    "```python\n",
    "class MyEnv(gym.Env):\n",
    "  def __init__(self):\n",
    "      # set up the environment\n",
    "\n",
    "    def step(self, action):\n",
    "        # make the action have an impact on the environment\n",
    "        # and return the information the algorithms need\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # reset this environment to the initial state and return the according observation\n",
    "        return observation\n",
    "\n",
    "    def render(self, mode='ansi'):\n",
    "        # spits out a human readable rendering of the environment\n",
    "        if mode == 'ansi':\n",
    "            return self._render_ansi()\n",
    "        if mode == 'human':\n",
    "            return self._render_human()            \n",
    "        elif mode == 'rgb_array':\n",
    "            return self._render_rgb() \n",
    "```\n",
    "\n",
    "https://gym.openai.com/\n",
    "\n",
    "https://github.com/openai/gym/blob/master/docs/creating-environments.md\n",
    "\n",
    "## Applying Reinforcement Learning to a problem\n",
    "\n",
    "* Is your problem approachable by Reinforcement Learning (more on that later)?\n",
    "  * Can you defined what would be the agent and what the environment?\n",
    "* You can simulate your environment or are able to safely perform a large number of experiments in the real world\n",
    "* You need to be able to define your problem as a Markov Decision Process (MDP)\n",
    "* Good sanity check: Would you as a human be able to play the game based on the observation and reward you get?\n",
    "* there are different kinds of observations\n",
    "  * fully observed environment (e.g. Jump'n'Run Game)\n",
    "  * partially observed environment (e.g. Ego Shooter)\n",
    "* Choose a proper Reinforcement Learning Algorithm (more on that later)\n",
    "\n",
    "## MDP: Markov Decision Process in Reinforcement Learning\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Markov_decision_process\n",
    "* Model your observation in a way that which actions the agent took to get to a certain state are irrelevant\n",
    "* That means all information necessary to derive a proper action need to be in a single observation\n",
    "* The agents must not be forced to also learn the sequence of observations\n",
    "* E.g. the position of the PacMan is not enough. Which path taken results in where the ghosts are and what pills have been eaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yEvuCliwJiD",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "class LineWorldEnv(gym.Env):\n",
    "\n",
    "    # constructor sets up the properties of the environment\n",
    "    # most important action space, observation space and reward range\n",
    "    def __init__(self, state=[10, 0, 0, 5, 0, 2, 15]):\n",
    "        self.state = np.array(state)\n",
    "        # the agent can perform  different actions\n",
    "        number_of_actions = 2\n",
    "        self.action_space = spaces.Discrete(number_of_actions)\n",
    "\n",
    "        self.size_of_world = len(state)\n",
    "\n",
    "        ### OZ->CHH: Hier würden wir was hinschreiben, damit es zur Observation passt, das müssten die Teilnehmer dann ebenfalls umbauen\n",
    "        # Für observation gleiche Werte für alles\n",
    "\n",
    "        # Observation size = 5\n",
    "        # Min = 0\n",
    "        # Max = 10\n",
    "\n",
    "        # Constante rein in v0\n",
    "\n",
    "        # Ableiten von State in get observation\n",
    "\n",
    "        # Plot arbeitet auf State, nicht observation\n",
    "\n",
    "        # the environment's state is described by the position of the agent and the remaining rewards\n",
    "        low = np.append([0], np.full(self.size_of_world, 0))\n",
    "        high = np.append([self.size_of_world - 1], np.full(self.size_of_world, 15))\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                            high=high,\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        self.reward_range = (-1, 1)\n",
    "        # 32 is only theoretical, because we need to travel at least 9 steps, leaving us with 23 practically\n",
    "        self.optimum = self.state.sum() - 9\n",
    "\n",
    "        self._figure = None\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    # OZ->CHH: Für die Übung würde hier nur ein flaches np.array herauskommen mit Nullen drin, geht dann natürlich nicht \n",
    "    def get_observation(self):\n",
    "        return np.append([self.pos], self.remaining_rewards)\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "        self.done = False\n",
    "        self.pos = math.floor(len(self.state) / 2)\n",
    "        self.steps = 0\n",
    "\n",
    "        self.remaining_rewards = np.array(self.state, copy=True)\n",
    "        return self.get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == LEFT and self.pos != 0:\n",
    "          self.pos -= 1\n",
    "        elif self.pos < self.size_of_world -1:\n",
    "          self.pos += 1\n",
    "\n",
    "        reward = self.remaining_rewards[self.pos] - 1\n",
    "        normalized_reward = reward / self.optimum\n",
    "        self.total_reward += normalized_reward\n",
    "        self.remaining_rewards[self.pos] = 0\n",
    "\n",
    "        if self.pos == 0 or self.total_reward * self.optimum <= -20:\n",
    "          self.done = True\n",
    "        self.steps += 1\n",
    "\n",
    "        observation = self.get_observation()\n",
    "        info = None\n",
    "        return observation, normalized_reward, self.done, info\n",
    "\n",
    "    def _render_to_ansi(self):\n",
    "        return 'position: {position}, remaining rewards: {rewards}, total reward so far: {total}, normalized total reward: {normalized_total}, steps so far: {steps}, game done: {done}'.format(\n",
    "            position=self.pos, \n",
    "            rewards=self.remaining_rewards, \n",
    "            total=self.total_reward * self.optimum, \n",
    "            normalized_total = self.total_reward,\n",
    "            done=self.done,\n",
    "            steps=self.steps)              \n",
    "\n",
    "    def _render_to_figure(self):\n",
    "        \"\"\" Renders the current state as a graph with matplotlib \"\"\"\n",
    "        if self._figure is not None:\n",
    "            plt.close(self._figure)\n",
    "        self._figure, ax = plt.subplots(1, figsize=(8, 4))\n",
    "        x = np.arange(0, self.size_of_world, 1, dtype=np.uint8)\n",
    "        y = self.remaining_rewards\n",
    "        plt.plot([self.pos, self.pos], [0, 2], 'r^-')\n",
    "        ax.scatter(x, y, s=75)\n",
    "        self._figure.canvas.draw()\n",
    "        \n",
    "    def _render_to_human(self):\n",
    "        \"\"\" show render_to_figure in a jupyter cell. \n",
    "            the result of each call is rendered in the same cell\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        self._render_to_figure()\n",
    "        plt.pause(0.01)\n",
    "        \n",
    "    def _render_to_rgb(self):\n",
    "        \"\"\" convert the output of render_to_figure to a rgb_array \"\"\"\n",
    "        self._render_to_figure()\n",
    "        self._figure.canvas.draw()\n",
    "        buf = self._figure.canvas.tostring_rgb()\n",
    "        num_cols, num_rows = self._figure.canvas.get_width_height()\n",
    "        plt.close(self._figure)\n",
    "        self._figure = None\n",
    "        result = np.fromstring(buf, dtype=np.uint8).reshape(num_rows, num_cols, 3)\n",
    "        return result        \n",
    "\n",
    "    def render(self, mode='ansi'):\n",
    "        if mode == 'ansi':\n",
    "            return self._render_to_ansi()\n",
    "        elif mode == 'human':\n",
    "            return self._render_to_human()\n",
    "        elif mode == 'rgb_array':\n",
    "            return self._render_to_rgb()\n",
    "        else:\n",
    "            super().render(mode=mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2aXwjnTc9LV"
   },
   "source": [
    "# Hands-On 1: Try out our environment\n",
    "\n",
    "_use yourself to simulate the agent_\n",
    "\n",
    "* Create an environment and play the game to the end manually using the cells below\n",
    "* Use a sequence of left and right commands to find the best solution possible\n",
    "* Show yourself only local observations and reward for each step and choose an action\n",
    "* Would this be sufficent for a good reward?\n",
    "* What reward do you get? What is the optimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fHrBrxxI5Rt",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "env = LineWorldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "hxcQG_ruV06p",
    "outputId": "d604f0cc-65d3-4e25-cf81-b1c901d87967"
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SJARCo5U9PfR",
    "outputId": "510eb2ef-52f8-443d-b141-f8ccf8010dad"
   },
   "outputs": [],
   "source": [
    "# shows the raw observation, position is first, then all rest rewards\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "stTFpBh_9R6X",
    "outputId": "0dc8fb40-d0f7-42e0-9864-820baddc7c48"
   },
   "outputs": [],
   "source": [
    "# more complete and readable information\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s2RYX2_2I5Ry",
    "outputId": "6e4180ed-50a9-4608-c29c-8c6b6fa9b8c9",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mGWH4CQCBSEk",
    "outputId": "099d8200-82af-400b-d595-fab2b565cae2"
   },
   "outputs": [],
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JdHyE42vBhbC",
    "outputId": "6bce282e-aa51-4964-b756-6fe109b96408"
   },
   "outputs": [],
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5QRACggIChZK",
    "outputId": "cb04cf27-b695-428c-a9d6-4aead296d1a0"
   },
   "outputs": [],
   "source": [
    "env.step(RIGHT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "RvfFHbSfckCy",
    "outputId": "296fc5b4-6af0-4133-d15c-d02a19faf71d"
   },
   "outputs": [],
   "source": [
    "env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r4zbZtCsBx7x",
    "outputId": "d939d5f4-7a10-4c8c-960d-d9baf2c1b1e8"
   },
   "outputs": [],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5eso3wdfFfGu",
    "outputId": "0a5a3748-62a7-4830-8daa-cafcf2f10595"
   },
   "outputs": [],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_FUidR2SFgq2",
    "outputId": "e9222e21-038c-47bb-ccea-bf6713540cce"
   },
   "outputs": [],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zaPJ-GDtFnRL",
    "outputId": "588a0a37-8e5b-4580-fcf1-ce49a9c1d2f9"
   },
   "outputs": [],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RdmP61D_FpTE",
    "outputId": "917980c6-e986-4a9e-e6a4-d32811881102"
   },
   "outputs": [],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y1dxqY92FqV2",
    "outputId": "569fad4b-bac9-4074-adb9-7674b1c60eba"
   },
   "outputs": [],
   "source": [
    "env.step(LEFT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uEfpOeeI5R8"
   },
   "source": [
    "# Train policy with tfagents PpoAgent\n",
    "\n",
    "### Register with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS3pnnbAI5R9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.easyenv import register_with_gym\n",
    "\n",
    "env_name=\"LineWorld-v0\"\n",
    "register_with_gym(gym_env_name=env_name, entry_point=LineWorldEnv, max_episode_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "gFISrkwhI5SZ",
    "outputId": "eabab98f-9994-4fd8-8023-1d564bc9dfaa",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from easyagents.tfagents import PpoAgent\n",
    "from easyagents.config import Training\n",
    "\n",
    "training=Training( num_iterations = 50,\n",
    "                   num_episodes_per_iteration = 10,\n",
    "                   max_steps_per_episode = 50,\n",
    "                   num_epochs_per_iteration = 5 )\n",
    "\n",
    "ppoAgent = PpoAgent(    gym_env_name = env_name,\n",
    "                        fc_layers=(500,500), \n",
    "                        training=training,\n",
    "                        learning_rate=1e-4\n",
    "                   )\n",
    "ppoAgent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "2oeN8hhRJqMr",
    "outputId": "0d96f889-bdc6-48e1-e448-67e2ac957ba9"
   },
   "outputs": [],
   "source": [
    "ppoAgent.render_episodes(num_episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OJYNSBaI5Sf"
   },
   "source": [
    "### Visualize training and policy (with custom y-limits and movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "JVAsAknHI5Sf",
    "outputId": "29c22448-9626-43e7-9cf9-71f4f9abcb8d",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "_ = ppoAgent.plot_episodes(ylim=[None,(0,1),(0,50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "C0XVkNWaI5Sj",
    "outputId": "9b2e16fa-238a-4e05-b50c-8ee1610816ee",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent.render_episodes_to_jupyter(num_episodes=1, fps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XFFIzxWJkQD",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRoljypzdBML"
   },
   "source": [
    "# Hands-On 2: Define the observation in a way that allows the agent to train\n",
    "\n",
    "* You see the agent does not learn with the observation given. Why is that?\n",
    "\n",
    "* Find an observation that works and impement it in `get_observation`\n",
    "\n",
    "*  You need to update the `observation_space` to the same shape of your observation at the same time.\n",
    "\n",
    "* Why does it work? Is this the best observation? How would you even know?\n",
    "\n",
    "* Do you think this would be sufficient for a real world example? If not, what is missing?\n",
    "\n",
    " # STOP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgVJIrHb8vyF"
   },
   "source": [
    "# Generalizing our Environment\n",
    "\n",
    "* solving a single set up of our line world is not very realistic\n",
    "* there should be variations to rewards, size, and starting position\n",
    "* while training with with each episode we could randomly set rewards and starting postition\n",
    "* different sizes could be simulated by using a fixed max size with a random number of padding entires (e.g -1) at the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDOWfWAmc8mx"
   },
   "source": [
    "# How does Reinforcement Learning really work?\n",
    "\n",
    "TODO: Link to slides of presentation for Reinforce|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uotjTcRgE2P"
   },
   "source": [
    "# Hands-On 3: Make your notebook work with Reinforce\n",
    "\n",
    "* What parts do you need to change?\n",
    "\n",
    "* How do the results compare to what you have seen with PPO?\n",
    "\n",
    "* Can you tweak the parameters (like the size of the neural network) to improve the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTHjAT-6Z7UQ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from easyagents.tfagents import ReinforceAgent\n",
    "from easyagents.config import Training\n",
    "\n",
    "training=Training( num_iterations = 200,\n",
    "                   num_episodes_per_iteration = 10,\n",
    "                   max_steps_per_episode = 50,\n",
    "                   num_epochs_per_iteration = 5 )\n",
    "\n",
    "reinforceAgent = ReinforceAgent(    gym_env_name = env_name,\n",
    "                        fc_layers=(500,500), \n",
    "                        training=training,\n",
    "                        learning_rate=1e-4\n",
    "                   )\n",
    "reinforceAgent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.tfagents import ReinforceAgent\n",
    "ReinforceAgent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "include_colab_link": true,
   "name": "easyagents_line.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}