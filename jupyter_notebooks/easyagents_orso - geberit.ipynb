{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/easyagents_orso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Orso's live running on EasyAgents\n",
    "\n",
    "Make our bear Orso find all the honey in his home turf choosing the most convenient path.  \n",
    "\n",
    "![Orso](images/Orso.png)\n",
    "\n",
    "\n",
    "![Orso](images/Turf.png)\n",
    "\n",
    "\n",
    "https://opendatascience.com/bears-need-to-learn-as-well-practical-reinforcement-learning-with-tensorflow-2-0-tf-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aS8yqznR8UL7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### suppress package warnings, in colab: load additional packages for rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3OdHyWEEEwy"
   },
   "source": [
    "# Define Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQyb_Aq8Kg9j",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpi\n",
    "from matplotlib.offsetbox import (OffsetImage, AnnotationBbox)\n",
    "from IPython.display import display, clear_output\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-olom0nwiSX"
   },
   "source": [
    "### Orso's Environment (OpenAI Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3plH2u3Swotj",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "graph = {\n",
    "            'S': [('A', 300), ('B', 100), ('C', 200)],\n",
    "            'A': [('S', 300), ('B', 100), ('E', 100), ('D', 100)],\n",
    "            'B': [('S', 100), ('A', 100), ('C', 50), ('K', 200)],\n",
    "            'C': [('S', 200), ('B', 50), ('M', 100), ('L', 200)],\n",
    "            'D': [('A', 100), ('F', 50)],\n",
    "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
    "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
    "            'G': [('F', 200), ('O', 300)],\n",
    "            'H': [('E', 100), ('K', 300)],\n",
    "            'K': [('B', 200), ('H', 300)],\n",
    "            'L': [('C', 200), ('M', 50)],\n",
    "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
    "            'N': [('M', 100), ('O', 100)],\n",
    "            'O': [('N', 100), ('G', 300)]\n",
    "        }\n",
    "\n",
    "def state_name_to_int(state):\n",
    "    state_name_map = {\n",
    "        'S': 0,\n",
    "        'A': 1,\n",
    "        'B': 2,\n",
    "        'C': 3,\n",
    "        'D': 4,\n",
    "        'E': 5,\n",
    "        'F': 6,\n",
    "        'G': 7,\n",
    "        'H': 8,\n",
    "        'K': 9,\n",
    "        'L': 10,\n",
    "        'M': 11,\n",
    "        'N': 12,\n",
    "        'O': 13\n",
    "    }\n",
    "    return state_name_map[state]\n",
    "\n",
    "def int_to_state_name(state_as_int):\n",
    "    state_map = {\n",
    "        0: 'S',\n",
    "        1: 'A',\n",
    "        2: 'B',\n",
    "        3: 'C',\n",
    "        4: 'D',\n",
    "        5: 'E',\n",
    "        6: 'F',\n",
    "        7: 'G',\n",
    "        8: 'H',\n",
    "        9: 'K',\n",
    "        10: 'L',\n",
    "        11: 'M',\n",
    "        12: 'N',\n",
    "        13: 'O'\n",
    "    }\n",
    "    return state_map[state_as_int]\n",
    "\n",
    "class OrsoEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['ansi']}\n",
    "    showStep = False\n",
    "\n",
    "    def __init__(self):\n",
    "        self.map = graph\n",
    "        max_paths = 4\n",
    "        self.action_space = spaces.Discrete(max_paths)\n",
    "\n",
    "        positions = len(self.map)\n",
    "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
    "        # non existing path is -1000 and no position change\n",
    "        # look at what #getObservation returns if you are confused\n",
    "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
    "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                            high=high,\n",
    "                                            dtype=np.float32)\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.envEpisodeCount = 0\n",
    "        self.envStepCount = 0\n",
    "        self._figure = None\n",
    "\n",
    "        self.reset()\n",
    "        self.optimum = self.calculate_customers_reward()\n",
    "\n",
    "        base = \"https://raw.githubusercontent.com/christianhidber/easyagents/master/images/\"\n",
    "        self.image_orso = mpi.imread(base + \"Bear.png\")\n",
    "        self.image_cave = mpi.imread(base + \"Cave.png\")\n",
    "        self.image_honey = mpi.imread(base + \"Honey.png\")\n",
    "        self.image_empty_pot = mpi.imread(base + \"EmptyPot.png\")\n",
    "        self.nx_graph, self.nx_pos = self._create_nx_graph()\n",
    "\n",
    "    def iterate_path(self, state, action):\n",
    "        paths = self.map[state]\n",
    "        if action < len(paths):\n",
    "            return paths[action]\n",
    "        else:\n",
    "            # sorry, no such action, stay where you are and pay a high penalty\n",
    "            return (state, 1000)\n",
    "\n",
    "    def step(self, action):\n",
    "        destination, cost = self.iterate_path(self.state, action)\n",
    "\n",
    "        self.cost = cost\n",
    "        self.action = action\n",
    "        self.lastStep_state = self.state\n",
    "        self.state = destination\n",
    "        self.customerReward = self.customer_reward[destination]\n",
    "        self.reward = 0\n",
    "        self.reward = (self.customerReward - self.cost) / self.optimum\n",
    "\n",
    "        self.customer_visited(destination)\n",
    "        done = (destination == 'S' and self.all_customers_visited())\n",
    "\n",
    "        stateAsInt = state_name_to_int(self.state)\n",
    "        self.totalReward += self.reward\n",
    "        self.stepCount += 1\n",
    "        self.envStepCount += 1\n",
    "\n",
    "        if done and not self.isDone:\n",
    "            self.envEpisodeCount += 1\n",
    "\n",
    "        self.isDone = done\n",
    "        observation = self.getObservation(stateAsInt)\n",
    "        info = {\"from\": self.state, \"to\": destination}\n",
    "        return observation, self.reward, done, info\n",
    "\n",
    "    def getObservation(self, position):\n",
    "        result = np.array([position,\n",
    "                           self.getPathObservation(position, 0),\n",
    "                           self.getPathObservation(position, 1),\n",
    "                           self.getPathObservation(position, 2),\n",
    "                           self.getPathObservation(position, 3)\n",
    "                           ],\n",
    "                          dtype=np.float32)\n",
    "        all_rest_rewards = list(self.customer_reward.values())\n",
    "        result = np.append(result, all_rest_rewards)\n",
    "        return result\n",
    "\n",
    "    def getPathObservation(self, position, path):\n",
    "        paths = self.map[self.state]\n",
    "        if path < len(paths):\n",
    "            target, cost = paths[path]\n",
    "            reward = self.customer_reward[target]\n",
    "            result = reward - cost\n",
    "        else:\n",
    "            result = -1000\n",
    "\n",
    "        return result\n",
    "\n",
    "    def customer_visited(self, customer):\n",
    "        self.customer_reward[customer] = 0\n",
    "\n",
    "    def all_customers_visited(self):\n",
    "        return self.calculate_customers_reward() == 0\n",
    "\n",
    "    def calculate_customers_reward(self):\n",
    "        sum = 0\n",
    "        for value in self.customer_reward.values():\n",
    "            sum += value\n",
    "        return sum\n",
    "\n",
    "    def modulate_reward(self):\n",
    "        number_of_customers = len(self.map) - 1\n",
    "        number_per_consultant = int(number_of_customers / 2)\n",
    "        self.customer_reward = {\n",
    "            'S': 0\n",
    "        }\n",
    "        self._honeypot_places = []\n",
    "        for customer_nr in range(1, number_of_customers + 1):\n",
    "            self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
    "\n",
    "        # every consultant only visits a few random customers\n",
    "        samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
    "        key_list = list(self.customer_reward.keys())\n",
    "        for sample in samples:\n",
    "            self.customer_reward[key_list[sample]] = 1000\n",
    "            self._honeypot_places = self._honeypot_places + [key_list[sample]]\n",
    "\n",
    "    def reset(self):\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "        self.state = 'S'\n",
    "        self.cost = 0\n",
    "        self.action = 0\n",
    "        self.lastStep_state = ''\n",
    "        self.customerReward = None\n",
    "        self._honeypot_places = None\n",
    "        self.reward = 0\n",
    "        self.envEpisodeCount += 1\n",
    "        self.modulate_reward()\n",
    "        self._figure = None\n",
    "        return self.getObservation(state_name_to_int(self.state))\n",
    "\n",
    "    def _create_nx_graph(self):\n",
    "        \"\"\" generates the networkx graph representing orso's world with all its paths.\n",
    "\n",
    "        :return: graph, positions\n",
    "        \"\"\"\n",
    "        nx_graph = nx.Graph()\n",
    "        for node_id in self.map.keys():\n",
    "            zoom = 0.6\n",
    "            image = self.image_empty_pot\n",
    "            nx_graph.add_node(node_id, image=image, zoom=zoom)\n",
    "        for source, connections in self.map.items():\n",
    "            for target, cost in connections:\n",
    "                if cost >= 300:\n",
    "                    color = 'dodgerblue'\n",
    "                elif cost >= 200:\n",
    "                    color = 'darkgoldenrod'\n",
    "                elif cost >= 100:\n",
    "                    color = 'forestgreen'\n",
    "                else:\n",
    "                    color = 'greenyellow'\n",
    "                nx_graph.add_edge(source, target, color=color, weight=6, image=self.image_cave)\n",
    "        nx_pos = nx.kamada_kawai_layout(nx_graph)\n",
    "        return nx_graph, nx_pos\n",
    "\n",
    "    def _render_to_figure(self):\n",
    "        \"\"\" Renders the current state as a graph with matplotlib\n",
    "        \"\"\"\n",
    "        # draw graph using matplotlib\n",
    "        if (self._figure is not None):\n",
    "            plt.close(self._figure)\n",
    "        self._figure = plt.figure(\"BeraterEnv\", figsize=(12, 9))\n",
    "        if len(self._figure.axes) == 0:\n",
    "            self._figure.add_subplot(1, 1, 1)\n",
    "        self._figure.axes[0].cla()\n",
    "        ax = self._figure.axes[0]\n",
    "\n",
    "        edges = self.nx_graph.edges()\n",
    "        edge_colors = [self.nx_graph[u][v]['color'] for u, v in edges]\n",
    "        edge_weights = [self.nx_graph[u][v]['weight'] for u, v in edges]\n",
    "        nx.draw(self.nx_graph, pos=self.nx_pos, ax=ax, node_color='lightgrey',\n",
    "                edges=edges, edge_color=edge_colors, width=edge_weights)\n",
    "\n",
    "        # draw images on graph nodes\n",
    "        # set image (according to the current state) and sizes (make orso's current position larger)\n",
    "        for node_id in self.nx_graph.nodes():\n",
    "            node = self.nx_graph.node[node_id]\n",
    "            node['zoom'] = 0.4\n",
    "            if node_id == self.state:\n",
    "                node['zoom'] = 0.6\n",
    "            if node_id in self._honeypot_places:\n",
    "                node['image'] = self.image_empty_pot\n",
    "                if self.customer_reward[node_id] > 0:\n",
    "                    node['image'] = self.image_honey\n",
    "            else:\n",
    "                node['image'] = None\n",
    "            if node_id == 'S':\n",
    "                node['image'] = self.image_cave\n",
    "            if self.state == node_id:\n",
    "                node['image'] = self.image_orso\n",
    "\n",
    "        # position images\n",
    "        for n in self.nx_pos:\n",
    "            node = self.nx_graph.node[n]\n",
    "            image = node['image']\n",
    "            if image is not None: \n",
    "                xp, yp = self.nx_pos[n]\n",
    "                offset_image = OffsetImage(image, node['zoom'])\n",
    "                offset_image.image.axes = ax\n",
    "                ab = AnnotationBbox(offset_image, (xp, yp),\n",
    "                                    xybox=(0, 0),\n",
    "                                    xycoords='data',\n",
    "                                    boxcoords=\"offset points\",\n",
    "                                    pad=0.0,\n",
    "                                    frameon=False\n",
    "                                    )\n",
    "                ax.add_artist(ab)\n",
    "\n",
    "        self._figure.canvas.draw()\n",
    "\n",
    "    def _render_ansi(self):\n",
    "        result = (\"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) +\n",
    "                  \" Step: \" + (\"%4.0f  \" % self.stepCount) +\n",
    "                  self.lastStep_state + ' --' + str(self.action) + '-> ' + self.state +\n",
    "                  ' R=' + (\"% 2.2f\" % self.reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) +\n",
    "                  ' cost=' + (\"%4.0f\" % self.cost) + ' customerR=' + (\"%4.0f\" % self.customerReward) + ' optimum=' + (\n",
    "                          \"%4.0f\" % self.optimum)\n",
    "                  )\n",
    "        return result\n",
    "\n",
    "    def _render_rgb(self):\n",
    "        self._render_to_figure()\n",
    "        self._figure.canvas.draw()\n",
    "        buf = self._figure.canvas.tostring_rgb()\n",
    "        num_cols, num_rows = self._figure.canvas.get_width_height()\n",
    "        plt.close(self._figure)\n",
    "        self._figure = None\n",
    "        result = np.fromstring(buf, dtype=np.uint8).reshape(num_rows, num_cols, 3)\n",
    "        return result\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'ansi':\n",
    "            return self._render_ansi()\n",
    "        elif mode == 'human':\n",
    "            clear_output(wait=True)\n",
    "            self._render_to_figure()\n",
    "            plt.pause(0.01)\n",
    "            return\n",
    "        elif mode == 'rgb_array':\n",
    "            return self._render_rgb()\n",
    "        else:\n",
    "            super().render(mode=mode)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKUZ3Pk8I5Rt",
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# First Steps\n",
    "\n",
    "Be god and create Orso's world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fHrBrxxI5Rt",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "env = OrsoEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "TOAwmZ4p8UMH",
    "outputId": "9b001b37-ccd1-407d-82fd-4182c843bdbd",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "s2RYX2_2I5Ry",
    "outputId": "bc2ee123-9bee-45e9-f5fe-919a95e3a003",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "TtXMSrM0I5R2",
    "outputId": "0d5a4d32-24aa-4abc-b493-2a8b078fb341",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.step(3)\n",
    "env.render()\n",
    "print(env.render(mode='ansi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OS3pnnbAI5R9",
    "outputId": "96494cef-ca9d-4f56-ad5b-5521dcf0e1a4",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from easyagents.env import register_with_gym\n",
    "\n",
    "env_name=\"Orso-v1\"\n",
    "register_with_gym(env_name, OrsoEnv, max_episode_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uEfpOeeI5R8"
   },
   "source": [
    "# Train policy with tfagents PpoAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzGp2sTZI5R_"
   },
   "source": [
    "##  Custom plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import log, plot, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "id": "r-MT_PYVI5SB",
    "outputId": "44241abc-091b-4118-95d9-bcc5b9f3fef5",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ppoAgent = PpoAgent('Orso-v1', fc_layers=(500,500,500))\n",
    "\n",
    "plots = [duration.Fast()]\n",
    "\n",
    "ppoAgent.train(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play (turn ouput into a movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppoAgent.play([plot.State(),plot.ToMovie(fps=3)], default_callbacks=False, num_episodes=5, max_steps_per_episode=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfO2VaVhI5SY"
   },
   "source": [
    "## Custom training (duration, learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "x8ZFVRWdI5Sd",
    "outputId": "fb05971a-5744-483f-bfc1-48610931e6c2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ppoAgent = PpoAgent( 'Orso-v1',fc_layers=(500,500,500))\n",
    "ppoAgent.train([plot.State()],learning_rate=0.0001,\n",
    "               num_iterations = 500, num_episodes_per_iteration = 10,\n",
    "               max_steps_per_episode = 50, num_epochs_per_iteration = 5 )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zpzHtN3-kQ26",
    "w3OdHyWEEEwy",
    "bzoq0VM85p46"
   ],
   "include_colab_link": true,
   "name": "easyagents_orso.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
