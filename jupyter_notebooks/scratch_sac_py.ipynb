{
 "cells": [],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7_SAC_minitaur_tutorial.ipynb",
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "from __future__ import absolute_import\n",
     "from __future__ import division\n",
     "from __future__ import print_function\n",
     "\n",
     "import base64\n",
     "import imageio\n",
     "import IPython\n",
     "import matplotlib\n",
     "import matplotlib.pyplot as plt\n",
     "import PIL.Image\n",
     "\n",
     "import tensorflow as tf\n",
     "tf.compat.v1.enable_v2_behavior()\n",
     "\n",
     "from tf_agents.agents.ddpg import critic_network\n",
     "from tf_agents.agents.sac import sac_agent\n",
     "from tf_agents.drivers import dynamic_step_driver\n",
     "from tf_agents.environments import suite_gym\n",
     "from tf_agents.environments import tf_py_environment\n",
     "from tf_agents.eval import metric_utils\n",
     "from tf_agents.metrics import tf_metrics\n",
     "from tf_agents.networks import actor_distribution_network\n",
     "from tf_agents.networks import normal_projection_network\n",
     "from tf_agents.policies import greedy_policy\n",
     "from tf_agents.policies import random_tf_policy\n",
     "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
     "from tf_agents.trajectories import trajectory\n",
     "from tf_agents.utils import common\n",
     "\n",
     "\n",
     "env_name = \"MinitaurBulletEnv-v0\" # @param {type:\"string\"}\n",
     "num_iterations = 1000000 # @param {type:\"integer\"}\n",
     "\n",
     "initial_collect_steps = 10000 # @param {type:\"integer\"} \n",
     "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
     "replay_buffer_capacity = 1000000 # @param {type:\"integer\"}\n",
     "\n",
     "batch_size = 256 # @param {type:\"integer\"}\n",
     "\n",
     "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
     "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
     "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
     "target_update_tau = 0.005 # @param {type:\"number\"}\n",
     "target_update_period = 1 # @param {type:\"number\"}\n",
     "gamma = 0.99 # @param {type:\"number\"}\n",
     "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
     "gradient_clipping = None # @param\n",
     "\n",
     "actor_fc_layer_params = (256, 256)\n",
     "critic_joint_fc_layer_params = (256, 256)\n",
     "\n",
     "log_interval = 5000 # @param {type:\"integer\"}\n",
     "\n",
     "num_eval_episodes = 30 # @param {type:\"integer\"}\n",
     "eval_interval = 10000 # @param {type:\"integer\"}\n",
     "\n",
     "train_py_env = suite_gym.load(env_name)\n",
     "eval_py_env = suite_gym.load(env_name)\n",
     "\n",
     "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
     "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
     "\n",
     "observation_spec = train_env.observation_spec()\n",
     "action_spec = train_env.action_spec()\n",
     "critic_net = critic_network.CriticNetwork(\n",
     "    (observation_spec, action_spec),\n",
     "    observation_fc_layer_params=None,\n",
     "    action_fc_layer_params=None,\n",
     "    joint_fc_layer_params=critic_joint_fc_layer_params)\n",
     "\n",
     "def normal_projection_net(action_spec,init_means_output_factor=0.1):\n",
     "  return normal_projection_network.NormalProjectionNetwork(\n",
     "      action_spec,\n",
     "      mean_transform=None,\n",
     "      state_dependent_std=True,\n",
     "      init_means_output_factor=init_means_output_factor,\n",
     "      std_transform=sac_agent.std_clip_transform,\n",
     "      scale_distribution=True)\n",
     "\n",
     "\n",
     "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
     "    observation_spec,\n",
     "    action_spec,\n",
     "    fc_layer_params=actor_fc_layer_params,\n",
     "    continuous_projection_net=normal_projection_net)\n",
     "\n",
     "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
     "tf_agent = sac_agent.SacAgent(\n",
     "    train_env.time_step_spec(),\n",
     "    action_spec,\n",
     "    actor_network=actor_net,\n",
     "    critic_network=critic_net,\n",
     "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
     "        learning_rate=actor_learning_rate),\n",
     "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
     "        learning_rate=critic_learning_rate),\n",
     "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
     "        learning_rate=alpha_learning_rate),\n",
     "    target_update_tau=target_update_tau,\n",
     "    target_update_period=target_update_period,\n",
     "    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\n",
     "    gamma=gamma,\n",
     "    reward_scale_factor=reward_scale_factor,\n",
     "    gradient_clipping=gradient_clipping,\n",
     "    train_step_counter=global_step)\n",
     "tf_agent.initialize()\n",
     "\n",
     "eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n",
     "collect_policy = tf_agent.collect_policy\n",
     "\n",
     "def compute_avg_return(environment, policy, num_episodes=5):\n",
     "\n",
     "  total_return = 0.0\n",
     "  for _ in range(num_episodes):\n",
     "\n",
     "    time_step = environment.reset()\n",
     "    episode_return = 0.0\n",
     "\n",
     "    while not time_step.is_last():\n",
     "      action_step = policy.action(time_step)\n",
     "      time_step = environment.step(action_step.action)\n",
     "      episode_return += time_step.reward\n",
     "    total_return += episode_return\n",
     "\n",
     "  avg_return = total_return / num_episodes\n",
     "  return avg_return.numpy()[0]\n",
     "\n",
     "\n",
     "\n",
     "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
     "    data_spec=tf_agent.collect_data_spec,\n",
     "    batch_size=train_env.batch_size,\n",
     "    max_length=replay_buffer_capacity)\n",
     "\n",
     "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
     "        train_env,\n",
     "        collect_policy,\n",
     "        observers=[replay_buffer.add_batch],\n",
     "        num_steps=initial_collect_steps)\n",
     "initial_collect_driver.run()\n",
     "\n",
     "# Dataset generates trajectories with shape [Bx2x...]\n",
     "dataset = replay_buffer.as_dataset(\n",
     "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
     "\n",
     "iterator = iter(dataset)\n",
     "\n",
     "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
     "    train_env,\n",
     "    collect_policy,\n",
     "    observers=[replay_buffer.add_batch],\n",
     "    num_steps=collect_steps_per_iteration)\n",
     "\n",
     "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
     "tf_agent.train = common.function(tf_agent.train)\n",
     "collect_driver.run = common.function(collect_driver.run)\n",
     "\n",
     "# Reset the train step\n",
     "tf_agent.train_step_counter.assign(0)\n",
     "\n",
     "# Evaluate the agent's policy once before training.\n",
     "avg_return = compute_avg_return(eval_env, eval_policy, num_eval_episodes)\n",
     "returns = [avg_return]\n",
     "\n",
     "for _ in range(num_iterations):\n",
     "\n",
     "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
     "  for _ in range(collect_steps_per_iteration):\n",
     "    collect_driver.run()\n",
     "\n",
     "  # Sample a batch of data from the buffer and update the agent's network.\n",
     "  experience, unused_info = next(iterator)\n",
     "  train_loss = tf_agent.train(experience)\n",
     "\n",
     "  step = tf_agent.train_step_counter.numpy()\n",
     "\n",
     "  if step % log_interval == 0:\n",
     "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
     "\n",
     "  if step % eval_interval == 0:\n",
     "    avg_return = compute_avg_return(eval_env, eval_policy, num_eval_episodes)\n",
     "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
     "    returns.append(avg_return)\n",
     "\n",
     "steps = range(0, num_iterations + 1, eval_interval)\n",
     "plt.plot(steps, returns)\n",
     "plt.ylabel('Average Return')\n",
     "plt.xlabel('Step')\n",
     "plt.ylim()\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}