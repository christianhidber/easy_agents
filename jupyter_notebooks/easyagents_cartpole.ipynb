{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itbylRO_XSzd"
   },
   "source": [
    "# CartPole Gym environment with TfAgents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JszJwM7GXkFK"
   },
   "source": [
    "## Install packages (gym, tfagents, tensorflow,....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### suppress package warnings, prepare matplotlib, if in colab: load additional packages for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !apt-get install xvfb >/dev/null\n",
    "    !pip install pyvirtualdisplay >/dev/null    \n",
    "    \n",
    "    from pyvirtualdisplay import Display\n",
    "    Display(visible=0, size=(960, 720)).start()    \n",
    "else:\n",
    "    #  for local installation\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### install easyagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C-jRHCOPW56b",
    "outputId": "28038221-7a27-4906-bc36-84b6efc7dba8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    !pip install easyagents >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67n5CylJZJpt"
   },
   "source": [
    "## Dry Run (very short training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick test and train for a few iterations using a default network architecture (2 layers, fully connected, 100 neurons each):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import duration\n",
    "\n",
    "ppoAgent = PpoAgent('CartPole-v0')\n",
    "ppoAgent.train([duration.Fast()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that rewards and step count are equal. That's because the CartPole environment doles out 1 reward point for \n",
    "each successful step, and stops after the first failed step.\n",
    "\n",
    "'duration.Fast()' configures the training length to 10 iterations with 10 episodes each. \n",
    "After every iteration the current policy is retrained. The resulting loss is depicted in the first plot.\n",
    "Since Ppo is an actor-critic algorithm we not only plot the total loss but also the loss for the actor- and\n",
    "critic-networks separately.\n",
    "\n",
    "In the \"steps\" and \"sum of rewards\" plots we see the result of the evaluation.\n",
    "The coloured area represents the max and min values encoutered for the current evaluation period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67n5CylJZJpt"
   },
   "source": [
    "## Train (plot state, custom network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.agents import PpoAgent\n",
    "from easyagents.callbacks import plot, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ppoAgent = PpoAgent('CartPole-v0', fc_layers=(100, 50, 25))\n",
    "ppoAgent.train([plot.State(), plot.Loss(), plot.Actions(), plot.Rewards()],\n",
    "               num_iterations=10, num_iterations_between_eval=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fc_layers argument defines the policy's neural network architecture. Here we use 3 fully connected layers\n",
    "with 100 neurons in the first, 50 in the second and 25 in the final layer. \n",
    "By default fc_layers=(75,75) is used.\n",
    "\n",
    "The first argument of the train method is a list of callbacks. Through callbacks we define the plots generated during \n",
    "training, the logging behaviour or control training duration. \n",
    "By passing [plot.State(), plot.Loss(), plot.Actions(), plot.Rewards()] we add in particular the State() plot, \n",
    "depicting the last observation state of the last evaluation episode. plot.Actions() displays a histogram of the \n",
    "actions taken for each episode played during the last evaluation period. \n",
    "\n",
    "Besides num_iterations there are quite a few parameters to specify the exact training duration (e.g. \n",
    "num_episodes_per_iteration, num_epochs_per_iteration, max_steps_per_episode,...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching the algorithm\n",
    "\n",
    "Switching from Ppo to Dqn is easy, essentially just replace PpoAgent with DqnAgent (the evaluation may take a few\n",
    "minuites):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyagents.agents import DqnAgent\n",
    "from easyagents.callbacks import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dqnAgent = DqnAgent('CartPole-v0', fc_layers=(100, ))\n",
    "dqnAgent.train([plot.State(), plot.Loss(), plot.Actions(), plot.Rewards()], \n",
    "               num_iterations=20000, num_iterations_between_eval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Dqn by default only takes 1 step per iteration (and thus an episode spans over several iterations) we increased\n",
    "the num_iterations parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67n5CylJZJpt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Next: Custom training (playing, mp4, logging)\n",
    "\n",
    "* see [Orso on colab](https://colab.research.google.com/github/christianhidber/easyagents/blob/master/jupyter_notebooks/easyagents_orso.ipynb)\n",
    "  (an example of a gym environment implementation based on a routing problem)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "easyagents_cartpole.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}